import torch
import os
import sys
import argparse
import json
import time
import random
import numpy as np
from tqdm import tqdm
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer, AutoModelForCausalLM
from str_utils import de_md_logits_processor_for_llama3_1, flaming_tokens
import str_utils

################
# Configurations
################
def get_args():
    parser = argparse.ArgumentParser(description="HLEæ•°å­¦æ¨è«–ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ - DeepSeek R1å¯¾å¿œç‰ˆ")
    parser.add_argument("--model_path", type=str, default="deepseek-ai/DeepSeek-R1",
                        help="HLEæ•°å­¦å¯¾ç­–ã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ï¼ˆDeepSeek R1æ¨å¥¨ï¼‰")
    
    # Generation Parameters
    parser.add_argument("--temperature", type=float, default=1.0)
    parser.add_argument("--top_p", type=float, default=1.0)
    parser.add_argument("--n", type=int, default=200, help="Number of samples to generate for one time.")
    parser.add_argument("--repeat", type=int, default=None, help="Number of times to repeat the instruction generation. Only available when total prompts is not specified.")
    parser.add_argument("--total_prompts", type=int, default=1000, help="Total number of prompts to generate. If specified, repeat will be ignored.")
    parser.add_argument("--max_tokens", type=int, default=2048)
    parser.add_argument("--max_model_len", type=int, default=4096)

    # Generation Settings
    parser.add_argument("--early_stopping", type=bool, default=True, help="Stop generation when the \n is generated.")
    parser.add_argument("--disable_early_stopping", action="store_false", dest="early_stopping", help="Disable early stopping.")
    parser.add_argument("--system_prompt", action="store_true", help="Enable system prompt for extracting the input.")
    parser.add_argument("--sanitize", action="store_true", help="Sanitize the generated instructions. Only available for Gemma and Llama-3 models.")
    parser.add_argument("--logits_processor", action="store_true", help="Enable logits processor for the generation.")
    parser.add_argument("--flaming_tokens", action="store_true", help="Enable flaming initial tokens (increase temperature) for more diverse generation.")
    parser.add_argument("--control_tasks", type=str, default="math", choices=[None, "math", "probability"],  help="HLEæ•°å­¦å¯¾ç­–ç‰¹åŒ–ç‰ˆã§ã¯æ•°å­¦ã‚¿ã‚¹ã‚¯ã®ã¿ã‚µãƒãƒ¼ãƒˆ")
    parser.add_argument("--domain", type=str, default=None, help="æ•°å­¦ãƒ‰ãƒ¡ã‚¤ãƒ³æŒ‡å®š (algebra, calculus, geometry, etc.)")
    parser.add_argument("--shuffle", type=bool, default=True, help="Shuffle the outputs generated by vllm.")
    parser.add_argument("--skip_special_tokens", type=bool, default=True)

    # System Settings
    parser.add_argument('--engine', default="vllm", type=str, choices=["vllm", "hf"])
    parser.add_argument("--device", type=str, default="0")
    parser.add_argument("--dtype", type=str, default="bfloat16", choices=["float16", "bfloat16"])
    parser.add_argument("--tensor_parallel_size", type=int, default=1, help="Number of GPUs to use for tensor parallelism. Only used for Llama 70B models.")
    parser.add_argument("--gpu_memory_utilization", type=float, default=0.95)
    parser.add_argument("--swap_space", type=float, default=2.0)
    parser.add_argument("--checkpoint_every", type=int, default=100, help="Save checkpoint every n repeats.")
    parser.add_argument("--output_folder", type=str, default="../data")
    parser.add_argument("--save_path", type=str, default=None, help="Full path for output file. If specified, overrides default naming.")
    parser.add_argument("--job_name", type=str, default=None, help="Job Name. Get from the script.")
    parser.add_argument("--timestamp", type=int, default=int(time.time()), help="Timestamp for the job. Also used as the random seed.")
    parser.add_argument("--seed", type=int, default=None, help="Random seed.")

    return parser.parse_args()

# Main function to control workflow
def main():
    args = get_args()
    print(f"HLEæ•°å­¦æ¨è«–ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼é–‹å§‹ã€‚å¼•æ•°: {args}")  # ãƒ­ã‚°ç”¨æ—¥æœ¬èªå‡ºåŠ›

    # Raise error if sanitization is requested for unsupported models
    if args.sanitize:
        if not ("gemma" in args.model_path.lower() or "llama-3" in args.model_path.lower()):
            raise ValueError("Sanitization is only supported for Gemma and Llama-3 models.")
    
    if args.total_prompts is None:
        if args.repeat is None:
            raise ValueError("Either total prompts or repeat should be specified.")
        args.total_prompts = args.repeat * args.n
    else:
        # If total prompts is specified, repeat will be ignored
        args.repeat = int(np.ceil(args.total_prompts / args.n))
    
    # Set the random seed for NumPy
    if args.seed is not None:
        np.random.seed(args.seed)
        # Set the random seed for PyTorch
        torch.manual_seed(args.seed)
        # If you are using CUDA (i.e., a GPU), also set the seed for it
        torch.cuda.manual_seed_all(args.seed)
    
    # Create output file / folder
    if args.save_path:
        output_dir = args.save_path
        # Ensure directory exists
        os.makedirs(os.path.dirname(output_dir), exist_ok=True)
    else:
        output_filename = f"Magpie_{args.model_path.split('/')[-1]}_{args.total_prompts}_{args.timestamp}_ins.json"
        if not args.job_name:
            if not os.path.exists(args.output_folder):
                os.makedirs(args.output_folder)
            output_dir = f"{args.output_folder}/{output_filename}"
        else:
            output_dir = f"{args.output_folder}/{args.job_name}/{output_filename}"
    
    # Set the device
    os.environ["CUDA_VISIBLE_DEVICES"] = args.device
    # Set generation engine
    if args.engine == "vllm":
        # Create vllm instance  
        llm = LLM(model=args.model_path, 
                dtype=args.dtype,
                trust_remote_code=True,
                gpu_memory_utilization=args.gpu_memory_utilization,
                max_model_len=args.max_model_len,
                swap_space=args.swap_space,
                tensor_parallel_size=args.tensor_parallel_size,
                seed=args.seed if args.seed is not None else args.timestamp,
                enable_prefix_caching=True)
    elif args.engine == "hf":
        # Load the model and tokenizer
        tokenizer = AutoTokenizer.from_pretrained(args.model_path)
        model = AutoModelForCausalLM.from_pretrained(
            args.model_path,
            device_map={'':torch.cuda.current_device()},
            torch_dtype=torch.bfloat16 if args.dtype == "bfloat16" else torch.float16
        )
    
    
    # Obtain config from configs/model_configs.json
    try:
        with open("../configs/model_configs.json", "r", encoding="utf-8") as f:
            model_configs = json.load(f)
            if args.model_path not in model_configs:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ« '{args.model_path}' ãŒ model_configs.json ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                print(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:")
                for model in sorted(model_configs.keys()):
                    print(f"  - {model}")
                print(f"\nğŸ”§ è§£æ±ºæ–¹æ³•: model_configs.json ã«ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’è¿½åŠ ã™ã‚‹ã‹ã€æ­£ã—ã„ãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
                sys.exit(1)
            model_config = model_configs[args.model_path]
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {args.model_path}")
    except FileNotFoundError:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: model_configs.json ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print(f"ğŸ“ ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}")
        print(f"ğŸ” æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ã‚¹: ../configs/model_configs.json")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: model_configs.json ã® JSON å½¢å¼ãŒä¸æ­£ã§ã™: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)
    
    if args.control_tasks:
        if args.domain:
            print(f"ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–: {args.domain}")
            # ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å„ªå…ˆä½¿ç”¨
            domain_template_key = f"pre_query_template_{args.domain}"
            if domain_template_key in model_config:
                pre_query_template = model_config[domain_template_key]
                print(f"ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½¿ç”¨: {args.domain}")
            else:
                pre_query_template = model_config[f"pre_query_template_{args.control_tasks}"]
                print(f"ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ±ç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½¿ç”¨: {args.control_tasks}")
        else:
            pre_query_template = model_config[f"pre_query_template_{args.control_tasks}"]
            print(f"åˆ¶å¾¡ã‚¿ã‚¹ã‚¯: {args.control_tasks} (HLEæ•°å­¦å¯¾ç­–ç‰¹åŒ–)")
    elif args.system_prompt:
        pre_query_template = model_config["pre_query_template_with_system_prompt"]
        print("ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ‰åŠ¹ã€‚æ³¨æ„: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯æ€§èƒ½ã‚’ä½ä¸‹ã•ã›ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    else:
        pre_query_template = model_config["pre_query_template"]
    stop_tokens = model_config["stop_tokens"]
    stop_tokens_assistant = model_config["stop_tokens_assistant"]
    stop_tokens += stop_tokens_assistant
    stop_token_ids = model_config["stop_token_ids"]
    
    # Process early stopping. We found that sometimes LLM will generate responses immediately after the \n token.
    if args.early_stopping:
        stop_tokens.append("\n")
    
    print(f"äº‹å‰ã‚¯ã‚¨ãƒªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {pre_query_template}")
    print(f"åœæ­¢ãƒˆãƒ¼ã‚¯ãƒ³: {stop_tokens}")
    print(f"åœæ­¢ãƒˆãƒ¼ã‚¯ãƒ³ID: {stop_token_ids}")
    
    # Apply logits processors
    if args.logits_processor and args.flaming_tokens:
        raise ValueError("Cannot enable both logits processor and flaming tokens")
    
    if args.logits_processor and "llama-3.1" in args.model_path.lower():
        logits_processor = de_md_logits_processor_for_llama3_1
        print(f"Logits processor applied: {logits_processor}")
    elif args.flaming_tokens:
        logits_processor = flaming_tokens
        print(f"Logits processor applied: {logits_processor}")
    else:
        logits_processor = None
    
    # Initialize stop tokens if not defined
    if 'stop_tokens' not in locals():
        stop_tokens = []
        stop_token_ids = []
        
    
    # Define sampling parameters
    sampling_params = SamplingParams(
        n=args.n,
        temperature=args.temperature,
        top_p=args.top_p,
        max_tokens=args.max_tokens,
        skip_special_tokens=args.skip_special_tokens,
        stop=stop_tokens,
        stop_token_ids=stop_token_ids,
        logits_processors=[logits_processor] if logits_processor else None
    )
    
    ################
    # Generate outputs
    ################
    results = []
    for rounds in tqdm(range(args.repeat)):
        # Generate outputs
        if args.engine == "vllm":
            output = llm.generate(pre_query_template, sampling_params)
            output_list = output[0].outputs
            if args.shuffle:
                random.shuffle(output_list)
        
        elif args.engine == "hf":
            input = tokenizer.encode(pre_query_template, add_special_tokens=False, return_tensors="pt").to(torch.cuda.current_device())
            # Gemma-2 bug, so we cannot set num_return_sequences > 1. 
            # Instead, we repeat the input n times.
            inputs = input.repeat(args.n, 1).to(torch.cuda.current_device())
            output = model.generate(inputs,
                                    tokenizer=tokenizer, 
                                    do_sample=True, 
                                    temperature=args.temperature, 
                                    top_p=args.top_p, 
                                    max_length=args.max_tokens, 
                                    num_return_sequences=1,
                                    )
            # Remove the input from the output
            output_list = tokenizer.batch_decode(output[i][len(inputs[0]):] for i in range(args.n))
            # Stop on the first stop token
            for i, completion in enumerate(output_list):
                for stop_token in stop_tokens:
                    if stop_token in completion:
                        output_list[i] = completion[:completion.index(stop_token)]
                                                 
        # Save outputs
        for i, completion in enumerate(output_list):
            if args.engine == "vllm":
                instruction = completion.text.strip()
            elif args.engine == "hf":
                instruction = completion.strip()
    
            if args.sanitize:
                sanitized_instruction, class_num = str_utils.instruction_post_process(instruction, args.model_path)
                result = {
                    "id": rounds * args.n + i,
                    "pre_query_template": f"{pre_query_template}",
                    "raw_instruction": instruction,
                    "instruction": sanitized_instruction,
                    "instruction_sanitize_class_num": class_num,
                    "response": None,
                    "created": int(time.time()),
                    "gen_input_configs": {
                        "temperature": args.temperature,
                        "top_p": args.top_p,
                        "input_generator": f"{args.model_path}",
                        "seed": args.seed,
                        "domain": args.domain,
                    },
                    "gen_response_configs": None,
                }
            else:
                result = {
                    "id": rounds * args.n + i,
                    "pre_query_template": f"{pre_query_template}",
                    "instruction": instruction,
                    "response": None,
                    "created": int(time.time()),
                    "gen_input_configs": {
                        "temperature": args.temperature,
                        "top_p": args.top_p,
                        "input_generator": f"{args.model_path}",
                        "seed": args.seed,
                        "domain": args.domain,
                    },
                    "gen_response_configs": None,
                }
            results.append(result)
    
        # Save the checkpoints every args.checkpoint_every rounds
        if rounds % args.checkpoint_every == 0:
            with open(output_dir, "w") as f:
                json.dump(results, f, indent=2)
            print(f"Checkpoint saved. Total prompts: {len(results)}")
    
    # Save the final results
    with open(output_dir, "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"HLEæ•°å­¦æ¨è«–ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†ã€‚ãƒ¢ãƒ‡ãƒ«: {args.model_path}ã€ç·å•é¡Œæ•°: {len(results)}")

# Run the main function
if __name__ == "__main__":
    main()
