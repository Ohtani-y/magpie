import json
import nbformat as nbf

nb = nbf.v4.new_notebook()

cells = [
    nbf.v4.new_markdown_cell('# ğŸ§® Magpie Reasoning - HLEæ•°å­¦å¯¾ç­–ç‰¹åŒ–ç‰ˆ\n\nã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€HLEï¼ˆé«˜ç­‰ãƒ¬ãƒ™ãƒ«è©¦é¨“ï¼‰æ•°å­¦å¯¾ç­–ã«ç‰¹åŒ–ã—ãŸreasoningï¼ˆæ¨è«–ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã®æœ¬ç•ªç”¨çµ±åˆç’°å¢ƒã§ã™ã€‚\n\n## ğŸ¯ æ©Ÿèƒ½\n- **DeepSeek R1ã«ã‚ˆã‚‹é«˜å“è³ªæ•°å­¦æ¨è«–ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ**\n- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°**\n- **Alignãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå—œå¥½ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼‰**\n- **HLEå¯¾ç­–ç‰¹åŒ–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**\n- **Chain-of-Thoughtæ¨è«–ã‚µãƒãƒ¼ãƒˆ**'),
    
    nbf.v4.new_markdown_cell('## âš™ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šå¤‰æ•°\n\nä»¥ä¸‹ã®å¤‰æ•°ã‚’å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ã—ã¦ãã ã•ã„ï¼š'),
    
    nbf.v4.new_code_cell('# =============================================================================\n# ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šå¤‰æ•° - å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ã—ã¦ãã ã•ã„\n# =============================================================================\n\n# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š\nDATASET_NAME = "HLE-Math-Reasoning-Dataset"  # ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå\nOUTPUT_DIR = "./data"  # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\nTOTAL_PROBLEMS = 1000  # ç”Ÿæˆã™ã‚‹å•é¡Œæ•°\n\n# ãƒ¢ãƒ‡ãƒ«è¨­å®š\nMODEL_PATH = "deepseek-ai/DeepSeek-R1"  # ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆDeepSeek R1æ¨å¥¨ï¼‰\nDEVICE = "0"  # GPU ãƒ‡ãƒã‚¤ã‚¹ç•ªå·\nTENSOR_PARALLEL = 4  # ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—æ•°ï¼ˆDeepSeek R1ç”¨ï¼‰\nGPU_MEMORY_UTILIZATION = 0.90  # GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡\n\n# ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nINSTRUCTION_TEMPERATURE = 1.2  # å•é¡Œç”Ÿæˆã®å‰µé€ æ€§\nINSTRUCTION_TOP_P = 1.0  # å•é¡Œç”Ÿæˆã®å¤šæ§˜æ€§\nRESPONSE_TEMPERATURE = 0.1  # è§£ç­”ç”Ÿæˆã®ä¸€è²«æ€§\nRESPONSE_TOP_P = 1.0  # è§£ç­”ç”Ÿæˆã®ãƒãƒ©ãƒ³ã‚¹\n\n# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š\nMIN_INSTRUCTION_LENGTH = 20  # æœ€å°å•é¡Œæ–‡é•·\nMIN_RESPONSE_LENGTH = 50  # æœ€å°è§£ç­”é•·\nMAX_INSTRUCTION_LENGTH = 1000  # æœ€å¤§å•é¡Œæ–‡é•·\nMAX_RESPONSE_LENGTH = 4000  # æœ€å¤§è§£ç­”é•·\nQUALITY_THRESHOLD = 0.7  # å“è³ªã‚¹ã‚³ã‚¢é–¾å€¤\n\n# Alignãƒ‡ãƒ¼ã‚¿ç”Ÿæˆè¨­å®š\nENABLE_ALIGN_DATA = True  # Alignãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚’æœ‰åŠ¹ã«ã™ã‚‹\nALIGN_CANDIDATES = 4  # Alignãƒ‡ãƒ¼ã‚¿ç”¨å€™è£œè§£ç­”æ•°\nALIGN_TEMPERATURE = 0.8  # Alignãƒ‡ãƒ¼ã‚¿å€™è£œç”Ÿæˆæ¸©åº¦\n\n# HLEç‰¹åŒ–è¨­å®š\nMATH_DOMAINS = ["algebra", "calculus", "geometry", "statistics", "number_theory"]  # å¯¾è±¡æ•°å­¦åˆ†é‡\nDIFFICULTY_LEVELS = ["intermediate", "advanced", "expert"]  # é›£æ˜“åº¦ãƒ¬ãƒ™ãƒ«\n\nprint(f"ğŸ“Š è¨­å®šå®Œäº†: {DATASET_NAME} - {TOTAL_PROBLEMS}å•é¡Œã‚’ç”Ÿæˆ")\nprint(f"ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {MODEL_PATH}")\nprint(f"ğŸ¯ å¯¾è±¡åˆ†é‡: {\", \".join(MATH_DOMAINS)}")'),
    
    nbf.v4.new_markdown_cell('## ğŸ“¦ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ'),
    
    nbf.v4.new_code_cell('import os\nimport sys\nimport json\nimport time\nimport random\nimport subprocess\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict, Any, Optional\n\n# Magpieé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\nsys.path.insert(0, os.path.abspath("."))\nsys.path.insert(0, os.path.abspath("./exp"))\n\n# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™å®Œäº†: {OUTPUT_DIR}")'),
    
    nbf.v4.new_markdown_cell('## ğŸ”§ ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°'),
    
    nbf.v4.new_code_cell('def create_job_name(prefix="HLE-Math"):\n    """ã‚¸ãƒ§ãƒ–åã‚’ç”Ÿæˆã™ã‚‹"""\n    timestamp = int(time.time())\n    return f"{prefix}_{TOTAL_PROBLEMS}_{timestamp}"\n\ndef setup_logging(job_name):\n    """ãƒ­ã‚°è¨­å®šã‚’è¡Œã†"""\n    log_dir = Path(OUTPUT_DIR) / job_name\n    log_dir.mkdir(parents=True, exist_ok=True)\n    return log_dir\n\ndef run_magpie_generation(script_type, **kwargs):\n    """Magpieç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹"""\n    try:\n        if script_type == "instruction":\n            cmd = [\n                "python", "exp/gen_ins.py",\n                "--model_path", kwargs.get("model_path", MODEL_PATH),\n                "--total_prompts", str(kwargs.get("total_prompts", TOTAL_PROBLEMS)),\n                "--temperature", str(kwargs.get("temperature", INSTRUCTION_TEMPERATURE)),\n                "--top_p", str(kwargs.get("top_p", INSTRUCTION_TOP_P)),\n                "--tensor_parallel_size", str(kwargs.get("tensor_parallel_size", TENSOR_PARALLEL)),\n                "--gpu_memory_utilization", str(kwargs.get("gpu_memory_utilization", GPU_MEMORY_UTILIZATION)),\n                "--control_tasks", "math",\n                "--device", str(kwargs.get("device", DEVICE)),\n                "--job_name", kwargs.get("job_name", "default"),\n                "--timestamp", str(kwargs.get("timestamp", int(time.time()))),\n                "--n", str(kwargs.get("n", 50)),\n                "--max_tokens", str(kwargs.get("max_tokens", 3072)),\n                "--max_model_len", str(kwargs.get("max_model_len", 8192))\n            ]\n        elif script_type == "response":\n            cmd = [\n                "python", "exp/gen_res.py",\n                "--model_path", kwargs.get("model_path", MODEL_PATH),\n                "--input_file", kwargs.get("input_file", ""),\n                "--temperature", str(kwargs.get("temperature", RESPONSE_TEMPERATURE)),\n                "--top_p", str(kwargs.get("top_p", RESPONSE_TOP_P)),\n                "--tensor_parallel_size", str(kwargs.get("tensor_parallel_size", TENSOR_PARALLEL)),\n                "--gpu_memory_utilization", str(kwargs.get("gpu_memory_utilization", GPU_MEMORY_UTILIZATION)),\n                "--device", str(kwargs.get("device", DEVICE)),\n                "--batch_size", str(kwargs.get("batch_size", 50)),\n                "--repetition_penalty", str(kwargs.get("repetition_penalty", 1.0)),\n                "--use_tokenizer_template",\n                "--offline",\n                "--max_tokens", str(kwargs.get("max_tokens", 4096))\n            ]\n        else:\n            raise ValueError(f"æœªçŸ¥ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¿ã‚¤ãƒ—: {script_type}")\n        \n        print(f"ğŸš€ å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {\" \".join(cmd)}")\n        result = subprocess.run(cmd, capture_output=True, text=True, cwd=".")\n        \n        if result.returncode == 0:\n            print(f"âœ… {script_type}ç”ŸæˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")\n            return True, result.stdout\n        else:\n            print(f"âŒ {script_type}ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")\n            print(f"ã‚¨ãƒ©ãƒ¼å‡ºåŠ›: {result.stderr}")\n            return False, result.stderr\n            \n    except Exception as e:\n        print(f"âŒ {script_type}ç”Ÿæˆã§ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")\n        return False, str(e)\n\nprint("ğŸ”§ ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ãŒæº–å‚™å®Œäº†")'),
    
    nbf.v4.new_markdown_cell('## ğŸš€ Step 1: HLEæ•°å­¦å•é¡Œç”Ÿæˆ'),
    
    nbf.v4.new_code_cell('print("ğŸ§® HLEæ•°å­¦å•é¡Œç”Ÿæˆã‚’é–‹å§‹...")\n\n# ã‚¸ãƒ§ãƒ–è¨­å®š\njob_name = create_job_name("DeepSeek-R1-HLE-Math")\nlog_dir = setup_logging(job_name)\ntimestamp = int(time.time())\n\nprint(f"ğŸ“ ã‚¸ãƒ§ãƒ–å: {job_name}")\nprint(f"ğŸ“ ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {log_dir}")\n\n# å•é¡Œç”Ÿæˆã®å®Ÿè¡Œ\ninstruction_success, instruction_output = run_magpie_generation(\n    "instruction",\n    model_path=MODEL_PATH,\n    total_prompts=TOTAL_PROBLEMS,\n    temperature=INSTRUCTION_TEMPERATURE,\n    top_p=INSTRUCTION_TOP_P,\n    tensor_parallel_size=TENSOR_PARALLEL,\n    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n    device=DEVICE,\n    job_name=job_name,\n    timestamp=timestamp,\n    n=50,\n    max_tokens=3072,\n    max_model_len=8192\n)\n\nif instruction_success:\n    # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™\n    model_name = MODEL_PATH.split("/")[-1]\n    instructions_file = log_dir / f"Magpie_{model_name}_{TOTAL_PROBLEMS}_{timestamp}_ins.json"\n    \n    if instructions_file.exists():\n        with open(instructions_file, "r", encoding="utf-8") as f:\n            instructions_data = json.load(f)\n        \n        print(f"âœ… æ•°å­¦å•é¡Œç”Ÿæˆå®Œäº†: {len(instructions_data)}å•é¡Œ")\n        print(f"ğŸ’¾ ä¿å­˜å…ˆ: {instructions_file}")\n    else:\n        print(f"âš ï¸ æœŸå¾…ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {instructions_file}")\n        files = list(log_dir.glob("*_ins.json"))\n        if files:\n            instructions_file = files[0]\n            print(f"ğŸ“ ä»£æ›¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {instructions_file}")\n        else:\n            instructions_file = None\nelse:\n    print(f"âŒ å•é¡Œç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {instruction_output}")\n    instructions_file = None'),
    
    nbf.v4.new_markdown_cell('## ğŸ¯ Step 2: æ•°å­¦è§£ç­”ç”Ÿæˆ'),
    
    nbf.v4.new_code_cell('if instructions_file and instructions_file.exists():\n    print("ğŸ¯ HLEæ•°å­¦è§£ç­”ç”Ÿæˆã‚’é–‹å§‹...")\n    \n    # è§£ç­”ç”Ÿæˆã®å®Ÿè¡Œ\n    response_success, response_output = run_magpie_generation(\n        "response",\n        model_path=MODEL_PATH,\n        input_file=str(instructions_file),\n        temperature=RESPONSE_TEMPERATURE,\n        top_p=RESPONSE_TOP_P,\n        tensor_parallel_size=TENSOR_PARALLEL,\n        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n        device=DEVICE,\n        batch_size=50,\n        repetition_penalty=1.0,\n        max_tokens=4096\n    )\n    \n    if response_success:\n        # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™\n        responses_files = list(log_dir.glob("*_ins_res.json"))\n        if responses_files:\n            responses_file = responses_files[0]\n            \n            with open(responses_file, "r", encoding="utf-8") as f:\n                responses_data = json.load(f)\n            \n            print(f"âœ… æ•°å­¦è§£ç­”ç”Ÿæˆå®Œäº†: {len(responses_data)}è§£ç­”")\n            print(f"ğŸ’¾ ä¿å­˜å…ˆ: {responses_file}")\n        else:\n            print("âš ï¸ è§£ç­”ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")\n            responses_file = None\n    else:\n        print(f"âŒ è§£ç­”ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {response_output}")\n        responses_file = None\nelse:\n    print("âš ï¸ å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Step 1ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")\n    responses_file = None'),
    
    nbf.v4.new_markdown_cell('## ğŸ” Step 3: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå“è³ªåˆ†æã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°'),
    
    nbf.v4.new_code_cell('def validate_dataset(data):\n    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å“è³ªã‚’æ¤œè¨¼ã™ã‚‹"""\n    if not data:\n        return False, "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç©ºã§ã™"\n    \n    valid_count = 0\n    issues = []\n    \n    for i, item in enumerate(data):\n        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª\n        if "instruction" not in item or "response" not in item:\n            issues.append(f"é …ç›® {i}: å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒä¸è¶³")\n            continue\n            \n        # é•·ã•ã®ç¢ºèª\n        inst_len = len(item["instruction"])\n        resp_len = len(item["response"])\n        \n        if inst_len < MIN_INSTRUCTION_LENGTH or inst_len > MAX_INSTRUCTION_LENGTH:\n            issues.append(f"é …ç›® {i}: å•é¡Œæ–‡é•·ãŒç¯„å›²å¤– ({inst_len})")\n            continue\n            \n        if resp_len < MIN_RESPONSE_LENGTH or resp_len > MAX_RESPONSE_LENGTH:\n            issues.append(f"é …ç›® {i}: è§£ç­”é•·ãŒç¯„å›²å¤– ({resp_len})")\n            continue\n            \n        valid_count += 1\n    \n    success_rate = valid_count / len(data) if data else 0\n    return success_rate >= QUALITY_THRESHOLD, f"æœ‰åŠ¹ç‡: {success_rate:.2%}, å•é¡Œ: {len(issues)}"\n\ndef filter_dataset(data):\n    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹"""\n    filtered_data = []\n    \n    for item in data:\n        if ("instruction" in item and "response" in item and\n            MIN_INSTRUCTION_LENGTH <= len(item["instruction"]) <= MAX_INSTRUCTION_LENGTH and\n            MIN_RESPONSE_LENGTH <= len(item["response"]) <= MAX_RESPONSE_LENGTH):\n            filtered_data.append(item)\n    \n    return filtered_data\n\nif responses_file and responses_file.exists():\n    print("ğŸ” ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå“è³ªåˆ†æã‚’é–‹å§‹...")\n    \n    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n    with open(responses_file, "r", encoding="utf-8") as f:\n        raw_dataset = json.load(f)\n    \n    print(f"ğŸ“Š ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(raw_dataset)}é …ç›®")\n    \n    # å“è³ªæ¤œè¨¼\n    is_valid, validation_msg = validate_dataset(raw_dataset)\n    print(f"ğŸ” å“è³ªæ¤œè¨¼çµæœ: {validation_msg}")\n    \n    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ\n    print("\\nğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")\n    filtered_dataset = filter_dataset(raw_dataset)\n    \n    print(f"âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Œäº†: {len(raw_dataset)} â†’ {len(filtered_dataset)}é …ç›®")\n    \n    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜\n    filtered_file = log_dir / f"Magpie_{MODEL_PATH.split(\"/\")[-1]}_filtered_{len(filtered_dataset)}.json"\n    with open(filtered_file, "w", encoding="utf-8") as f:\n        json.dump(filtered_dataset, f, ensure_ascii=False, indent=2)\n    \n    print(f"ğŸ’¾ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜: {filtered_file}")\n    \nelse:\n    print("âš ï¸ è§£ç­”ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Step 2ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")\n    filtered_dataset = None\n    filtered_file = None'),
    
    nbf.v4.new_markdown_cell('## ğŸ¯ Step 4: Alignãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå—œå¥½ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼‰'),
    
    nbf.v4.new_code_cell('def generate_align_candidates(instruction, num_candidates=ALIGN_CANDIDATES):\n    """Alignãƒ‡ãƒ¼ã‚¿ç”¨ã®è¤‡æ•°å€™è£œè§£ç­”ã‚’ç”Ÿæˆã™ã‚‹ï¼ˆæœ¬ç•ªå®Ÿè£…ï¼‰"""\n    candidates = []\n    \n    # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€VLLMã‚’ä½¿ç”¨ã—ã¦ç•°ãªã‚‹æ¸©åº¦è¨­å®šã§è¤‡æ•°ã®è§£ç­”ã‚’ç”Ÿæˆ\n    for i in range(num_candidates):\n        # æ¸©åº¦ã‚’å¤‰ãˆã¦å¤šæ§˜ãªè§£ç­”ã‚’ç”Ÿæˆ\n        temp = ALIGN_TEMPERATURE + (i * 0.1)\n        \n        # æœ¬ç•ªå®Ÿè£…ã§ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«VLLMã‚’ä½¿ç”¨\n        # try:\n        #     llm = LLM(model=MODEL_PATH, tensor_parallel_size=TENSOR_PARALLEL)\n        #     sampling_params = SamplingParams(temperature=temp, top_p=RESPONSE_TOP_P, max_tokens=4096)\n        #     outputs = llm.generate([instruction], sampling_params)\n        #     candidate_text = outputs[0].outputs[0].text\n        # except Exception as e:\n        #     print(f"å€™è£œç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")\n        #     candidate_text = f"ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}"\n        \n        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ä¸Šè¨˜ã®VLLMã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„\n        candidate_text = f"æ•°å­¦å•é¡Œã®è§£ç­”å€™è£œ {i+1}: {instruction[:100]}... ã«å¯¾ã™ã‚‹è©³ç´°ãªè§£ç­”"\n        \n        # å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ãªè©•ä¾¡æŒ‡æ¨™ã‚’ä½¿ç”¨ï¼‰\n        quality_score = max(0.1, 1.0 - (i * 0.2) + random.uniform(-0.05, 0.05))\n        \n        candidates.append({\n            "response": candidate_text,\n            "temperature": temp,\n            "quality_score": quality_score,\n            "candidate_id": i\n        })\n    \n    return candidates\n\ndef create_align_pairs(candidates):\n    """Alignãƒ‡ãƒ¼ã‚¿ç”¨ã®preferred/rejectedãƒšã‚¢ã‚’ä½œæˆã™ã‚‹"""\n    # å“è³ªã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ\n    sorted_candidates = sorted(candidates, key=lambda x: x["quality_score"], reverse=True)\n    \n    pairs = []\n    for i in range(len(sorted_candidates) - 1):\n        pairs.append({\n            "preferred": sorted_candidates[i],\n            "rejected": sorted_candidates[i + 1],\n            "preference_strength": sorted_candidates[i]["quality_score"] - sorted_candidates[i + 1]["quality_score"]\n        })\n    \n    return pairs\n\nif ENABLE_ALIGN_DATA and filtered_dataset:\n    print("ğŸ¯ Alignãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå—œå¥½ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼‰ã‚’é–‹å§‹...")\n    \n    # Alignãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ\n    align_dataset = []\n    \n    # å‡¦ç†ã™ã‚‹å•é¡Œæ•°ã‚’åˆ¶é™ï¼ˆæœ¬ç•ªã§ã¯å…¨å•é¡Œã‚’å‡¦ç†ï¼‰\n    sample_size = min(50, len(filtered_dataset))  # ãƒ‡ãƒ¢ã§ã¯50å•ã€æœ¬ç•ªã§ã¯å…¨å•é¡Œ\n    print(f"âš¡ {sample_size}å•é¡Œã«å¯¾ã—ã¦Alignãƒ‡ãƒ¼ã‚¿å€™è£œã‚’ç”Ÿæˆä¸­...")\n    \n    for item in tqdm(filtered_dataset[:sample_size]):\n        instruction = item["instruction"]\n        \n        try:\n            # è¤‡æ•°å€™è£œã‚’ç”Ÿæˆ\n            candidates = generate_align_candidates(instruction)\n            \n            # Alignãƒ‡ãƒ¼ã‚¿ãƒšã‚¢ã‚’ä½œæˆ\n            align_pairs = create_align_pairs(candidates)\n            \n            # Alignãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«è¿½åŠ \n            for pair in align_pairs:\n                align_item = {\n                    "instruction": instruction,\n                    "preferred_response": pair["preferred"]["response"],\n                    "rejected_response": pair["rejected"]["response"],\n                    "preference_strength": pair["preference_strength"],\n                    "preferred_quality": pair["preferred"]["quality_score"],\n                    "rejected_quality": pair["rejected"]["quality_score"],\n                    "preferred_temperature": pair["preferred"]["temperature"],\n                    "rejected_temperature": pair["rejected"]["temperature"],\n                    "domain": item.get("domain", "unknown"),\n                    "timestamp": int(time.time())\n                }\n                align_dataset.append(align_item)\n                \n        except Exception as e:\n            print(f"âš ï¸ å•é¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")\n            continue\n    \n    print(f"âœ… Alignãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆå®Œäº†: {len(align_dataset)}ãƒšã‚¢")\n    \n    # Alignãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜\n    align_file = log_dir / f"Magpie_{MODEL_PATH.split(\"/\")[-1]}_Align_{len(align_dataset)}.json"\n    with open(align_file, "w", encoding="utf-8") as f:\n        json.dump(align_dataset, f, ensure_ascii=False, indent=2)\n    \n    print(f"ğŸ’¾ Alignãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜: {align_file}")\n    \nelse:\n    if not ENABLE_ALIGN_DATA:\n        print("â„¹ï¸ Alignãƒ‡ãƒ¼ã‚¿ç”ŸæˆãŒç„¡åŠ¹ã«ãªã£ã¦ã„ã¾ã™ã€‚ENABLE_ALIGN_DATA = True ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")\n    else:\n        print("âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Step 3ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")\n    align_dataset = None\n    align_file = None'),
    
    nbf.v4.new_markdown_cell('## ğŸ“Š Step 5: æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã¨çµ±åˆ'),
    
    nbf.v4.new_code_cell('print("ğŸ“Š HLEæ•°å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ - æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ")\nprint("=" * 60)\n\n# è¨­å®šæƒ…å ±\nprint(f"ğŸ¯ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå: {DATASET_NAME}")\nprint(f"ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {MODEL_PATH}")\nprint(f"ğŸ“… ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}")\nprint(f"ğŸ² ç›®æ¨™å•é¡Œæ•°: {TOTAL_PROBLEMS}")\n\n# ç”Ÿæˆçµæœã‚µãƒãƒªãƒ¼\nprint("\\nğŸ“ˆ ç”Ÿæˆçµæœã‚µãƒãƒªãƒ¼:")\nif "filtered_dataset" in locals() and filtered_dataset:\n    print(f"  âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿å•é¡Œæ•°: {len(filtered_dataset)}")\n    \n    if "align_dataset" in locals() and align_dataset:\n        print(f"  âœ… Alignãƒ‡ãƒ¼ã‚¿ãƒšã‚¢æ•°: {len(align_dataset)}")\n        print(f"  ğŸ“Š å¹³å‡é¸å¥½å¼·åº¦: {np.mean([item[\"preference_strength\"] for item in align_dataset]):.3f}")\n    else:\n        print("  âš ï¸ Alignãƒ‡ãƒ¼ã‚¿ã¯ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")\nelse:\n    print("  âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")\n\n# ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚µãƒãƒªãƒ¼\nprint("\\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")\nif "log_dir" in locals():\n    output_files = list(log_dir.glob("*.json"))\n    for file in output_files:\n        print(f"  ğŸ“„ {file.name}")\n\nprint("\\nğŸ‰ HLEæ•°å­¦æ¨è«–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ï¼")\nprint("\\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")\nprint("  1. ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å“è³ªã‚’ç¢ºèª")\nprint("  2. å¿…è¦ã«å¿œã˜ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®šã‚’èª¿æ•´")\nprint("  3. Alignãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã‚’å®Ÿè¡Œ")\nprint("  4. HLEå¯¾ç­–ç”¨ã®è©•ä¾¡æŒ‡æ¨™ã§ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’æ¸¬å®š")')
]

nb.cells = cells

with open('demo_production.ipynb', 'w', encoding='utf-8') as f:
    nbf.write(nb, f)

print('âœ… æœ¬ç•ªç”¨Jupyter Notebookä½œæˆå®Œäº†')
