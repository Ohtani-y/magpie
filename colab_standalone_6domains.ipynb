{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_header"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/your-repo/magpie/blob/main/colab_standalone_6domains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# 🧮 Magpie: 6ドメイン数学データセット生成 (単独実行版)\n",
    "\n",
    "このノートブックは、**外部ファイルを一切使用せずに**DeepSeek R1を使用して6つの数学ドメイン別データセットを生成・統合する完全単独実行版です。\n",
    "\n",
    "## 🎯 特徴\n",
    "- ✅ **完全単独実行**: 外部リポジトリ・ファイル不要\n",
    "- ✅ **全機能内蔵**: 生成・統合・分析・ダウンロード\n",
    "- ✅ **GPU自動最適化**: T4/A100両対応\n",
    "- ✅ **エラー回復**: 堅牢なエラーハンドリング\n",
    "\n",
    "## 📊 対応ドメイン\n",
    "1. **Algebra** (代数学): 方程式、多項式、関数\n",
    "2. **Applied Mathematics** (応用数学): 微分方程式、最適化\n",
    "3. **Calculus** (微積分学): 微積分、極限、級数\n",
    "4. **Discrete Mathematics** (離散数学): 組合せ、グラフ理論\n",
    "5. **Geometry** (幾何学): 解析幾何、空間図形\n",
    "6. **Number Theory** (数論): 素数、合同式、暗号応用\n",
    "\n",
    "## ⚠️ 重要事項\n",
    "- **GPU必須**: A100推奨、T4でも動作可能\n",
    "- **実行時間**: 30分-3時間（設定による）\n",
    "- **メモリ制限**: 自動調整機能付き"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "settings_header"
   },
   "source": [
    "## ⚙️ ユーザー設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "user_settings"
   },
   "outputs": [],
   "source": [
    "# ===== ユーザー設定 =====\n",
    "\n",
    "# 基本設定\n",
    "DATASET_NAME = \"HLE_6Domains_Math_Standalone\"  # データセット名\n",
    "PROBLEMS_PER_DOMAIN = 15  # 各ドメインの問題数（Colab用に調整）\n",
    "OUTPUT_DIR = \"/content/magpie_output\"  # 出力ディレクトリ\n",
    "\n",
    "# モデル設定（GPU能力に応じて自動選択）\n",
    "AUTO_MODEL_SELECTION = True  # True: GPU性能で自動選択, False: 手動選択\n",
    "\n",
    "# 手動選択時の設定\n",
    "MANUAL_USE_LIGHTWEIGHT = True  # TrueでQwen2.5-3B、FalseでDeepSeek R1\n",
    "\n",
    "# 生成パラメータ\n",
    "INSTRUCTION_TEMP = 1.2\n",
    "INSTRUCTION_TOP_P = 1.0\n",
    "RESPONSE_TEMP = 0.1\n",
    "RESPONSE_TOP_P = 1.0\n",
    "\n",
    "# 対象ドメイン（カスタマイズ可能）\n",
    "DOMAINS = [\n",
    "    \"algebra\",\n",
    "    \"applied-mathematics\", \n",
    "    \"calculus\",\n",
    "    \"discrete-mathematics\",\n",
    "    \"geometry\",\n",
    "    \"number-theory\"\n",
    "]\n",
    "\n",
    "# デバッグ・開発用設定\n",
    "DEBUG_MODE = False  # True: 詳細ログ出力\n",
    "ENABLE_QUALITY_ANALYSIS = True  # 品質分析実行\n",
    "CREATE_VISUALIZATION = True  # グラフ作成\n",
    "\n",
    "print(f\"📊 設定完了:\")\n",
    "print(f\"  データセット名: {DATASET_NAME}\")\n",
    "print(f\"  各ドメイン問題数: {PROBLEMS_PER_DOMAIN}\")\n",
    "print(f\"  総問題数: {len(DOMAINS) * PROBLEMS_PER_DOMAIN}\")\n",
    "print(f\"  対象ドメイン: {len(DOMAINS)}個\")\n",
    "print(f\"  自動モデル選択: {'有効' if AUTO_MODEL_SELECTION else '無効'}\")\n",
    "print(f\"  出力先: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## 🚀 環境セットアップ・依存関係インストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dependencies_install"
   },
   "outputs": [],
   "source": [
    "# 依存関係インストール\n",
    "print(\"📦 依存関係インストール中...\")\n",
    "\n",
    "!pip install -q vllm==0.6.5\n",
    "!pip install -q transformers>=4.36.0\n",
    "!pip install -q torch>=2.0.0\n",
    "!pip install -q accelerate\n",
    "!pip install -q datasets\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q tiktoken\n",
    "!pip install -q numpy pandas tqdm\n",
    "!pip install -q matplotlib seaborn\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "print(\"✅ 依存関係インストール完了\")\n",
    "\n",
    "# 必要なモジュールインポート\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# GPU確認\n",
    "import torch\n",
    "print(f\"\\n🔍 システム情報:\")\n",
    "print(f\"  CUDA利用可能: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"  GPU名: {gpu_name}\")\n",
    "    print(f\"  GPUメモリ: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"  ⚠️ GPU not available - CPU mode will be very slow\")\n",
    "\n",
    "# 出力ディレクトリ作成\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"\\n📁 出力ディレクトリ作成: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_config_header"
   },
   "source": [
    "## 🤖 モデル設定・自動最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_configuration"
   },
   "outputs": [],
   "source": [
    "# GPU性能に基づく自動モデル選択\n",
    "def auto_select_model_config():\n",
    "    \"\"\"GPU性能に基づいてモデル設定を自動選択\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return {\n",
    "            'model_path': 'Qwen/Qwen2.5-1.5B-Instruct',\n",
    "            'tensor_parallel': 1,\n",
    "            'gpu_memory_util': 0.70,\n",
    "            'batch_size': 2,\n",
    "            'problems_per_domain': 5,\n",
    "            'mode': 'CPU (非推奨)'\n",
    "        }\n",
    "    \n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    gpu_name = torch.cuda.get_device_name(0).lower()\n",
    "    \n",
    "    if 'a100' in gpu_name or gpu_memory > 70:\n",
    "        # A100 or high-end GPU\n",
    "        return {\n",
    "            'model_path': 'deepseek-ai/DeepSeek-R1',\n",
    "            'tensor_parallel': 2,\n",
    "            'gpu_memory_util': 0.90,\n",
    "            'batch_size': 8,\n",
    "            'problems_per_domain': PROBLEMS_PER_DOMAIN,\n",
    "            'mode': 'A100 フルモード'\n",
    "        }\n",
    "    elif 'v100' in gpu_name or gpu_memory > 30:\n",
    "        # V100 or mid-range GPU\n",
    "        return {\n",
    "            'model_path': 'Qwen/Qwen2.5-7B-Instruct',\n",
    "            'tensor_parallel': 1,\n",
    "            'gpu_memory_util': 0.85,\n",
    "            'batch_size': 6,\n",
    "            'problems_per_domain': PROBLEMS_PER_DOMAIN,\n",
    "            'mode': 'V100 中設定'\n",
    "        }\n",
    "    else:\n",
    "        # T4 or low-end GPU\n",
    "        return {\n",
    "            'model_path': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "            'tensor_parallel': 1,\n",
    "            'gpu_memory_util': 0.80,\n",
    "            'batch_size': 4,\n",
    "            'problems_per_domain': min(PROBLEMS_PER_DOMAIN, 20),\n",
    "            'mode': 'T4 軽量モード'\n",
    "        }\n",
    "\n",
    "# モデル設定決定\n",
    "if AUTO_MODEL_SELECTION:\n",
    "    config = auto_select_model_config()\n",
    "    print(f\"🤖 自動選択モード: {config['mode']}\")\nelse:\n",
    "    if MANUAL_USE_LIGHTWEIGHT:\n",
    "        config = {\n",
    "            'model_path': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "            'tensor_parallel': 1,\n",
    "            'gpu_memory_util': 0.80,\n",
    "            'batch_size': 4,\n",
    "            'problems_per_domain': PROBLEMS_PER_DOMAIN,\n",
    "            'mode': '手動 軽量設定'\n",
    "        }\n",
    "    else:\n",
    "        config = {\n",
    "            'model_path': 'deepseek-ai/DeepSeek-R1',\n",
    "            'tensor_parallel': 2,\n",
    "            'gpu_memory_util': 0.90,\n",
    "            'batch_size': 8,\n",
    "            'problems_per_domain': PROBLEMS_PER_DOMAIN,\n",
    "            'mode': '手動 フル設定'\n",
    "        }\n",
    "    print(f\"🎛️ 手動選択モード: {config['mode']}\")\n\n# 設定表示\nMODEL_PATH = config['model_path']\nTENSOR_PARALLEL = config['tensor_parallel']\nGPU_MEMORY_UTIL = config['gpu_memory_util']\nBATCH_SIZE = config['batch_size']\nACTUAL_PROBLEMS_PER_DOMAIN = config['problems_per_domain']\n\nprint(f\"\\n📋 最終設定:\")\nprint(f\"  モデル: {MODEL_PATH}\")\nprint(f\"  テンソル並列: {TENSOR_PARALLEL}\")\nprint(f\"  GPU使用率: {GPU_MEMORY_UTIL}\")\nprint(f\"  バッチサイズ: {BATCH_SIZE}\")\nprint(f\"  各ドメイン問題数: {ACTUAL_PROBLEMS_PER_DOMAIN}\")\nprint(f\"  総問題数: {len(DOMAINS) * ACTUAL_PROBLEMS_PER_DOMAIN}\")\n\n# 認証チェック（DeepSeek R1使用時）\nif 'deepseek' in MODEL_PATH.lower():\n    print(\"\\n🔐 DeepSeek R1認証が必要です\")\n    print(\"以下のセルで認証を行ってください\")\nelse:\n    print(\"\\n✅ 認証不要モデル選択済み\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auth_setup"
   },
   "outputs": [],
   "source": [
    "# Hugging Face認証（DeepSeek R1使用時のみ）\n",
    "if 'deepseek' in MODEL_PATH.lower():\n",
    "    print(\"🔐 DeepSeek R1認証を実行してください\")\n",
    "    print(\"https://huggingface.co/settings/tokens でトークンを取得\")\n",
    "    \n",
    "    # 手動認証 - 以下のコメントを外して実行\n",
    "    # from huggingface_hub import login\n",
    "    # login()  # トークンを入力\n",
    "    \n",
    "    print(\"⚠️ 上記のコメントアウトを外して認証してください\")\n",
    "    print(\"認証後に次のセルに進んでください\")\nelse:\n",
    "    print(\"✅ 軽量モデル使用のため認証不要\")\n    print(\"そのまま次のセルに進んでください\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "embedded_code_header"
   },
   "source": [
    "## 🛠️ 組み込みコード定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "embedded_core_functions"
   },
   "outputs": [],
   "source": [
    "# ===== 組み込みコア関数 =====\n",
    "\n",
    "# モデル設定テンプレート（内蔵）\n",
    "MODEL_CONFIGS = {\n",
    "    \"deepseek-ai/DeepSeek-R1\": {\n",
    "        \"model_name\": \"deepseek-ai/DeepSeek-R1\",\n",
    "        \"stop_tokens\": [\"<｜begin▁of▁sentence｜>\", \"<｜end▁of▁sentence｜>\", \"<｜end▁of▁text｜>\", \"<｜im_start｜>\", \"<｜im_end｜>\"],\n",
    "        \"stop_token_ids\": [100001, 100002, 100003, 100011, 100012],\n",
    "        \"pre_query_template_math\": \"<｜im_start｜>system\\nYou are an advanced AI assistant specialized in mathematical reasoning and problem-solving for high-level examinations (HLE). Your expertise covers algebra, calculus, geometry, statistics, and number theory.\\n\\nWhen solving mathematical problems, you should:\\n1. Carefully analyze the given problem and identify key information\\n2. Choose the most appropriate solution method\\n3. Show detailed step-by-step reasoning using Chain-of-Thought approach\\n4. Verify your solution and explain the reasoning behind each step\\n5. Provide insights that help understand the underlying mathematical concepts\\n\\nYour responses should demonstrate clear logical thinking, mathematical rigor, and educational value suitable for advanced mathematics preparation.<｜im_end｜>\\n<｜im_start｜>user\\n\",\n",
    "        \"pre_query_template\": \"<｜im_start｜>user\\n\"\n",
    "    },\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\": {\n",
    "        \"model_name\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "        \"stop_tokens\": [\"<|im_start|>\", \"<|im_end|>\", \"<|endoftext|>\"],\n",
    "        \"stop_token_ids\": [151643, 151644, 151645],\n",
    "        \"pre_query_template_math\": \"<|im_start|>system\\nYou are an AI assistant specialized in mathematical reasoning and problem-solving. Your expertise covers algebra, calculus, geometry, statistics, and number theory. Provide clear, step-by-step solutions with detailed explanations.<|im_end|>\\n<|im_start|>user\\n\",\n",
    "        \"pre_query_template\": \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n\"\n",
    "    },\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\": {\n",
    "        \"model_name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"stop_tokens\": [\"<|im_start|>\", \"<|im_end|>\", \"<|endoftext|>\"],\n",
    "        \"stop_token_ids\": [151643, 151644, 151645],\n",
    "        \"pre_query_template_math\": \"<|im_start|>system\\nYou are an AI assistant specialized in mathematical reasoning and problem-solving. Your expertise covers algebra, calculus, geometry, statistics, and number theory. Provide clear, step-by-step solutions with detailed explanations.<|im_end|>\\n<|im_start|>user\\n\",\n",
    "        \"pre_query_template\": \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n\"\n",
    "    },\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\": {\n",
    "        \"model_name\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        \"stop_tokens\": [\"<|im_start|>\", \"<|im_end|>\", \"<|endoftext|>\"],\n",
    "        \"stop_token_ids\": [151643, 151644, 151645],\n",
    "        \"pre_query_template_math\": \"<|im_start|>system\\nYou are an AI assistant specialized in mathematical reasoning and problem-solving. Provide clear, step-by-step solutions with detailed explanations.<|im_end|>\\n<|im_start|>user\\n\",\n",
    "        \"pre_query_template\": \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_model_config(model_path, use_math_template=True):\n",
    "    \"\"\"モデル設定を取得\"\"\"\n",
    "    if model_path not in MODEL_CONFIGS:\n",
    "        raise ValueError(f\"Unsupported model: {model_path}\")\n",
    "    \n",
    "    config = MODEL_CONFIGS[model_path]\n",
    "    template_key = \"pre_query_template_math\" if use_math_template else \"pre_query_template\"\n",
    "    \n",
    "    return {\n",
    "        'pre_query_template': config.get(template_key, config['pre_query_template']),\n",
    "        'stop_tokens': config['stop_tokens'],\n",
    "        'stop_token_ids': config.get('stop_token_ids', [])\n",
    "    }\n",
    "\n",
    "def log_message(message, level=\"INFO\"):\n",
    "    \"\"\"ログ出力\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    if DEBUG_MODE or level in [\"ERROR\", \"SUCCESS\"]:\n",
    "        prefix = {\"INFO\": \"ℹ️\", \"SUCCESS\": \"✅\", \"ERROR\": \"❌\", \"WARNING\": \"⚠️\"}[level]\n",
    "        print(f\"[{timestamp}] {prefix} {message}\")\n\nprint(\"✅ 組み込みコア関数定義完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "embedded_generation_functions"
   },
   "outputs": [],
   "source": [
    "# ===== データ生成関数 =====\n",
    "\n",
    "def generate_instructions_vllm(model_path, domain, num_problems, timestamp):\n",
    "    \"\"\"vLLMを使用して問題を生成\"\"\"\n",
    "    log_message(f\"Starting instruction generation for {domain}\")\n",
    "    \n",
    "    try:\n",
    "        from vllm import LLM, SamplingParams\n",
    "        \n",
    "        # モデル初期化\n",
    "        llm = LLM(\n",
    "            model=model_path,\n",
    "            tensor_parallel_size=TENSOR_PARALLEL,\n",
    "            gpu_memory_utilization=GPU_MEMORY_UTIL,\n",
    "            max_model_len=8192,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # モデル設定取得\n",
    "        model_config = get_model_config(model_path, use_math_template=True)\n",
    "        pre_query_template = model_config['pre_query_template']\n",
    "        \n",
    "        # サンプリングパラメータ\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=INSTRUCTION_TEMP,\n",
    "            top_p=INSTRUCTION_TOP_P,\n",
    "            max_tokens=3072,\n",
    "            stop=model_config['stop_tokens']\n",
    "        )\n",
    "        \n",
    "        # プロンプト準備（空プロンプトでinstruction生成）\n",
    "        prompts = [pre_query_template] * num_problems\n",
    "        \n",
    "        # 生成実行\n",
    "        log_message(f\"Generating {num_problems} problems for {domain}\")\n",
    "        outputs = llm.generate(prompts, sampling_params)\n",
    "        \n",
    "        # 結果整理\n",
    "        results = []\n",
    "        for i, output in enumerate(outputs):\n",
    "            instruction = output.outputs[0].text.strip()\n",
    "            \n",
    "            result = {\n",
    "                \"id\": i,\n",
    "                \"pre_query_template\": pre_query_template,\n",
    "                \"instruction\": instruction,\n",
    "                \"response\": None,\n",
    "                \"created\": timestamp,\n",
    "                \"gen_input_configs\": {\n",
    "                    \"temperature\": INSTRUCTION_TEMP,\n",
    "                    \"top_p\": INSTRUCTION_TOP_P,\n",
    "                    \"input_generator\": model_path,\n",
    "                    \"domain\": domain\n",
    "                },\n",
    "                \"gen_response_configs\": None\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        log_message(f\"Generated {len(results)} instructions for {domain}\", \"SUCCESS\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error in instruction generation for {domain}: {e}\", \"ERROR\")\n",
    "        return []\n",
    "\n",
    "def generate_responses_vllm(model_path, instructions_data):\n",
    "    \"\"\"vLLMを使用して解答を生成\"\"\"\n",
    "    log_message(\"Starting response generation\")\n",
    "    \n",
    "    try:\n",
    "        from vllm import LLM, SamplingParams\n",
    "        \n",
    "        # モデル初期化\n",
    "        llm = LLM(\n",
    "            model=model_path,\n",
    "            tensor_parallel_size=TENSOR_PARALLEL,\n",
    "            gpu_memory_utilization=GPU_MEMORY_UTIL,\n",
    "            max_model_len=8192,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # モデル設定取得\n",
    "        model_config = get_model_config(model_path, use_math_template=True)\n",
    "        \n",
    "        # サンプリングパラメータ\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=RESPONSE_TEMP,\n",
    "            top_p=RESPONSE_TOP_P,\n",
    "            max_tokens=4096,\n",
    "            stop=model_config['stop_tokens']\n",
    "        )\n",
    "        \n",
    "        # プロンプト準備\n",
    "        prompts = []\n",
    "        for item in instructions_data:\n",
    "            prompt = item['pre_query_template'] + item['instruction']\n",
    "            prompts.append(prompt)\n",
    "        \n",
    "        # バッチ処理で生成\n",
    "        results = []\n",
    "        for i in range(0, len(prompts), BATCH_SIZE):\n",
    "            batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "            batch_data = instructions_data[i:i+BATCH_SIZE]\n",
    "            \n",
    "            log_message(f\"Generating responses for batch {i//BATCH_SIZE + 1}\")\n",
    "            outputs = llm.generate(batch_prompts, sampling_params)\n",
    "            \n",
    "            for j, output in enumerate(outputs):\n",
    "                response = output.outputs[0].text.strip()\n",
    "                \n",
    "                # 元データをコピーして解答を追加\n",
    "                result = batch_data[j].copy()\n",
    "                result['response'] = response\n",
    "                result['gen_response_configs'] = {\n",
    "                    \"temperature\": RESPONSE_TEMP,\n",
    "                    \"top_p\": RESPONSE_TOP_P,\n",
    "                    \"output_generator\": model_path\n",
    "                }\n",
    "                results.append(result)\n",
    "        \n",
    "        log_message(f\"Generated {len(results)} responses\", \"SUCCESS\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error in response generation: {e}\", \"ERROR\")\n",
    "        return instructions_data  # 失敗時は元データを返す\n\nprint(\"✅ データ生成関数定義完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "embedded_analysis_functions"
   },
   "outputs": [],
   "source": [
    "# ===== データ分析・統合関数 =====\n",
    "\n",
    "def merge_domain_data(domain_datasets):\n",
    "    \"\"\"ドメイン別データを統合・シャッフル\"\"\"\n",
    "    log_message(\"Starting data merging and shuffling\")\n",
    "    \n",
    "    all_data = []\n",
    "    domain_stats = {}\n",
    "    \n",
    "    for domain, data in domain_datasets.items():\n",
    "        if not data:\n",
    "            continue\n",
    "            \n",
    "        # ドメインメタデータ追加\n",
    "        for item in data:\n",
    "            item['domain'] = domain\n",
    "            item['source'] = 'colab-standalone'\n",
    "            item['dataset_version'] = '1.0'\n",
    "        \n",
    "        all_data.extend(data)\n",
    "        domain_stats[domain] = len(data)\n",
    "        \n",
    "        log_message(f\"Added {len(data)} items from {domain}\")\n",
    "    \n",
    "    # シャッフル\n",
    "    random.seed(42)\n",
    "    random.shuffle(all_data)\n",
    "    \n",
    "    log_message(f\"Merged and shuffled {len(all_data)} total items\", \"SUCCESS\")\n",
    "    return all_data, domain_stats\n",
    "\n",
    "def create_sharegpt_format(data):\n",
    "    \"\"\"ShareGPT形式に変換\"\"\"\n",
    "    log_message(\"Converting to ShareGPT format\")\n",
    "    \n",
    "    sharegpt_data = []\n",
    "    for i, item in enumerate(data):\n",
    "        if not item.get('response'):\n",
    "            continue\n",
    "            \n",
    "        sharegpt_entry = {\n",
    "            \"conversation_id\": f\"colab-standalone-{i}\",\n",
    "            \"domain\": item.get('domain', 'unknown'),\n",
    "            \"source\": item.get('source', 'colab-standalone'),\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": item['instruction']},\n",
    "                {\"from\": \"gpt\", \"value\": item['response']}\n",
    "            ],\n",
    "            \"gen_input_configs\": item.get('gen_input_configs', {}),\n",
    "            \"gen_response_configs\": item.get('gen_response_configs', {}),\n",
    "            \"created\": item.get('created', ''),\n",
    "            \"id\": item.get('id', i)\n",
    "        }\n",
    "        sharegpt_data.append(sharegpt_entry)\n",
    "    \n",
    "    log_message(f\"Created ShareGPT format with {len(sharegpt_data)} entries\", \"SUCCESS\")\n",
    "    return sharegpt_data\n",
    "\n",
    "def analyze_dataset_quality(data):\n",
    "    \"\"\"データセット品質分析\"\"\"\n",
    "    log_message(\"Starting quality analysis\")\n",
    "    \n",
    "    analysis = {\n",
    "        \"total_samples\": len(data),\n",
    "        \"domains\": Counter(),\n",
    "        \"instruction_lengths\": [],\n",
    "        \"response_lengths\": [],\n",
    "        \"math_keywords\": 0,\n",
    "        \"reasoning_patterns\": 0,\n",
    "        \"empty_responses\": 0\n",
    "    }\n",
    "    \n",
    "    math_keywords = ['equation', 'solve', 'calculate', 'theorem', 'proof', 'derivative', 'integral',\n",
    "                     'algebra', 'calculus', 'geometry', 'statistics', 'number theory']\n",
    "    reasoning_patterns = ['step', 'first', 'then', 'therefore', 'because', 'since', 'solution']\n",
    "    \n",
    "    for item in data:\n",
    "        domain = item.get('domain', 'unknown')\n",
    "        instruction = item.get('instruction', '')\n",
    "        response = item.get('response', '')\n",
    "        \n",
    "        analysis['domains'][domain] += 1\n",
    "        analysis['instruction_lengths'].append(len(instruction))\n",
    "        analysis['response_lengths'].append(len(response))\n",
    "        \n",
    "        if not response.strip():\n",
    "            analysis['empty_responses'] += 1\n",
    "        \n",
    "        # キーワード・パターン検出\n",
    "        text = (instruction + ' ' + response).lower()\n",
    "        if any(keyword in text for keyword in math_keywords):\n",
    "            analysis['math_keywords'] += 1\n",
    "        \n",
    "        if any(pattern in response.lower() for pattern in reasoning_patterns):\n",
    "            analysis['reasoning_patterns'] += 1\n",
    "    \n",
    "    log_message(\"Quality analysis completed\", \"SUCCESS\")\n",
    "    return analysis\n",
    "\n",
    "def create_quality_visualization(analysis):\n",
    "    \"\"\"品質分析の可視化\"\"\"\n",
    "    if not CREATE_VISUALIZATION:\n",
    "        return None\n",
    "        \n",
    "    log_message(\"Creating quality visualization\")\n",
    "    \n",
    "    try:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # ドメイン分布\n",
    "        if analysis['domains']:\n",
    "            domains = list(analysis['domains'].keys())\n",
    "            counts = list(analysis['domains'].values())\n",
    "            axes[0,0].pie(counts, labels=domains, autopct='%1.1f%%')\n",
    "            axes[0,0].set_title('ドメイン分布')\n",
    "        \n",
    "        # 問題文長分布\n",
    "        if analysis['instruction_lengths']:\n",
    "            axes[0,1].hist(analysis['instruction_lengths'], bins=20, alpha=0.7)\n",
    "            axes[0,1].set_title('問題文長分布')\n",
    "            axes[0,1].set_xlabel('文字数')\n",
    "        \n",
    "        # 解答長分布\n",
    "        if analysis['response_lengths']:\n",
    "            axes[1,0].hist(analysis['response_lengths'], bins=20, alpha=0.7, color='orange')\n",
    "            axes[1,0].set_title('解答長分布')\n",
    "            axes[1,0].set_xlabel('文字数')\n",
    "        \n",
    "        # 品質指標\n",
    "        total = analysis['total_samples']\n",
    "        if total > 0:\n",
    "            metrics = ['数学キーワード', '推論パターン', '完全解答']\n",
    "            values = [\n",
    "                analysis['math_keywords']/total*100,\n",
    "                analysis['reasoning_patterns']/total*100,\n",
    "                (total-analysis['empty_responses'])/total*100\n",
    "            ]\n",
    "            axes[1,1].bar(metrics, values)\n",
    "            axes[1,1].set_title('品質指標 (%)')\n",
    "            axes[1,1].set_ylabel('含有率 (%)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 保存\n",
    "        viz_path = f'{OUTPUT_DIR}/quality_analysis.png'\n",
    "        plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        log_message(f\"Visualization saved to {viz_path}\", \"SUCCESS\")\n",
    "        return viz_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error creating visualization: {e}\", \"ERROR\")\n",
    "        return None\n",
    "\nprint(\"✅ 分析・統合関数定義完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generation_header"
   },
   "source": [
    "## 🎯 Step 1: 6ドメイン別データ生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "domain_generation"
   },
   "outputs": [],
   "source": [
    "# 6ドメイン データ生成実行\n",
    "print(f\"🚀 6ドメイン数学データセット生成開始\")\n",
    "print(f\"📊 設定: {MODEL_PATH}\")\n",
    "print(f\"📝 各ドメイン問題数: {ACTUAL_PROBLEMS_PER_DOMAIN}\")\n",
    "print(f\"📈 総問題数: {len(DOMAINS) * ACTUAL_PROBLEMS_PER_DOMAIN}\")\n\ntimestamp = int(time.time())\ndomain_datasets = {}\nsuccess_count = 0\nfailed_domains = []\n\n# 進捗表示用\ntotal_domains = len(DOMAINS)\n\nfor i, domain in enumerate(DOMAINS):\n    print(f\"\\n{'='*50}\")\n    print(f\"📈 進捗: {i+1}/{total_domains} - {domain.upper()}\")\n    print(f\"{'='*50}\")\n    \n    try:\n        # 問題生成\n        log_message(f\"Generating instructions for {domain}\")\n        instructions = generate_instructions_vllm(\n            MODEL_PATH, domain, ACTUAL_PROBLEMS_PER_DOMAIN, timestamp\n        )\n        \n        if not instructions:\n            log_message(f\"Failed to generate instructions for {domain}\", \"ERROR\")\n            failed_domains.append(domain)\n            continue\n        \n        # 解答生成\n        log_message(f\"Generating responses for {domain}\")\n        responses = generate_responses_vllm(MODEL_PATH, instructions)\n        \n        if not responses:\n            log_message(f\"Failed to generate responses for {domain}\", \"ERROR\")\n            failed_domains.append(domain)\n            continue\n        \n        # 成功\n        domain_datasets[domain] = responses\n        success_count += 1\n        \n        log_message(f\"✅ {domain} completed: {len(responses)} items\", \"SUCCESS\")\n        \n        # サンプル表示\n        if responses and DEBUG_MODE:\n            sample = responses[0]\n            print(f\"\\n📝 サンプル ({domain}):\")\n            print(f\"問題: {sample['instruction'][:100]}...\")\n            print(f\"解答: {sample.get('response', 'No response')[:100]}...\")\n        \n        # GPU cooldown\n        if i < total_domains - 1:\n            log_message(\"GPU cooldown...\")\n            time.sleep(5)\n            \n    except Exception as e:\n        log_message(f\"Error processing domain {domain}: {e}\", \"ERROR\")\n        failed_domains.append(domain)\n        continue\n\n# 結果サマリー\nprint(f\"\\n{'='*60}\")\nprint(f\"📊 6ドメイン生成結果サマリー\")\nprint(f\"{'='*60}\")\nprint(f\"✅ 成功: {success_count}/{total_domains} ドメイン\")\nprint(f\"📝 成功ドメイン: {list(domain_datasets.keys())}\")\nif failed_domains:\n    print(f\"❌ 失敗ドメイン: {failed_domains}\")\n\n# 詳細統計\ntotal_generated = sum(len(data) for data in domain_datasets.values())\nprint(f\"\\n📈 生成統計:\")\nfor domain, data in domain_datasets.items():\n    print(f\"  {domain}: {len(data)}問題\")\nprint(f\"  総計: {total_generated}問題\")\n\nif domain_datasets:\n    print(\"\\n✅ 生成フェーズ完了 - 次のステップに進んでください\")\nelse:\n    print(\"\\n❌ 全ドメイン失敗 - 設定を確認してください\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "merge_header"
   },
   "source": [
    "## 🔄 Step 2: データ統合・シャッフル・形式変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_merge"
   },
   "outputs": [],
   "source": [
    "# データ統合・シャッフル実行\nif not domain_datasets:\n    print(\"❌ 統合可能なデータがありません。Step 1を確認してください。\")\nelse:\n    print(\"🔄 データ統合・シャッフル開始\")\n    \n    # データ統合\n    merged_data, domain_stats = merge_domain_data(domain_datasets)\n    \n    # 統計表示\n    print(f\"\\n📊 統合結果:\")\n    total_problems = sum(domain_stats.values())\n    for domain, count in domain_stats.items():\n        percentage = (count / total_problems) * 100 if total_problems > 0 else 0\n        print(f\"  {domain}: {count}問題 ({percentage:.1f}%)\")\n    print(f\"  合計: {total_problems}問題\")\n    \n    if merged_data:\n        # ファイル名生成\n        timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        base_filename = f\"{DATASET_NAME}_{len(merged_data)}_{timestamp_str}\"\n        \n        # 統合JSONファイル保存\n        merged_file = f\"{OUTPUT_DIR}/{base_filename}.json\"\n        with open(merged_file, 'w', encoding='utf-8') as f:\n            json.dump(merged_data, f, ensure_ascii=False, indent=2)\n        \n        log_message(f\"Merged dataset saved: {merged_file}\", \"SUCCESS\")\n        \n        # ShareGPT形式変換\n        sharegpt_data = create_sharegpt_format(merged_data)\n        \n        if sharegpt_data:\n            sharegpt_file = f\"{OUTPUT_DIR}/{base_filename}_sharegpt.jsonl\"\n            with open(sharegpt_file, 'w', encoding='utf-8') as f:\n                for item in sharegpt_data:\n                    f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n            \n            log_message(f\"ShareGPT format saved: {sharegpt_file}\", \"SUCCESS\")\n        \n        # サンプル表示\n        print(f\"\\n📝 統合データサンプル:\")\n        sample = merged_data[0]\n        print(f\"ドメイン: {sample.get('domain', 'unknown')}\")\n        print(f\"問題: {sample['instruction'][:150]}...\")\n        print(f\"解答: {sample.get('response', 'No response')[:150]}...\")\n        \n        print(\"\\n✅ 統合・変換フェーズ完了\")\n    else:\n        print(\"❌ 統合データが空です\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis_header"
   },
   "source": [
    "## 📊 Step 3: データ品質分析・可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quality_analysis"
   },
   "outputs": [],
   "source": [
    "# 品質分析実行\nif 'merged_data' not in locals() or not merged_data:\n    print(\"⚠️ 分析対象データがありません。Step 2を確認してください。\")\nelse:\n    if ENABLE_QUALITY_ANALYSIS:\n        print(\"📊 データセット品質分析開始\")\n        \n        # 品質分析実行\n        analysis = analyze_dataset_quality(merged_data)\n        \n        # 結果表示\n        print(f\"\\n📈 品質分析結果:\")\n        print(f\"📝 総サンプル数: {analysis['total_samples']}\")\n        \n        if analysis['instruction_lengths']:\n            print(f\"📏 平均問題文長: {np.mean(analysis['instruction_lengths']):.1f} 文字\")\n        if analysis['response_lengths']:\n            print(f\"📏 平均解答長: {np.mean(analysis['response_lengths']):.1f} 文字\")\n        \n        total = analysis['total_samples']\n        if total > 0:\n            print(f\"❌ 空解答: {analysis['empty_responses']} ({analysis['empty_responses']/total*100:.1f}%)\")\n            print(f\"🧮 数学キーワード含有: {analysis['math_keywords']} ({analysis['math_keywords']/total*100:.1f}%)\")\n            print(f\"🧠 推論パターン含有: {analysis['reasoning_patterns']} ({analysis['reasoning_patterns']/total*100:.1f}%)\")\n        \n        print(f\"\\n📊 ドメイン別統計:\")\n        for domain, count in analysis['domains'].items():\n            percentage = count / total * 100 if total > 0 else 0\n            print(f\"  {domain}: {count}問題 ({percentage:.1f}%)\")\n        \n        # 可視化作成\n        viz_path = create_quality_visualization(analysis)\n        \n        # 分析結果保存\n        analysis_file = f\"{OUTPUT_DIR}/quality_analysis.json\"\n        analysis_serializable = {\n            k: (dict(v) if isinstance(v, Counter) else v) \n            for k, v in analysis.items()\n        }\n        \n        with open(analysis_file, 'w', encoding='utf-8') as f:\n            json.dump(analysis_serializable, f, ensure_ascii=False, indent=2)\n        \n        log_message(f\"Analysis results saved: {analysis_file}\", \"SUCCESS\")\n        \n        print(\"\\n✅ 品質分析完了\")\n    else:\n        print(\"⏭️ 品質分析はスキップされました（設定により無効）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_header"
   },
   "source": [
    "## 📥 Step 4: ファイルダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "file_download"
   },
   "outputs": [],
   "source": [
    "# ファイルダウンロード準備\nfrom google.colab import files\nimport zipfile\n\n# 生成ファイル一覧\noutput_files = []\nfor file_path in Path(OUTPUT_DIR).glob('*'):\n    if file_path.is_file():\n        output_files.append(str(file_path))\n\nprint(\"📁 生成されたファイル一覧:\")\nif output_files:\n    total_size = 0\n    for file_path in output_files:\n        filename = os.path.basename(file_path)\n        size = os.path.getsize(file_path) / 1024  # KB\n        total_size += size\n        print(f\"  📄 {filename} ({size:.1f} KB)\")\n    \n    print(f\"\\n📊 総ファイルサイズ: {total_size:.1f} KB\")\n    \n    # ZIPファイル作成\n    zip_file = f\"{OUTPUT_DIR}/{DATASET_NAME}_complete.zip\"\n    \n    print(f\"\\n📦 統合ZIPファイル作成中...\")\n    with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file_path in output_files:\n            arcname = os.path.basename(file_path)\n            zipf.write(file_path, arcname)\n    \n    zip_size = os.path.getsize(zip_file) / 1024\n    print(f\"✅ ZIPファイル作成完了: {zip_file} ({zip_size:.1f} KB)\")\n    \n    # 自動ダウンロード\n    print(f\"\\n📥 ファイルダウンロード開始...\")\n    try:\n        files.download(zip_file)\n        print(\"✅ ダウンロード完了！\")\n    except Exception as e:\n        print(f\"⚠️ 自動ダウンロードエラー: {e}\")\n        print(\"手動でファイルをダウンロードしてください\")\n    \n    # 個別ダウンロードオプション\n    print(f\"\\n💡 個別ファイルダウンロード:\")\n    print(\"以下のコードで個別にダウンロード可能:\")\n    for file_path in output_files:\n        if file_path.endswith(('.json', '.jsonl', '.png')):\n            filename = os.path.basename(file_path)\n            print(f\"# files.download('{file_path}')  # {filename}\")\nelse:\n    print(\"⚠️ ダウンロード可能なファイルがありません\")\n    print(\"Step 1-3 を実行してデータを生成してください\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_header"
   },
   "source": [
    "## 🎉 実行完了サマリー\n",
    "\n",
    "### 📊 生成結果\n",
    "- **6ドメイン数学データセット**: 代数学、応用数学、微積分学、離散数学、幾何学、数論\n",
    "- **完全単独実行**: 外部ファイル依存なし\n",
    "- **統合・シャッフル済み**: ドメインバランス保持\n",
    "- **ShareGPT互換**: 機械学習フレームワーク対応\n",
    "- **品質分析付き**: 自動品質評価・可視化\n",
    "\n",
    "### 📁 出力ファイル\n",
    "- `{DATASET_NAME}_XXX.json`: 統合データセット\n",
    "- `{DATASET_NAME}_XXX_sharegpt.jsonl`: ShareGPT形式\n",
    "- `quality_analysis.json`: 品質分析結果\n",
    "- `quality_analysis.png`: 可視化グラフ\n",
    "- `{DATASET_NAME}_complete.zip`: 全ファイル統合ZIP\n",
    "\n",
    "### 🚀 次のステップ\n",
    "1. **ファインチューニング**: 生成データでモデル訓練\n",
    "2. **品質評価**: HLE試験問題での性能測定  \n",
    "3. **スケールアップ**: より大規模なデータセット生成\n",
    "4. **ドメイン拡張**: 追加数学分野の対応\n",
    "5. **継続改善**: パラメータ調整・再生成\n",
    "\n",
    "### 🔧 カスタマイズのヒント\n",
    "- **問題数調整**: `PROBLEMS_PER_DOMAIN` を変更\n",
    "- **モデル変更**: `AUTO_MODEL_SELECTION = False` で手動選択\n",
    "- **ドメイン追加**: `DOMAINS` リストにドメイン追加\n",
    "- **パラメータ調整**: 生成温度・top_p値の調整\n",
    "\n",
    "### 📚 参考資料\n",
    "- [Magpie論文](https://arxiv.org/abs/2406.08464)\n",
    "- [DeepSeek R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)\n",
    "- [vLLM Documentation](https://docs.vllm.ai/)\n",
    "\n",
    "**🎯 単独実行版6ドメイン数学データセット生成完了！**\n",
    "\n",
    "このノートブックは外部ファイルに依存せず、Google Colab環境で完全に動作します。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",\n  "colab": {\n   "gpuType": "T4",\n   "provenance": [],\n   "include_colab_link": true\n  },\n  "kernelspec": {\n   "display_name": "Python 3",\n   "name": "python3"\n  },\n  "language_info": {\n   "name": "python"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 0\n}