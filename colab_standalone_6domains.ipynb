{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_header"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/your-repo/magpie/blob/main/colab_standalone_6domains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# ğŸ§® Magpie: 6ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ (å˜ç‹¬å®Ÿè¡Œç‰ˆ)\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€**å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€åˆ‡ä½¿ç”¨ã›ãšã«**DeepSeek R1ã‚’ä½¿ç”¨ã—ã¦6ã¤ã®æ•°å­¦ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆãƒ»çµ±åˆã™ã‚‹å®Œå…¨å˜ç‹¬å®Ÿè¡Œç‰ˆã§ã™ã€‚\n",
    "\n",
    "## ğŸ¯ ç‰¹å¾´\n",
    "- âœ… **å®Œå…¨å˜ç‹¬å®Ÿè¡Œ**: å¤–éƒ¨ãƒªãƒã‚¸ãƒˆãƒªãƒ»ãƒ•ã‚¡ã‚¤ãƒ«ä¸è¦\n",
    "- âœ… **å…¨æ©Ÿèƒ½å†…è”µ**: ç”Ÿæˆãƒ»çµ±åˆãƒ»åˆ†æãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "- âœ… **GPUè‡ªå‹•æœ€é©åŒ–**: T4/A100ä¸¡å¯¾å¿œ\n",
    "- âœ… **ã‚¨ãƒ©ãƒ¼å›å¾©**: å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°\n",
    "\n",
    "## ğŸ“Š å¯¾å¿œãƒ‰ãƒ¡ã‚¤ãƒ³\n",
    "1. **Algebra** (ä»£æ•°å­¦): æ–¹ç¨‹å¼ã€å¤šé …å¼ã€é–¢æ•°\n",
    "2. **Applied Mathematics** (å¿œç”¨æ•°å­¦): å¾®åˆ†æ–¹ç¨‹å¼ã€æœ€é©åŒ–\n",
    "3. **Calculus** (å¾®ç©åˆ†å­¦): å¾®ç©åˆ†ã€æ¥µé™ã€ç´šæ•°\n",
    "4. **Discrete Mathematics** (é›¢æ•£æ•°å­¦): çµ„åˆã›ã€ã‚°ãƒ©ãƒ•ç†è«–\n",
    "5. **Geometry** (å¹¾ä½•å­¦): è§£æå¹¾ä½•ã€ç©ºé–“å›³å½¢\n",
    "6. **Number Theory** (æ•°è«–): ç´ æ•°ã€åˆåŒå¼ã€æš—å·å¿œç”¨\n",
    "\n",
    "## âš ï¸ é‡è¦äº‹é …\n",
    "- **GPUå¿…é ˆ**: A100æ¨å¥¨ã€T4ã§ã‚‚å‹•ä½œå¯èƒ½\n",
    "- **å®Ÿè¡Œæ™‚é–“**: 30åˆ†-3æ™‚é–“ï¼ˆè¨­å®šã«ã‚ˆã‚‹ï¼‰\n",
    "- **ãƒ¡ãƒ¢ãƒªåˆ¶é™**: è‡ªå‹•èª¿æ•´æ©Ÿèƒ½ä»˜ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "settings_header"
   },
   "source": [
    "## âš™ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "user_settings"
   },
   "outputs": [],
   "source": [
    "# ===== ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š =====\n",
    "\n",
    "# åŸºæœ¬è¨­å®š\n",
    "DATASET_NAME = \"HLE_6Domains_Math_Standalone\"  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå\n",
    "PROBLEMS_PER_DOMAIN = 15  # å„ãƒ‰ãƒ¡ã‚¤ãƒ³ã®å•é¡Œæ•°ï¼ˆColabç”¨ã«èª¿æ•´ï¼‰\n",
    "OUTPUT_DIR = \"/content/magpie_output\"  # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆGPUèƒ½åŠ›ã«å¿œã˜ã¦è‡ªå‹•é¸æŠï¼‰\n",
    "AUTO_MODEL_SELECTION = True  # True: GPUæ€§èƒ½ã§è‡ªå‹•é¸æŠ, False: æ‰‹å‹•é¸æŠ\n",
    "\n",
    "# æ‰‹å‹•é¸æŠæ™‚ã®è¨­å®š\n",
    "MANUAL_USE_LIGHTWEIGHT = True  # Trueã§Qwen2.5-3Bã€Falseã§DeepSeek R1\n",
    "\n",
    "# ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "INSTRUCTION_TEMP = 1.2\n",
    "INSTRUCTION_TOP_P = 1.0\n",
    "RESPONSE_TEMP = 0.1\n",
    "RESPONSE_TOP_P = 1.0\n",
    "\n",
    "# å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ï¼‰\n",
    "DOMAINS = [\n",
    "    \"algebra\",\n",
    "    \"applied-mathematics\", \n",
    "    \"calculus\",\n",
    "    \"discrete-mathematics\",\n",
    "    \"geometry\",\n",
    "    \"number-theory\"\n",
    "]\n",
    "\n",
    "# ãƒ‡ãƒãƒƒã‚°ãƒ»é–‹ç™ºç”¨è¨­å®š\n",
    "DEBUG_MODE = False  # True: è©³ç´°ãƒ­ã‚°å‡ºåŠ›\n",
    "ENABLE_QUALITY_ANALYSIS = True  # å“è³ªåˆ†æå®Ÿè¡Œ\n",
    "CREATE_VISUALIZATION = True  # ã‚°ãƒ©ãƒ•ä½œæˆ\n",
    "\n",
    "print(f\"ğŸ“Š è¨­å®šå®Œäº†:\")\n",
    "print(f\"  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå: {DATASET_NAME}\")\n",
    "print(f\"  å„ãƒ‰ãƒ¡ã‚¤ãƒ³å•é¡Œæ•°: {PROBLEMS_PER_DOMAIN}\")\n",
    "print(f\"  ç·å•é¡Œæ•°: {len(DOMAINS) * PROBLEMS_PER_DOMAIN}\")\n",
    "print(f\"  å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³: {len(DOMAINS)}å€‹\")\n",
    "print(f\"  è‡ªå‹•ãƒ¢ãƒ‡ãƒ«é¸æŠ: {'æœ‰åŠ¹' if AUTO_MODEL_SELECTION else 'ç„¡åŠ¹'}\")\n",
    "print(f\"  å‡ºåŠ›å…ˆ: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## ğŸš€ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ»ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dependencies_install"
   },
   "outputs": [],
   "source": [
    "# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "print(\"ğŸ“¦ ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "\n",
    "!pip install -q vllm==0.6.5\n",
    "!pip install -q transformers>=4.36.0\n",
    "!pip install -q torch>=2.0.0\n",
    "!pip install -q accelerate\n",
    "!pip install -q datasets\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q tiktoken\n",
    "!pip install -q numpy pandas tqdm\n",
    "!pip install -q matplotlib seaborn\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "print(\"âœ… ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
    "\n",
    "# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# GPUç¢ºèª\n",
    "import torch\n",
    "print(f\"\\nğŸ” ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:\")\n",
    "print(f\"  CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"  GPUå: {gpu_name}\")\n",
    "    print(f\"  GPUãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"  âš ï¸ GPU not available - CPU mode will be very slow\")\n",
    "\n",
    "# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"\\nğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_config_header"
   },
   "source": [
    "## ğŸ¤– ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ»è‡ªå‹•æœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_configuration"
   },
   "outputs": [],
   "source": [
    "# GPUæ€§èƒ½ã«åŸºã¥ãè‡ªå‹•ãƒ¢ãƒ‡ãƒ«é¸æŠ\n",
    "def auto_select_model_config():\n",
    "    \"\"\"GPUæ€§èƒ½ã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’è‡ªå‹•é¸æŠ\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return {\n",
    "            'model_path': 'Qwen/Qwen2.5-1.5B-Instruct',\n",
    "            'tensor_parallel': 1,\n",
    "            'gpu_memory_util': 0.70,\n",
    "            'batch_size': 2,\n",
    "            'problems_per_domain': 5,\n",
    "            'mode': 'CPU (éæ¨å¥¨)'\n",
    "        }\n",
    "    \n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    gpu_name = torch.cuda.get_device_name(0).lower()\n",
    "    \n",
    "    if 'a100' in gpu_name or gpu_memory > 70:\n",
    "        # A100 or high-end GPU\n",
    "        return {\n",
    "            'model_path': 'deepseek-ai/DeepSeek-R1',\n",
    "            'tensor_parallel': 2,\n",
    "            'gpu_memory_util': 0.90,\n",
    "            'batch_size': 8,\n",
    "            'problems_per_domain': PROBLEMS_PER_DOMAIN,\n",
    "            'mode': 'A100 ãƒ•ãƒ«ãƒ¢ãƒ¼ãƒ‰'\n",
    "        }\n",
    "    elif 'v100' in gpu_name or gpu_memory > 30:\n",
    "        # V100 or mid-range GPU\n",
    "        return {\n",
    "            'model_path': 'Qwen/Qwen2.5-7B-Instruct',\n",
    "            'tensor_parallel': 1,\n",
    "            'gpu_memory_util': 0.85,\n",
    "            'batch_size': 6,\n",
    "            'problems_per_domain': PROBLEMS_PER_DOMAIN,\n",
    "            'mode': 'V100 ä¸­è¨­å®š'\n",
    "        }\n",
    "    else:\n",
    "        # T4 or low-end GPU\n",
    "        return {\n",
    "            'model_path': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "            'tensor_parallel': 1,\n",
    "            'gpu_memory_util': 0.80,\n",
    "            'batch_size': 4,\n",
    "            'problems_per_domain': min(PROBLEMS_PER_DOMAIN, 20),\n",
    "            'mode': 'T4 è»½é‡ãƒ¢ãƒ¼ãƒ‰'\n",
    "        }\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«è¨­å®šæ±ºå®š\n",
    "if AUTO_MODEL_SELECTION:\n",
    "    config = auto_select_model_config()\n",
    "    print(f\"ğŸ¤– è‡ªå‹•é¸æŠãƒ¢ãƒ¼ãƒ‰: {config['mode']}\")\nelse:\n",
    "    if MANUAL_USE_LIGHTWEIGHT:\n",
    "        config = {\n",
    "            'model_path': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "            'tensor_parallel': 1,\n",
    "            'gpu_memory_util': 0.80,\n",
    "            'batch_size': 4,\n",
    "            'problems_per_domain': PROBLEMS_PER_DOMAIN,\n",
    "            'mode': 'æ‰‹å‹• è»½é‡è¨­å®š'\n",
    "        }\n",
    "    else:\n",
    "        config = {\n",
    "            'model_path': 'deepseek-ai/DeepSeek-R1',\n",
    "            'tensor_parallel': 2,\n",
    "            'gpu_memory_util': 0.90,\n",
    "            'batch_size': 8,\n",
    "            'problems_per_domain': PROBLEMS_PER_DOMAIN,\n",
    "            'mode': 'æ‰‹å‹• ãƒ•ãƒ«è¨­å®š'\n",
    "        }\n",
    "    print(f\"ğŸ›ï¸ æ‰‹å‹•é¸æŠãƒ¢ãƒ¼ãƒ‰: {config['mode']}\")\n\n# è¨­å®šè¡¨ç¤º\nMODEL_PATH = config['model_path']\nTENSOR_PARALLEL = config['tensor_parallel']\nGPU_MEMORY_UTIL = config['gpu_memory_util']\nBATCH_SIZE = config['batch_size']\nACTUAL_PROBLEMS_PER_DOMAIN = config['problems_per_domain']\n\nprint(f\"\\nğŸ“‹ æœ€çµ‚è¨­å®š:\")\nprint(f\"  ãƒ¢ãƒ‡ãƒ«: {MODEL_PATH}\")\nprint(f\"  ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—: {TENSOR_PARALLEL}\")\nprint(f\"  GPUä½¿ç”¨ç‡: {GPU_MEMORY_UTIL}\")\nprint(f\"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH_SIZE}\")\nprint(f\"  å„ãƒ‰ãƒ¡ã‚¤ãƒ³å•é¡Œæ•°: {ACTUAL_PROBLEMS_PER_DOMAIN}\")\nprint(f\"  ç·å•é¡Œæ•°: {len(DOMAINS) * ACTUAL_PROBLEMS_PER_DOMAIN}\")\n\n# èªè¨¼ãƒã‚§ãƒƒã‚¯ï¼ˆDeepSeek R1ä½¿ç”¨æ™‚ï¼‰\nif 'deepseek' in MODEL_PATH.lower():\n    print(\"\\nğŸ” DeepSeek R1èªè¨¼ãŒå¿…è¦ã§ã™\")\n    print(\"ä»¥ä¸‹ã®ã‚»ãƒ«ã§èªè¨¼ã‚’è¡Œã£ã¦ãã ã•ã„\")\nelse:\n    print(\"\\nâœ… èªè¨¼ä¸è¦ãƒ¢ãƒ‡ãƒ«é¸æŠæ¸ˆã¿\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auth_setup"
   },
   "outputs": [],
   "source": [
    "# Hugging Faceèªè¨¼ï¼ˆDeepSeek R1ä½¿ç”¨æ™‚ã®ã¿ï¼‰\n",
    "if 'deepseek' in MODEL_PATH.lower():\n",
    "    print(\"ğŸ” DeepSeek R1èªè¨¼ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "    print(\"https://huggingface.co/settings/tokens ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—\")\n",
    "    \n",
    "    # æ‰‹å‹•èªè¨¼ - ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦å®Ÿè¡Œ\n",
    "    # from huggingface_hub import login\n",
    "    # login()  # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›\n",
    "    \n",
    "    print(\"âš ï¸ ä¸Šè¨˜ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã—ã¦èªè¨¼ã—ã¦ãã ã•ã„\")\n",
    "    print(\"èªè¨¼å¾Œã«æ¬¡ã®ã‚»ãƒ«ã«é€²ã‚“ã§ãã ã•ã„\")\nelse:\n",
    "    print(\"âœ… è»½é‡ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ã®ãŸã‚èªè¨¼ä¸è¦\")\n    print(\"ãã®ã¾ã¾æ¬¡ã®ã‚»ãƒ«ã«é€²ã‚“ã§ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "embedded_code_header"
   },
   "source": [
    "## ğŸ› ï¸ çµ„ã¿è¾¼ã¿ã‚³ãƒ¼ãƒ‰å®šç¾©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "embedded_core_functions"
   },
   "outputs": [],
   "source": [
    "# ===== çµ„ã¿è¾¼ã¿ã‚³ã‚¢é–¢æ•° =====\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«è¨­å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆå†…è”µï¼‰\n",
    "MODEL_CONFIGS = {\n",
    "    \"deepseek-ai/DeepSeek-R1\": {\n",
    "        \"model_name\": \"deepseek-ai/DeepSeek-R1\",\n",
    "        \"stop_tokens\": [\"<ï½œbeginâ–ofâ–sentenceï½œ>\", \"<ï½œendâ–ofâ–sentenceï½œ>\", \"<ï½œendâ–ofâ–textï½œ>\", \"<ï½œim_startï½œ>\", \"<ï½œim_endï½œ>\"],\n",
    "        \"stop_token_ids\": [100001, 100002, 100003, 100011, 100012],\n",
    "        \"pre_query_template_math\": \"<ï½œim_startï½œ>system\\nYou are an advanced AI assistant specialized in mathematical reasoning and problem-solving for high-level examinations (HLE). Your expertise covers algebra, calculus, geometry, statistics, and number theory.\\n\\nWhen solving mathematical problems, you should:\\n1. Carefully analyze the given problem and identify key information\\n2. Choose the most appropriate solution method\\n3. Show detailed step-by-step reasoning using Chain-of-Thought approach\\n4. Verify your solution and explain the reasoning behind each step\\n5. Provide insights that help understand the underlying mathematical concepts\\n\\nYour responses should demonstrate clear logical thinking, mathematical rigor, and educational value suitable for advanced mathematics preparation.<ï½œim_endï½œ>\\n<ï½œim_startï½œ>user\\n\",\n",
    "        \"pre_query_template\": \"<ï½œim_startï½œ>user\\n\"\n",
    "    },\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\": {\n",
    "        \"model_name\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "        \"stop_tokens\": [\"<|im_start|>\", \"<|im_end|>\", \"<|endoftext|>\"],\n",
    "        \"stop_token_ids\": [151643, 151644, 151645],\n",
    "        \"pre_query_template_math\": \"<|im_start|>system\\nYou are an AI assistant specialized in mathematical reasoning and problem-solving. Your expertise covers algebra, calculus, geometry, statistics, and number theory. Provide clear, step-by-step solutions with detailed explanations.<|im_end|>\\n<|im_start|>user\\n\",\n",
    "        \"pre_query_template\": \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n\"\n",
    "    },\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\": {\n",
    "        \"model_name\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"stop_tokens\": [\"<|im_start|>\", \"<|im_end|>\", \"<|endoftext|>\"],\n",
    "        \"stop_token_ids\": [151643, 151644, 151645],\n",
    "        \"pre_query_template_math\": \"<|im_start|>system\\nYou are an AI assistant specialized in mathematical reasoning and problem-solving. Your expertise covers algebra, calculus, geometry, statistics, and number theory. Provide clear, step-by-step solutions with detailed explanations.<|im_end|>\\n<|im_start|>user\\n\",\n",
    "        \"pre_query_template\": \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n\"\n",
    "    },\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\": {\n",
    "        \"model_name\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        \"stop_tokens\": [\"<|im_start|>\", \"<|im_end|>\", \"<|endoftext|>\"],\n",
    "        \"stop_token_ids\": [151643, 151644, 151645],\n",
    "        \"pre_query_template_math\": \"<|im_start|>system\\nYou are an AI assistant specialized in mathematical reasoning and problem-solving. Provide clear, step-by-step solutions with detailed explanations.<|im_end|>\\n<|im_start|>user\\n\",\n",
    "        \"pre_query_template\": \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_model_config(model_path, use_math_template=True):\n",
    "    \"\"\"ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’å–å¾—\"\"\"\n",
    "    if model_path not in MODEL_CONFIGS:\n",
    "        raise ValueError(f\"Unsupported model: {model_path}\")\n",
    "    \n",
    "    config = MODEL_CONFIGS[model_path]\n",
    "    template_key = \"pre_query_template_math\" if use_math_template else \"pre_query_template\"\n",
    "    \n",
    "    return {\n",
    "        'pre_query_template': config.get(template_key, config['pre_query_template']),\n",
    "        'stop_tokens': config['stop_tokens'],\n",
    "        'stop_token_ids': config.get('stop_token_ids', [])\n",
    "    }\n",
    "\n",
    "def log_message(message, level=\"INFO\"):\n",
    "    \"\"\"ãƒ­ã‚°å‡ºåŠ›\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    if DEBUG_MODE or level in [\"ERROR\", \"SUCCESS\"]:\n",
    "        prefix = {\"INFO\": \"â„¹ï¸\", \"SUCCESS\": \"âœ…\", \"ERROR\": \"âŒ\", \"WARNING\": \"âš ï¸\"}[level]\n",
    "        print(f\"[{timestamp}] {prefix} {message}\")\n\nprint(\"âœ… çµ„ã¿è¾¼ã¿ã‚³ã‚¢é–¢æ•°å®šç¾©å®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "embedded_generation_functions"
   },
   "outputs": [],
   "source": [
    "# ===== ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–¢æ•° =====\n",
    "\n",
    "def generate_instructions_vllm(model_path, domain, num_problems, timestamp):\n",
    "    \"\"\"vLLMã‚’ä½¿ç”¨ã—ã¦å•é¡Œã‚’ç”Ÿæˆ\"\"\"\n",
    "    log_message(f\"Starting instruction generation for {domain}\")\n",
    "    \n",
    "    try:\n",
    "        from vllm import LLM, SamplingParams\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–\n",
    "        llm = LLM(\n",
    "            model=model_path,\n",
    "            tensor_parallel_size=TENSOR_PARALLEL,\n",
    "            gpu_memory_utilization=GPU_MEMORY_UTIL,\n",
    "            max_model_len=8192,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«è¨­å®šå–å¾—\n",
    "        model_config = get_model_config(model_path, use_math_template=True)\n",
    "        pre_query_template = model_config['pre_query_template']\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=INSTRUCTION_TEMP,\n",
    "            top_p=INSTRUCTION_TOP_P,\n",
    "            max_tokens=3072,\n",
    "            stop=model_config['stop_tokens']\n",
    "        )\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™ï¼ˆç©ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§instructionç”Ÿæˆï¼‰\n",
    "        prompts = [pre_query_template] * num_problems\n",
    "        \n",
    "        # ç”Ÿæˆå®Ÿè¡Œ\n",
    "        log_message(f\"Generating {num_problems} problems for {domain}\")\n",
    "        outputs = llm.generate(prompts, sampling_params)\n",
    "        \n",
    "        # çµæœæ•´ç†\n",
    "        results = []\n",
    "        for i, output in enumerate(outputs):\n",
    "            instruction = output.outputs[0].text.strip()\n",
    "            \n",
    "            result = {\n",
    "                \"id\": i,\n",
    "                \"pre_query_template\": pre_query_template,\n",
    "                \"instruction\": instruction,\n",
    "                \"response\": None,\n",
    "                \"created\": timestamp,\n",
    "                \"gen_input_configs\": {\n",
    "                    \"temperature\": INSTRUCTION_TEMP,\n",
    "                    \"top_p\": INSTRUCTION_TOP_P,\n",
    "                    \"input_generator\": model_path,\n",
    "                    \"domain\": domain\n",
    "                },\n",
    "                \"gen_response_configs\": None\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        log_message(f\"Generated {len(results)} instructions for {domain}\", \"SUCCESS\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error in instruction generation for {domain}: {e}\", \"ERROR\")\n",
    "        return []\n",
    "\n",
    "def generate_responses_vllm(model_path, instructions_data):\n",
    "    \"\"\"vLLMã‚’ä½¿ç”¨ã—ã¦è§£ç­”ã‚’ç”Ÿæˆ\"\"\"\n",
    "    log_message(\"Starting response generation\")\n",
    "    \n",
    "    try:\n",
    "        from vllm import LLM, SamplingParams\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–\n",
    "        llm = LLM(\n",
    "            model=model_path,\n",
    "            tensor_parallel_size=TENSOR_PARALLEL,\n",
    "            gpu_memory_utilization=GPU_MEMORY_UTIL,\n",
    "            max_model_len=8192,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«è¨­å®šå–å¾—\n",
    "        model_config = get_model_config(model_path, use_math_template=True)\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=RESPONSE_TEMP,\n",
    "            top_p=RESPONSE_TOP_P,\n",
    "            max_tokens=4096,\n",
    "            stop=model_config['stop_tokens']\n",
    "        )\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™\n",
    "        prompts = []\n",
    "        for item in instructions_data:\n",
    "            prompt = item['pre_query_template'] + item['instruction']\n",
    "            prompts.append(prompt)\n",
    "        \n",
    "        # ãƒãƒƒãƒå‡¦ç†ã§ç”Ÿæˆ\n",
    "        results = []\n",
    "        for i in range(0, len(prompts), BATCH_SIZE):\n",
    "            batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "            batch_data = instructions_data[i:i+BATCH_SIZE]\n",
    "            \n",
    "            log_message(f\"Generating responses for batch {i//BATCH_SIZE + 1}\")\n",
    "            outputs = llm.generate(batch_prompts, sampling_params)\n",
    "            \n",
    "            for j, output in enumerate(outputs):\n",
    "                response = output.outputs[0].text.strip()\n",
    "                \n",
    "                # å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦è§£ç­”ã‚’è¿½åŠ \n",
    "                result = batch_data[j].copy()\n",
    "                result['response'] = response\n",
    "                result['gen_response_configs'] = {\n",
    "                    \"temperature\": RESPONSE_TEMP,\n",
    "                    \"top_p\": RESPONSE_TOP_P,\n",
    "                    \"output_generator\": model_path\n",
    "                }\n",
    "                results.append(result)\n",
    "        \n",
    "        log_message(f\"Generated {len(results)} responses\", \"SUCCESS\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error in response generation: {e}\", \"ERROR\")\n",
    "        return instructions_data  # å¤±æ•—æ™‚ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™\n\nprint(\"âœ… ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–¢æ•°å®šç¾©å®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "embedded_analysis_functions"
   },
   "outputs": [],
   "source": [
    "# ===== ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»çµ±åˆé–¢æ•° =====\n",
    "\n",
    "def merge_domain_data(domain_datasets):\n",
    "    \"\"\"ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«\"\"\"\n",
    "    log_message(\"Starting data merging and shuffling\")\n",
    "    \n",
    "    all_data = []\n",
    "    domain_stats = {}\n",
    "    \n",
    "    for domain, data in domain_datasets.items():\n",
    "        if not data:\n",
    "            continue\n",
    "            \n",
    "        # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ \n",
    "        for item in data:\n",
    "            item['domain'] = domain\n",
    "            item['source'] = 'colab-standalone'\n",
    "            item['dataset_version'] = '1.0'\n",
    "        \n",
    "        all_data.extend(data)\n",
    "        domain_stats[domain] = len(data)\n",
    "        \n",
    "        log_message(f\"Added {len(data)} items from {domain}\")\n",
    "    \n",
    "    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "    random.seed(42)\n",
    "    random.shuffle(all_data)\n",
    "    \n",
    "    log_message(f\"Merged and shuffled {len(all_data)} total items\", \"SUCCESS\")\n",
    "    return all_data, domain_stats\n",
    "\n",
    "def create_sharegpt_format(data):\n",
    "    \"\"\"ShareGPTå½¢å¼ã«å¤‰æ›\"\"\"\n",
    "    log_message(\"Converting to ShareGPT format\")\n",
    "    \n",
    "    sharegpt_data = []\n",
    "    for i, item in enumerate(data):\n",
    "        if not item.get('response'):\n",
    "            continue\n",
    "            \n",
    "        sharegpt_entry = {\n",
    "            \"conversation_id\": f\"colab-standalone-{i}\",\n",
    "            \"domain\": item.get('domain', 'unknown'),\n",
    "            \"source\": item.get('source', 'colab-standalone'),\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": item['instruction']},\n",
    "                {\"from\": \"gpt\", \"value\": item['response']}\n",
    "            ],\n",
    "            \"gen_input_configs\": item.get('gen_input_configs', {}),\n",
    "            \"gen_response_configs\": item.get('gen_response_configs', {}),\n",
    "            \"created\": item.get('created', ''),\n",
    "            \"id\": item.get('id', i)\n",
    "        }\n",
    "        sharegpt_data.append(sharegpt_entry)\n",
    "    \n",
    "    log_message(f\"Created ShareGPT format with {len(sharegpt_data)} entries\", \"SUCCESS\")\n",
    "    return sharegpt_data\n",
    "\n",
    "def analyze_dataset_quality(data):\n",
    "    \"\"\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå“è³ªåˆ†æ\"\"\"\n",
    "    log_message(\"Starting quality analysis\")\n",
    "    \n",
    "    analysis = {\n",
    "        \"total_samples\": len(data),\n",
    "        \"domains\": Counter(),\n",
    "        \"instruction_lengths\": [],\n",
    "        \"response_lengths\": [],\n",
    "        \"math_keywords\": 0,\n",
    "        \"reasoning_patterns\": 0,\n",
    "        \"empty_responses\": 0\n",
    "    }\n",
    "    \n",
    "    math_keywords = ['equation', 'solve', 'calculate', 'theorem', 'proof', 'derivative', 'integral',\n",
    "                     'algebra', 'calculus', 'geometry', 'statistics', 'number theory']\n",
    "    reasoning_patterns = ['step', 'first', 'then', 'therefore', 'because', 'since', 'solution']\n",
    "    \n",
    "    for item in data:\n",
    "        domain = item.get('domain', 'unknown')\n",
    "        instruction = item.get('instruction', '')\n",
    "        response = item.get('response', '')\n",
    "        \n",
    "        analysis['domains'][domain] += 1\n",
    "        analysis['instruction_lengths'].append(len(instruction))\n",
    "        analysis['response_lengths'].append(len(response))\n",
    "        \n",
    "        if not response.strip():\n",
    "            analysis['empty_responses'] += 1\n",
    "        \n",
    "        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º\n",
    "        text = (instruction + ' ' + response).lower()\n",
    "        if any(keyword in text for keyword in math_keywords):\n",
    "            analysis['math_keywords'] += 1\n",
    "        \n",
    "        if any(pattern in response.lower() for pattern in reasoning_patterns):\n",
    "            analysis['reasoning_patterns'] += 1\n",
    "    \n",
    "    log_message(\"Quality analysis completed\", \"SUCCESS\")\n",
    "    return analysis\n",
    "\n",
    "def create_quality_visualization(analysis):\n",
    "    \"\"\"å“è³ªåˆ†æã®å¯è¦–åŒ–\"\"\"\n",
    "    if not CREATE_VISUALIZATION:\n",
    "        return None\n",
    "        \n",
    "    log_message(\"Creating quality visualization\")\n",
    "    \n",
    "    try:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†å¸ƒ\n",
    "        if analysis['domains']:\n",
    "            domains = list(analysis['domains'].keys())\n",
    "            counts = list(analysis['domains'].values())\n",
    "            axes[0,0].pie(counts, labels=domains, autopct='%1.1f%%')\n",
    "            axes[0,0].set_title('ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†å¸ƒ')\n",
    "        \n",
    "        # å•é¡Œæ–‡é•·åˆ†å¸ƒ\n",
    "        if analysis['instruction_lengths']:\n",
    "            axes[0,1].hist(analysis['instruction_lengths'], bins=20, alpha=0.7)\n",
    "            axes[0,1].set_title('å•é¡Œæ–‡é•·åˆ†å¸ƒ')\n",
    "            axes[0,1].set_xlabel('æ–‡å­—æ•°')\n",
    "        \n",
    "        # è§£ç­”é•·åˆ†å¸ƒ\n",
    "        if analysis['response_lengths']:\n",
    "            axes[1,0].hist(analysis['response_lengths'], bins=20, alpha=0.7, color='orange')\n",
    "            axes[1,0].set_title('è§£ç­”é•·åˆ†å¸ƒ')\n",
    "            axes[1,0].set_xlabel('æ–‡å­—æ•°')\n",
    "        \n",
    "        # å“è³ªæŒ‡æ¨™\n",
    "        total = analysis['total_samples']\n",
    "        if total > 0:\n",
    "            metrics = ['æ•°å­¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³', 'å®Œå…¨è§£ç­”']\n",
    "            values = [\n",
    "                analysis['math_keywords']/total*100,\n",
    "                analysis['reasoning_patterns']/total*100,\n",
    "                (total-analysis['empty_responses'])/total*100\n",
    "            ]\n",
    "            axes[1,1].bar(metrics, values)\n",
    "            axes[1,1].set_title('å“è³ªæŒ‡æ¨™ (%)')\n",
    "            axes[1,1].set_ylabel('å«æœ‰ç‡ (%)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # ä¿å­˜\n",
    "        viz_path = f'{OUTPUT_DIR}/quality_analysis.png'\n",
    "        plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        log_message(f\"Visualization saved to {viz_path}\", \"SUCCESS\")\n",
    "        return viz_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Error creating visualization: {e}\", \"ERROR\")\n",
    "        return None\n",
    "\nprint(\"âœ… åˆ†æãƒ»çµ±åˆé–¢æ•°å®šç¾©å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generation_header"
   },
   "source": [
    "## ğŸ¯ Step 1: 6ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "domain_generation"
   },
   "outputs": [],
   "source": [
    "# 6ãƒ‰ãƒ¡ã‚¤ãƒ³ ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Ÿè¡Œ\n",
    "print(f\"ğŸš€ 6ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆé–‹å§‹\")\n",
    "print(f\"ğŸ“Š è¨­å®š: {MODEL_PATH}\")\n",
    "print(f\"ğŸ“ å„ãƒ‰ãƒ¡ã‚¤ãƒ³å•é¡Œæ•°: {ACTUAL_PROBLEMS_PER_DOMAIN}\")\n",
    "print(f\"ğŸ“ˆ ç·å•é¡Œæ•°: {len(DOMAINS) * ACTUAL_PROBLEMS_PER_DOMAIN}\")\n\ntimestamp = int(time.time())\ndomain_datasets = {}\nsuccess_count = 0\nfailed_domains = []\n\n# é€²æ—è¡¨ç¤ºç”¨\ntotal_domains = len(DOMAINS)\n\nfor i, domain in enumerate(DOMAINS):\n    print(f\"\\n{'='*50}\")\n    print(f\"ğŸ“ˆ é€²æ—: {i+1}/{total_domains} - {domain.upper()}\")\n    print(f\"{'='*50}\")\n    \n    try:\n        # å•é¡Œç”Ÿæˆ\n        log_message(f\"Generating instructions for {domain}\")\n        instructions = generate_instructions_vllm(\n            MODEL_PATH, domain, ACTUAL_PROBLEMS_PER_DOMAIN, timestamp\n        )\n        \n        if not instructions:\n            log_message(f\"Failed to generate instructions for {domain}\", \"ERROR\")\n            failed_domains.append(domain)\n            continue\n        \n        # è§£ç­”ç”Ÿæˆ\n        log_message(f\"Generating responses for {domain}\")\n        responses = generate_responses_vllm(MODEL_PATH, instructions)\n        \n        if not responses:\n            log_message(f\"Failed to generate responses for {domain}\", \"ERROR\")\n            failed_domains.append(domain)\n            continue\n        \n        # æˆåŠŸ\n        domain_datasets[domain] = responses\n        success_count += 1\n        \n        log_message(f\"âœ… {domain} completed: {len(responses)} items\", \"SUCCESS\")\n        \n        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n        if responses and DEBUG_MODE:\n            sample = responses[0]\n            print(f\"\\nğŸ“ ã‚µãƒ³ãƒ—ãƒ« ({domain}):\")\n            print(f\"å•é¡Œ: {sample['instruction'][:100]}...\")\n            print(f\"è§£ç­”: {sample.get('response', 'No response')[:100]}...\")\n        \n        # GPU cooldown\n        if i < total_domains - 1:\n            log_message(\"GPU cooldown...\")\n            time.sleep(5)\n            \n    except Exception as e:\n        log_message(f\"Error processing domain {domain}: {e}\", \"ERROR\")\n        failed_domains.append(domain)\n        continue\n\n# çµæœã‚µãƒãƒªãƒ¼\nprint(f\"\\n{'='*60}\")\nprint(f\"ğŸ“Š 6ãƒ‰ãƒ¡ã‚¤ãƒ³ç”Ÿæˆçµæœã‚µãƒãƒªãƒ¼\")\nprint(f\"{'='*60}\")\nprint(f\"âœ… æˆåŠŸ: {success_count}/{total_domains} ãƒ‰ãƒ¡ã‚¤ãƒ³\")\nprint(f\"ğŸ“ æˆåŠŸãƒ‰ãƒ¡ã‚¤ãƒ³: {list(domain_datasets.keys())}\")\nif failed_domains:\n    print(f\"âŒ å¤±æ•—ãƒ‰ãƒ¡ã‚¤ãƒ³: {failed_domains}\")\n\n# è©³ç´°çµ±è¨ˆ\ntotal_generated = sum(len(data) for data in domain_datasets.values())\nprint(f\"\\nğŸ“ˆ ç”Ÿæˆçµ±è¨ˆ:\")\nfor domain, data in domain_datasets.items():\n    print(f\"  {domain}: {len(data)}å•é¡Œ\")\nprint(f\"  ç·è¨ˆ: {total_generated}å•é¡Œ\")\n\nif domain_datasets:\n    print(\"\\nâœ… ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºå®Œäº† - æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚“ã§ãã ã•ã„\")\nelse:\n    print(\"\\nâŒ å…¨ãƒ‰ãƒ¡ã‚¤ãƒ³å¤±æ•— - è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "merge_header"
   },
   "source": [
    "## ğŸ”„ Step 2: ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒ»å½¢å¼å¤‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_merge"
   },
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«å®Ÿè¡Œ\nif not domain_datasets:\n    print(\"âŒ çµ±åˆå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Step 1ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\nelse:\n    print(\"ğŸ”„ ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«é–‹å§‹\")\n    \n    # ãƒ‡ãƒ¼ã‚¿çµ±åˆ\n    merged_data, domain_stats = merge_domain_data(domain_datasets)\n    \n    # çµ±è¨ˆè¡¨ç¤º\n    print(f\"\\nğŸ“Š çµ±åˆçµæœ:\")\n    total_problems = sum(domain_stats.values())\n    for domain, count in domain_stats.items():\n        percentage = (count / total_problems) * 100 if total_problems > 0 else 0\n        print(f\"  {domain}: {count}å•é¡Œ ({percentage:.1f}%)\")\n    print(f\"  åˆè¨ˆ: {total_problems}å•é¡Œ\")\n    \n    if merged_data:\n        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ\n        timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        base_filename = f\"{DATASET_NAME}_{len(merged_data)}_{timestamp_str}\"\n        \n        # çµ±åˆJSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜\n        merged_file = f\"{OUTPUT_DIR}/{base_filename}.json\"\n        with open(merged_file, 'w', encoding='utf-8') as f:\n            json.dump(merged_data, f, ensure_ascii=False, indent=2)\n        \n        log_message(f\"Merged dataset saved: {merged_file}\", \"SUCCESS\")\n        \n        # ShareGPTå½¢å¼å¤‰æ›\n        sharegpt_data = create_sharegpt_format(merged_data)\n        \n        if sharegpt_data:\n            sharegpt_file = f\"{OUTPUT_DIR}/{base_filename}_sharegpt.jsonl\"\n            with open(sharegpt_file, 'w', encoding='utf-8') as f:\n                for item in sharegpt_data:\n                    f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n            \n            log_message(f\"ShareGPT format saved: {sharegpt_file}\", \"SUCCESS\")\n        \n        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n        print(f\"\\nğŸ“ çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:\")\n        sample = merged_data[0]\n        print(f\"ãƒ‰ãƒ¡ã‚¤ãƒ³: {sample.get('domain', 'unknown')}\")\n        print(f\"å•é¡Œ: {sample['instruction'][:150]}...\")\n        print(f\"è§£ç­”: {sample.get('response', 'No response')[:150]}...\")\n        \n        print(\"\\nâœ… çµ±åˆãƒ»å¤‰æ›ãƒ•ã‚§ãƒ¼ã‚ºå®Œäº†\")\n    else:\n        print(\"âŒ çµ±åˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis_header"
   },
   "source": [
    "## ğŸ“Š Step 3: ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æãƒ»å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quality_analysis"
   },
   "outputs": [],
   "source": [
    "# å“è³ªåˆ†æå®Ÿè¡Œ\nif 'merged_data' not in locals() or not merged_data:\n    print(\"âš ï¸ åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Step 2ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\nelse:\n    if ENABLE_QUALITY_ANALYSIS:\n        print(\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå“è³ªåˆ†æé–‹å§‹\")\n        \n        # å“è³ªåˆ†æå®Ÿè¡Œ\n        analysis = analyze_dataset_quality(merged_data)\n        \n        # çµæœè¡¨ç¤º\n        print(f\"\\nğŸ“ˆ å“è³ªåˆ†æçµæœ:\")\n        print(f\"ğŸ“ ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {analysis['total_samples']}\")\n        \n        if analysis['instruction_lengths']:\n            print(f\"ğŸ“ å¹³å‡å•é¡Œæ–‡é•·: {np.mean(analysis['instruction_lengths']):.1f} æ–‡å­—\")\n        if analysis['response_lengths']:\n            print(f\"ğŸ“ å¹³å‡è§£ç­”é•·: {np.mean(analysis['response_lengths']):.1f} æ–‡å­—\")\n        \n        total = analysis['total_samples']\n        if total > 0:\n            print(f\"âŒ ç©ºè§£ç­”: {analysis['empty_responses']} ({analysis['empty_responses']/total*100:.1f}%)\")\n            print(f\"ğŸ§® æ•°å­¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰: {analysis['math_keywords']} ({analysis['math_keywords']/total*100:.1f}%)\")\n            print(f\"ğŸ§  æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³å«æœ‰: {analysis['reasoning_patterns']} ({analysis['reasoning_patterns']/total*100:.1f}%)\")\n        \n        print(f\"\\nğŸ“Š ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥çµ±è¨ˆ:\")\n        for domain, count in analysis['domains'].items():\n            percentage = count / total * 100 if total > 0 else 0\n            print(f\"  {domain}: {count}å•é¡Œ ({percentage:.1f}%)\")\n        \n        # å¯è¦–åŒ–ä½œæˆ\n        viz_path = create_quality_visualization(analysis)\n        \n        # åˆ†æçµæœä¿å­˜\n        analysis_file = f\"{OUTPUT_DIR}/quality_analysis.json\"\n        analysis_serializable = {\n            k: (dict(v) if isinstance(v, Counter) else v) \n            for k, v in analysis.items()\n        }\n        \n        with open(analysis_file, 'w', encoding='utf-8') as f:\n            json.dump(analysis_serializable, f, ensure_ascii=False, indent=2)\n        \n        log_message(f\"Analysis results saved: {analysis_file}\", \"SUCCESS\")\n        \n        print(\"\\nâœ… å“è³ªåˆ†æå®Œäº†\")\n    else:\n        print(\"â­ï¸ å“è³ªåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸï¼ˆè¨­å®šã«ã‚ˆã‚Šç„¡åŠ¹ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_header"
   },
   "source": [
    "## ğŸ“¥ Step 4: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "file_download"
   },
   "outputs": [],
   "source": [
    "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æº–å‚™\nfrom google.colab import files\nimport zipfile\n\n# ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§\noutput_files = []\nfor file_path in Path(OUTPUT_DIR).glob('*'):\n    if file_path.is_file():\n        output_files.append(str(file_path))\n\nprint(\"ğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:\")\nif output_files:\n    total_size = 0\n    for file_path in output_files:\n        filename = os.path.basename(file_path)\n        size = os.path.getsize(file_path) / 1024  # KB\n        total_size += size\n        print(f\"  ğŸ“„ {filename} ({size:.1f} KB)\")\n    \n    print(f\"\\nğŸ“Š ç·ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {total_size:.1f} KB\")\n    \n    # ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n    zip_file = f\"{OUTPUT_DIR}/{DATASET_NAME}_complete.zip\"\n    \n    print(f\"\\nğŸ“¦ çµ±åˆZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­...\")\n    with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file_path in output_files:\n            arcname = os.path.basename(file_path)\n            zipf.write(file_path, arcname)\n    \n    zip_size = os.path.getsize(zip_file) / 1024\n    print(f\"âœ… ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {zip_file} ({zip_size:.1f} KB)\")\n    \n    # è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n    print(f\"\\nğŸ“¥ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...\")\n    try:\n        files.download(zip_file)\n        print(\"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼\")\n    except Exception as e:\n        print(f\"âš ï¸ è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}\")\n        print(\"æ‰‹å‹•ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n    \n    # å€‹åˆ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³\n    print(f\"\\nğŸ’¡ å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰:\")\n    print(\"ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã§å€‹åˆ¥ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½:\")\n    for file_path in output_files:\n        if file_path.endswith(('.json', '.jsonl', '.png')):\n            filename = os.path.basename(file_path)\n            print(f\"# files.download('{file_path}')  # {filename}\")\nelse:\n    print(\"âš ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“\")\n    print(\"Step 1-3 ã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_header"
   },
   "source": [
    "## ğŸ‰ å®Ÿè¡Œå®Œäº†ã‚µãƒãƒªãƒ¼\n",
    "\n",
    "### ğŸ“Š ç”Ÿæˆçµæœ\n",
    "- **6ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: ä»£æ•°å­¦ã€å¿œç”¨æ•°å­¦ã€å¾®ç©åˆ†å­¦ã€é›¢æ•£æ•°å­¦ã€å¹¾ä½•å­¦ã€æ•°è«–\n",
    "- **å®Œå…¨å˜ç‹¬å®Ÿè¡Œ**: å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ä¾å­˜ãªã—\n",
    "- **çµ±åˆãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ¸ˆã¿**: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒãƒ©ãƒ³ã‚¹ä¿æŒ\n",
    "- **ShareGPTäº’æ›**: æ©Ÿæ¢°å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å¯¾å¿œ\n",
    "- **å“è³ªåˆ†æä»˜ã**: è‡ªå‹•å“è³ªè©•ä¾¡ãƒ»å¯è¦–åŒ–\n",
    "\n",
    "### ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "- `{DATASET_NAME}_XXX.json`: çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "- `{DATASET_NAME}_XXX_sharegpt.jsonl`: ShareGPTå½¢å¼\n",
    "- `quality_analysis.json`: å“è³ªåˆ†æçµæœ\n",
    "- `quality_analysis.png`: å¯è¦–åŒ–ã‚°ãƒ©ãƒ•\n",
    "- `{DATASET_NAME}_complete.zip`: å…¨ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆZIP\n",
    "\n",
    "### ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "1. **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´\n",
    "2. **å“è³ªè©•ä¾¡**: HLEè©¦é¨“å•é¡Œã§ã®æ€§èƒ½æ¸¬å®š  \n",
    "3. **ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—**: ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ\n",
    "4. **ãƒ‰ãƒ¡ã‚¤ãƒ³æ‹¡å¼µ**: è¿½åŠ æ•°å­¦åˆ†é‡ã®å¯¾å¿œ\n",
    "5. **ç¶™ç¶šæ”¹å–„**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãƒ»å†ç”Ÿæˆ\n",
    "\n",
    "### ğŸ”§ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã®ãƒ’ãƒ³ãƒˆ\n",
    "- **å•é¡Œæ•°èª¿æ•´**: `PROBLEMS_PER_DOMAIN` ã‚’å¤‰æ›´\n",
    "- **ãƒ¢ãƒ‡ãƒ«å¤‰æ›´**: `AUTO_MODEL_SELECTION = False` ã§æ‰‹å‹•é¸æŠ\n",
    "- **ãƒ‰ãƒ¡ã‚¤ãƒ³è¿½åŠ **: `DOMAINS` ãƒªã‚¹ãƒˆã«ãƒ‰ãƒ¡ã‚¤ãƒ³è¿½åŠ \n",
    "- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´**: ç”Ÿæˆæ¸©åº¦ãƒ»top_på€¤ã®èª¿æ•´\n",
    "\n",
    "### ğŸ“š å‚è€ƒè³‡æ–™\n",
    "- [Magpieè«–æ–‡](https://arxiv.org/abs/2406.08464)\n",
    "- [DeepSeek R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)\n",
    "- [vLLM Documentation](https://docs.vllm.ai/)\n",
    "\n",
    "**ğŸ¯ å˜ç‹¬å®Ÿè¡Œç‰ˆ6ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆå®Œäº†ï¼**\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¾å­˜ã›ãšã€Google Colabç’°å¢ƒã§å®Œå…¨ã«å‹•ä½œã—ã¾ã™ã€‚"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",\n  "colab": {\n   "gpuType": "T4",\n   "provenance": [],\n   "include_colab_link": true\n  },\n  "kernelspec": {\n   "display_name": "Python 3",\n   "name": "python3"\n  },\n  "language_info": {\n   "name": "python"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 0\n}