{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_header"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/your-repo/magpie/blob/main/colab_6domains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# ğŸ§® Magpie: 6ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ (Google Colabç‰ˆ)\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€DeepSeek R1ã‚’ä½¿ç”¨ã—ã¦6ã¤ã®æ•°å­¦ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã—ã€çµ±åˆãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹Google Colabç‰ˆã§ã™ã€‚\n",
    "\n",
    "## ğŸ¯ å¯¾å¿œãƒ‰ãƒ¡ã‚¤ãƒ³\n",
    "1. **Algebra** (ä»£æ•°å­¦): æ–¹ç¨‹å¼ã€å¤šé …å¼ã€é–¢æ•°\n",
    "2. **Applied Mathematics** (å¿œç”¨æ•°å­¦): å¾®åˆ†æ–¹ç¨‹å¼ã€æœ€é©åŒ–\n",
    "3. **Calculus** (å¾®ç©åˆ†å­¦): å¾®ç©åˆ†ã€æ¥µé™ã€ç´šæ•°\n",
    "4. **Discrete Mathematics** (é›¢æ•£æ•°å­¦): çµ„åˆã›ã€ã‚°ãƒ©ãƒ•ç†è«–\n",
    "5. **Geometry** (å¹¾ä½•å­¦): è§£æå¹¾ä½•ã€ç©ºé–“å›³å½¢\n",
    "6. **Number Theory** (æ•°è«–): ç´ æ•°ã€åˆåŒå¼ã€æš—å·å¿œç”¨\n",
    "\n",
    "## âš ï¸ é‡è¦äº‹é …\n",
    "- **GPUå¿…é ˆ**: A100æ¨å¥¨ã€T4ã§ã‚‚å‹•ä½œå¯èƒ½ï¼ˆè»½é‡ãƒ¢ãƒ¼ãƒ‰ï¼‰\n",
    "- **å®Ÿè¡Œæ™‚é–“**: å„ãƒ‰ãƒ¡ã‚¤ãƒ³10-50å•ç¨‹åº¦ã§30åˆ†-2æ™‚é–“\n",
    "- **ãƒ¡ãƒ¢ãƒªåˆ¶é™**: Colabã®åˆ¶é™ã«å¿œã˜ã¦å•é¡Œæ•°ã‚’èª¿æ•´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "settings_header"
   },
   "source": [
    "## âš™ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "user_settings"
   },
   "outputs": [],
   "source": [
    "# ===== ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š =====\n",
    "\n",
    "# åŸºæœ¬è¨­å®š\n",
    "DATASET_NAME = \"HLE_6Domains_Math\"  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå\n",
    "PROBLEMS_PER_DOMAIN = 20  # å„ãƒ‰ãƒ¡ã‚¤ãƒ³ã®å•é¡Œæ•°ï¼ˆColabç”¨ã«å°‘ãªã‚ï¼‰\n",
    "OUTPUT_DIR = \"/content/magpie_6domains\"  # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆGPUèƒ½åŠ›ã«å¿œã˜ã¦é¸æŠï¼‰\n",
    "USE_LIGHTWEIGHT_MODEL = True  # Trueã§Qwen2.5-3Bã€Falseã§DeepSeek R1\n",
    "\n",
    "if USE_LIGHTWEIGHT_MODEL:\n",
    "    MODEL_PATH = \"Qwen/Qwen2.5-3B-Instruct\"  # è»½é‡ãƒ¢ãƒ‡ãƒ«ï¼ˆT4 GPUå¯¾å¿œï¼‰\n",
    "    TENSOR_PARALLEL = 1\n",
    "    GPU_MEMORY_UTIL = 0.80\n",
    "    BATCH_SIZE = 10\n",
    "else:\n",
    "    MODEL_PATH = \"deepseek-ai/DeepSeek-R1\"  # ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆA100æ¨å¥¨ï¼‰\n",
    "    TENSOR_PARALLEL = 2  # Colabã®åˆ¶é™ã«åˆã‚ã›ã¦èª¿æ•´\n",
    "    GPU_MEMORY_UTIL = 0.90\n",
    "    BATCH_SIZE = 5\n",
    "\n",
    "# ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "INSTRUCTION_TEMP = 1.2\n",
    "INSTRUCTION_TOP_P = 1.0\n",
    "RESPONSE_TEMP = 0.1\n",
    "RESPONSE_TOP_P = 1.0\n",
    "\n",
    "# å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
    "DOMAINS = [\n",
    "    \"algebra\",\n",
    "    \"applied-mathematics\", \n",
    "    \"calculus\",\n",
    "    \"discrete-mathematics\",\n",
    "    \"geometry\",\n",
    "    \"number-theory\"\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“Š è¨­å®šå®Œäº†:\")\n",
    "print(f\"  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå: {DATASET_NAME}\")\n",
    "print(f\"  å„ãƒ‰ãƒ¡ã‚¤ãƒ³å•é¡Œæ•°: {PROBLEMS_PER_DOMAIN}\")\n",
    "print(f\"  ç·å•é¡Œæ•°: {len(DOMAINS) * PROBLEMS_PER_DOMAIN}\")\n",
    "print(f\"  ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {MODEL_PATH}\")\n",
    "print(f\"  è»½é‡ãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if USE_LIGHTWEIGHT_MODEL else 'ç„¡åŠ¹'}\")\n",
    "print(f\"  å‡ºåŠ›å…ˆ: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## ğŸš€ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# GPUç¢ºèª\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPUå: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "repo_setup"
   },
   "outputs": [],
   "source": [
    "# ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³ã¨ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!git clone https://github.com/your-repo/magpie.git\n",
    "%cd magpie\n",
    "\n",
    "# å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install -q vllm transformers torch accelerate\n",
    "!pip install -q datasets sentencepiece tiktoken\n",
    "!pip install -q numpy pandas tqdm matplotlib\n",
    "\n",
    "# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"âœ… ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auth_setup"
   },
   "outputs": [],
   "source": [
    "# Hugging Faceèªè¨¼ï¼ˆDeepSeek R1ä½¿ç”¨æ™‚å¿…è¦ï¼‰\n",
    "if not USE_LIGHTWEIGHT_MODEL:\n",
    "    from huggingface_hub import login\n",
    "    \n",
    "    print(\"DeepSeek R1ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã«Hugging Faceãƒˆãƒ¼ã‚¯ãƒ³ãŒå¿…è¦ã§ã™\")\n",
    "    print(\"https://huggingface.co/settings/tokens ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã—ã¦ãã ã•ã„\")\n",
    "    \n",
    "    # æ‰‹å‹•ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›\n",
    "    # login()  # å¯¾è©±çš„ãƒ­ã‚°ã‚¤ãƒ³\n",
    "    \n",
    "    print(\"âš ï¸ ä¸Šè¨˜ã®login()ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„\")\nelse:\n",
    "    print(\"âœ… è»½é‡ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ã®ãŸã‚èªè¨¼ä¸è¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generation_header"
   },
   "source": [
    "## ğŸ¯ Step 1: 6ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "domain_generation"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def run_domain_generation(domain, model_path, problems, timestamp):\n",
    "    \"\"\"å˜ä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\"\"\"\n",
    "    print(f\"\\nğŸ”„ {domain} ãƒ‰ãƒ¡ã‚¤ãƒ³ç”Ÿæˆé–‹å§‹...\")\n",
    "    \n",
    "    # å•é¡Œç”Ÿæˆ\n",
    "    ins_cmd = [\n",
    "        \"python\", \"exp/gen_ins.py\",\n",
    "        \"--model_path\", model_path,\n",
    "        \"--total_prompts\", str(problems),\n",
    "        \"--temperature\", str(INSTRUCTION_TEMP),\n",
    "        \"--top_p\", str(INSTRUCTION_TOP_P),\n",
    "        \"--tensor_parallel_size\", str(TENSOR_PARALLEL),\n",
    "        \"--gpu_memory_utilization\", str(GPU_MEMORY_UTIL),\n",
    "        \"--control_tasks\", \"math\",\n",
    "        \"--domain\", domain,\n",
    "        \"--n\", str(BATCH_SIZE),\n",
    "        \"--job_name\", f\"Colab-{domain}\",\n",
    "        \"--timestamp\", str(timestamp),\n",
    "        \"--max_tokens\", \"3072\",\n",
    "        \"--max_model_len\", \"8192\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(ins_cmd, capture_output=True, text=True, check=True)\n",
    "        print(f\"âœ… {domain} å•é¡Œç”Ÿæˆå®Œäº†\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ {domain} å•é¡Œç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e.stderr[:500]}\")\n",
    "        return False\n",
    "    \n",
    "    # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    ins_file = f\"data/Colab-{domain}/Magpie_{model_name}_{problems}_{timestamp}_ins.json\"\n",
    "    \n",
    "    if not os.path.exists(ins_file):\n",
    "        print(f\"âŒ {domain} å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {ins_file}\")\n",
    "        return False\n",
    "    \n",
    "    # è§£ç­”ç”Ÿæˆ\n",
    "    res_cmd = [\n",
    "        \"python\", \"exp/gen_res.py\",\n",
    "        \"--model_path\", model_path,\n",
    "        \"--input_file\", ins_file,\n",
    "        \"--temperature\", str(RESPONSE_TEMP),\n",
    "        \"--top_p\", str(RESPONSE_TOP_P),\n",
    "        \"--tensor_parallel_size\", str(TENSOR_PARALLEL),\n",
    "        \"--gpu_memory_utilization\", str(GPU_MEMORY_UTIL),\n",
    "        \"--batch_size\", str(BATCH_SIZE),\n",
    "        \"--repetition_penalty\", \"1.0\",\n",
    "        \"--use_tokenizer_template\",\n",
    "        \"--offline\",\n",
    "        \"--max_tokens\", \"4096\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(res_cmd, capture_output=True, text=True, check=True)\n",
    "        print(f\"âœ… {domain} è§£ç­”ç”Ÿæˆå®Œäº†\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ {domain} è§£ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e.stderr[:500]}\")\n",
    "        return False\n",
    "\n",
    "# ç”Ÿæˆå®Ÿè¡Œ\n",
    "timestamp = int(time.time())\n",
    "success_domains = []\n",
    "failed_domains = []\n",
    "\n",
    "print(f\"ğŸš€ 6ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–‹å§‹\")\n",
    "print(f\"ğŸ“Š è¨­å®š: {MODEL_PATH}, å„{PROBLEMS_PER_DOMAIN}å•é¡Œ\")\n",
    "\n",
    "for i, domain in enumerate(DOMAINS):\n",
    "    print(f\"\\nğŸ“ˆ é€²æ—: {i+1}/{len(DOMAINS)} - {domain}\")\n",
    "    \n",
    "    if run_domain_generation(domain, MODEL_PATH, PROBLEMS_PER_DOMAIN, timestamp):\n",
    "        success_domains.append(domain)\n",
    "        print(f\"âœ… {domain} å®Œäº†\")\n",
    "    else:\n",
    "        failed_domains.append(domain)\n",
    "        print(f\"âŒ {domain} å¤±æ•—\")\n",
    "    \n",
    "    # GPU cooldown\n",
    "    if i < len(DOMAINS) - 1:\n",
    "        print(\"â¸ï¸ GPU cooldown...\")\n",
    "        time.sleep(10)\n",
    "\n",
    "print(f\"\\nğŸ“Š ç”Ÿæˆçµæœ:\")\n",
    "print(f\"  æˆåŠŸ: {len(success_domains)}/{len(DOMAINS)} ãƒ‰ãƒ¡ã‚¤ãƒ³\")\n",
    "print(f\"  æˆåŠŸãƒ‰ãƒ¡ã‚¤ãƒ³: {success_domains}\")\nif failed_domains:\n",
    "    print(f\"  å¤±æ•—ãƒ‰ãƒ¡ã‚¤ãƒ³: {failed_domains}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "merge_header"
   },
   "source": [
    "## ğŸ”„ Step 2: ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_merge"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def find_generated_files():\n",
    "    \"\"\"ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•æ¤œå‡º\"\"\"\n",
    "    domain_files = {}\n",
    "    model_name = MODEL_PATH.split(\"/\")[-1]\n",
    "    \n",
    "    for domain in success_domains:\n",
    "        pattern = f\"data/Colab-{domain}/Magpie_{model_name}_*_ins_res.json\"\n",
    "        files = glob.glob(pattern)\n",
    "        \n",
    "        if files:\n",
    "            domain_files[domain] = files[0]\n",
    "            print(f\"ğŸ“ {domain}: {files[0]}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ {domain}: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "    \n",
    "    return domain_files\n",
    "\n",
    "def merge_and_shuffle_colab(domain_files):\n",
    "    \"\"\"Colabç‰ˆãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«\"\"\"\n",
    "    all_data = []\n",
    "    domain_stats = {}\n",
    "    \n",
    "    print(\"ğŸ“Š ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "    \n",
    "    for domain, filepath in domain_files.items():\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                domain_data = json.load(f)\n",
    "            \n",
    "            # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ \n",
    "            for item in domain_data:\n",
    "                item['domain'] = domain\n",
    "                item['source'] = 'colab-magpie'\n",
    "                item['dataset_version'] = '1.0'\n",
    "            \n",
    "            all_data.extend(domain_data)\n",
    "            domain_stats[domain] = len(domain_data)\n",
    "            \n",
    "            print(f\"  âœ… {domain}: {len(domain_data)}å•é¡Œ\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {domain}: ã‚¨ãƒ©ãƒ¼ - {e}\")\n",
    "            continue\n",
    "    \n",
    "    # çµ±è¨ˆè¡¨ç¤º\n",
    "    total_problems = sum(domain_stats.values())\n",
    "    print(f\"\\nğŸ“ˆ çµ±è¨ˆ:\")\n",
    "    for domain, count in domain_stats.items():\n",
    "        percentage = (count / total_problems) * 100 if total_problems > 0 else 0\n",
    "        print(f\"  {domain}: {count}å•é¡Œ ({percentage:.1f}%)\")\n",
    "    print(f\"  åˆè¨ˆ: {total_problems}å•é¡Œ\")\n",
    "    \n",
    "    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "    print(\"\\nğŸ”€ ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ£ãƒƒãƒ•ãƒ«ä¸­...\")\n",
    "    random.seed(42)  # å†ç¾æ€§ã®ãŸã‚\n",
    "    random.shuffle(all_data)\n",
    "    \n",
    "    return all_data, domain_stats\n",
    "\n",
    "def create_sharegpt_format(data):\n",
    "    \"\"\"ShareGPTå½¢å¼ã«å¤‰æ›\"\"\"\n",
    "    sharegpt_data = []\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        sharegpt_entry = {\n",
    "            \"conversation_id\": f\"colab-magpie-{i}\",\n",
    "            \"domain\": item.get('domain', 'unknown'),\n",
    "            \"source\": item.get('source', 'colab-magpie'),\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": item['instruction']},\n",
    "                {\"from\": \"gpt\", \"value\": item['response']}\n",
    "            ],\n",
    "            \"gen_input_configs\": item.get('gen_input_configs', {}),\n",
    "            \"gen_response_configs\": item.get('gen_response_configs', {}),\n",
    "            \"created\": item.get('created', ''),\n",
    "            \"id\": item.get('id', i)\n",
    "        }\n",
    "        sharegpt_data.append(sharegpt_entry)\n",
    "    \n",
    "    return sharegpt_data\n",
    "\n",
    "# çµ±åˆå®Ÿè¡Œ\n",
    "if success_domains:\n",
    "    print(\"ğŸ”„ ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«é–‹å§‹\")\n",
    "    \n",
    "    domain_files = find_generated_files()\n",
    "    \n",
    "    if domain_files:\n",
    "        merged_data, stats = merge_and_shuffle_colab(domain_files)\n",
    "        \n",
    "        # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜\n",
    "        timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        merged_file = f\"{OUTPUT_DIR}/{DATASET_NAME}_{len(merged_data)}_{timestamp_str}.json\"\n",
    "        \n",
    "        with open(merged_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"âœ… çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜: {merged_file}\")\n",
    "        \n",
    "        # ShareGPTå½¢å¼\n",
    "        sharegpt_data = create_sharegpt_format(merged_data)\n",
    "        sharegpt_file = f\"{OUTPUT_DIR}/{DATASET_NAME}_{len(merged_data)}_{timestamp_str}_sharegpt.jsonl\"\n",
    "        \n",
    "        with open(sharegpt_file, 'w', encoding='utf-8') as f:\n",
    "            for item in sharegpt_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"âœ… ShareGPTå½¢å¼ä¿å­˜: {sharegpt_file}\")\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n",
    "        if merged_data:\n",
    "            print(\"\\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:\")\n",
    "            sample = merged_data[0]\n",
    "            print(f\"ãƒ‰ãƒ¡ã‚¤ãƒ³: {sample['domain']}\")\n",
    "            print(f\"å•é¡Œ: {sample['instruction'][:100]}...\")\n",
    "            print(f\"è§£ç­”: {sample['response'][:100]}...\")\n",
    "    \n",
    "    else:\n",
    "        print(\"âŒ çµ±åˆå¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\nelse:\n",
    "    print(\"âš ï¸ æˆåŠŸã—ãŸãƒ‰ãƒ¡ã‚¤ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Step 1ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis_header"
   },
   "source": [
    "## ğŸ“Š Step 3: ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quality_analysis"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def analyze_dataset_quality(data):\n",
    "    \"\"\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå“è³ªåˆ†æ\"\"\"\n",
    "    analysis = {\n",
    "        \"total_samples\": len(data),\n",
    "        \"domains\": Counter(),\n",
    "        \"instruction_lengths\": [],\n",
    "        \"response_lengths\": [],\n",
    "        \"math_keywords\": 0,\n",
    "        \"reasoning_patterns\": 0\n",
    "    }\n",
    "    \n",
    "    math_keywords = ['equation', 'solve', 'calculate', 'theorem', 'proof', \n",
    "                     'æ–¹ç¨‹å¼', 'è¨ˆç®—', 'å®šç†', 'è¨¼æ˜', 'derivative', 'integral']\n",
    "    reasoning_patterns = ['step', 'first', 'then', 'therefore', 'because',\n",
    "                         'ã‚¹ãƒ†ãƒƒãƒ—', 'ã¾ãš', 'ãã—ã¦', 'ã—ãŸãŒã£ã¦']\n",
    "    \n",
    "    for item in data:\n",
    "        domain = item.get('domain', 'unknown')\n",
    "        instruction = item.get('instruction', '')\n",
    "        response = item.get('response', '')\n",
    "        \n",
    "        analysis['domains'][domain] += 1\n",
    "        analysis['instruction_lengths'].append(len(instruction))\n",
    "        analysis['response_lengths'].append(len(response))\n",
    "        \n",
    "        # æ•°å­¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º\n",
    "        text = (instruction + ' ' + response).lower()\n",
    "        if any(keyword in text for keyword in math_keywords):\n",
    "            analysis['math_keywords'] += 1\n",
    "        \n",
    "        # æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º\n",
    "        if any(pattern in response.lower() for pattern in reasoning_patterns):\n",
    "            analysis['reasoning_patterns'] += 1\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def visualize_analysis(analysis):\n",
    "    \"\"\"åˆ†æçµæœã®å¯è¦–åŒ–\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†å¸ƒ\n",
    "    domains = list(analysis['domains'].keys())\n",
    "    counts = list(analysis['domains'].values())\n",
    "    \n",
    "    axes[0,0].pie(counts, labels=domains, autopct='%1.1f%%')\n",
    "    axes[0,0].set_title('ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†å¸ƒ')\n",
    "    \n",
    "    # å•é¡Œæ–‡é•·åˆ†å¸ƒ\n",
    "    axes[0,1].hist(analysis['instruction_lengths'], bins=20, alpha=0.7)\n",
    "    axes[0,1].set_title('å•é¡Œæ–‡é•·åˆ†å¸ƒ')\n",
    "    axes[0,1].set_xlabel('æ–‡å­—æ•°')\n",
    "    axes[0,1].set_ylabel('é »åº¦')\n",
    "    \n",
    "    # è§£ç­”é•·åˆ†å¸ƒ\n",
    "    axes[1,0].hist(analysis['response_lengths'], bins=20, alpha=0.7, color='orange')\n",
    "    axes[1,0].set_title('è§£ç­”é•·åˆ†å¸ƒ')\n",
    "    axes[1,0].set_xlabel('æ–‡å­—æ•°')\n",
    "    axes[1,0].set_ylabel('é »åº¦')\n",
    "    \n",
    "    # å“è³ªæŒ‡æ¨™\n",
    "    total = analysis['total_samples']\n",
    "    metrics = ['æ•°å­¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³']\n",
    "    values = [analysis['math_keywords']/total*100, analysis['reasoning_patterns']/total*100]\n",
    "    \n",
    "    axes[1,1].bar(metrics, values)\n",
    "    axes[1,1].set_title('å“è³ªæŒ‡æ¨™ (%)')\n",
    "    axes[1,1].set_ylabel('å«æœ‰ç‡ (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/quality_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# å“è³ªåˆ†æå®Ÿè¡Œ\n",
    "if 'merged_data' in locals() and merged_data:\n",
    "    print(\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå“è³ªåˆ†æä¸­...\")\n",
    "    \n",
    "    analysis = analyze_dataset_quality(merged_data)\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ å“è³ªåˆ†æçµæœ:\")\n",
    "    print(f\"  ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {analysis['total_samples']}\")\n",
    "    print(f\"  å¹³å‡å•é¡Œæ–‡é•·: {np.mean(analysis['instruction_lengths']):.1f} æ–‡å­—\")\n",
    "    print(f\"  å¹³å‡è§£ç­”é•·: {np.mean(analysis['response_lengths']):.1f} æ–‡å­—\")\n",
    "    print(f\"  æ•°å­¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ç‡: {analysis['math_keywords']/analysis['total_samples']*100:.1f}%\")\n",
    "    print(f\"  æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³å«æœ‰ç‡: {analysis['reasoning_patterns']/analysis['total_samples']*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\nğŸ“Š ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥çµ±è¨ˆ:\")\n",
    "    for domain, count in analysis['domains'].items():\n",
    "        percentage = count / analysis['total_samples'] * 100\n",
    "        print(f\"  {domain}: {count}å•é¡Œ ({percentage:.1f}%)\")\n",
    "    \n",
    "    # å¯è¦–åŒ–\n",
    "    visualize_analysis(analysis)\n",
    "    \n",
    "    # åˆ†æçµæœä¿å­˜\n",
    "    analysis_file = f\"{OUTPUT_DIR}/quality_analysis.json\"\n",
    "    analysis_serializable = {\n",
    "        k: (dict(v) if isinstance(v, Counter) else v) \n",
    "        for k, v in analysis.items()\n",
    "    }\n",
    "    \n",
    "    with open(analysis_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis_serializable, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… åˆ†æçµæœä¿å­˜: {analysis_file}\")\n",
    "\nelse:\n",
    "    print(\"âš ï¸ åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Step 2ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_header"
   },
   "source": [
    "## ğŸ“¥ Step 4: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "file_download"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "# ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§\n",
    "output_files = []\n",
    "for file_path in Path(OUTPUT_DIR).glob('*'):\n",
    "    if file_path.is_file():\n",
    "        output_files.append(str(file_path))\n",
    "\n",
    "print(\"ğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
    "for file_path in output_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    size = os.path.getsize(file_path) / 1024  # KB\n",
    "    print(f\"  ğŸ“„ {filename} ({size:.1f} KB)\")\n",
    "\n",
    "if output_files:\n",
    "    # ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n",
    "    zip_file = f\"{OUTPUT_DIR}/{DATASET_NAME}_complete.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
    "        for file_path in output_files:\n",
    "            arcname = os.path.basename(file_path)\n",
    "            zipf.write(file_path, arcname)\n",
    "    \n",
    "    print(f\"\\nğŸ“¦ çµ±åˆZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {zip_file}\")\n",
    "    print(f\"ğŸ“Š ZIPã‚µã‚¤ã‚º: {os.path.getsize(zip_file) / 1024:.1f} KB\")\n",
    "    \n",
    "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "    print(\"\\nğŸ“¥ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "    files.download(zip_file)\n",
    "    \n",
    "    print(\"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼\")\n",
    "    \n",
    "    # å€‹åˆ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³\n",
    "    print(\"\\nğŸ’¡ å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰:\")\n",
    "    print(\"ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã§å€‹åˆ¥ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™:\")\n",
    "    print(\"```python\")\n",
    "    for file_path in output_files:\n",
    "        if file_path.endswith('.json') or file_path.endswith('.jsonl'):\n",
    "            print(f\"# files.download('{file_path}')\")\n",
    "    print(\"```\")\n",
    "\nelse:\n",
    "    print(\"âš ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_header"
   },
   "source": [
    "## ğŸ‰ å®Œäº†ã‚µãƒãƒªãƒ¼\n",
    "\n",
    "### ğŸ“Š ç”Ÿæˆçµæœ\n",
    "- **6ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: ä»£æ•°å­¦ã€å¿œç”¨æ•°å­¦ã€å¾®ç©åˆ†å­¦ã€é›¢æ•£æ•°å­¦ã€å¹¾ä½•å­¦ã€æ•°è«–\n",
    "- **çµ±åˆãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ¸ˆã¿**: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒãƒ©ãƒ³ã‚¹ä¿æŒ\n",
    "- **ShareGPTäº’æ›**: æ©Ÿæ¢°å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å¯¾å¿œ\n",
    "\n",
    "### ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "- `{DATASET_NAME}_XXX.json`: çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "- `{DATASET_NAME}_XXX_sharegpt.jsonl`: ShareGPTå½¢å¼\n",
    "- `quality_analysis.json`: å“è³ªåˆ†æçµæœ\n",
    "- `quality_analysis.png`: å¯è¦–åŒ–ã‚°ãƒ©ãƒ•\n",
    "\n",
    "### ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "1. **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´\n",
    "2. **å“è³ªè©•ä¾¡**: HLEè©¦é¨“å•é¡Œã§ã®æ€§èƒ½æ¸¬å®š\n",
    "3. **ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—**: ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ\n",
    "4. **ãƒ‰ãƒ¡ã‚¤ãƒ³æ‹¡å¼µ**: è¿½åŠ æ•°å­¦åˆ†é‡ã®å¯¾å¿œ\n",
    "\n",
    "### ğŸ“š å‚è€ƒè³‡æ–™\n",
    "- [Magpieè«–æ–‡](https://arxiv.org/abs/2406.08464)\n",
    "- [DeepSeek R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)\n",
    "- [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆGitHub](https://github.com/your-repo/magpie)\n",
    "\n",
    "**ğŸ¯ Colabç‰ˆ6ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆå®Œäº†ï¼**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}