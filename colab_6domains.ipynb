{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_header"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/your-repo/magpie/blob/main/colab_6domains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# 🧮 Magpie: 6ドメイン数学データセット生成 (Google Colab版)\n",
    "\n",
    "このノートブックは、DeepSeek R1を使用して6つの数学ドメイン別データセットを生成し、統合・シャッフルするGoogle Colab版です。\n",
    "\n",
    "## 🎯 対応ドメイン\n",
    "1. **Algebra** (代数学): 方程式、多項式、関数\n",
    "2. **Applied Mathematics** (応用数学): 微分方程式、最適化\n",
    "3. **Calculus** (微積分学): 微積分、極限、級数\n",
    "4. **Discrete Mathematics** (離散数学): 組合せ、グラフ理論\n",
    "5. **Geometry** (幾何学): 解析幾何、空間図形\n",
    "6. **Number Theory** (数論): 素数、合同式、暗号応用\n",
    "\n",
    "## ⚠️ 重要事項\n",
    "- **GPU必須**: A100推奨、T4でも動作可能（軽量モード）\n",
    "- **実行時間**: 各ドメイン10-50問程度で30分-2時間\n",
    "- **メモリ制限**: Colabの制限に応じて問題数を調整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "settings_header"
   },
   "source": [
    "## ⚙️ ユーザー設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "user_settings"
   },
   "outputs": [],
   "source": [
    "# ===== ユーザー設定 =====\n",
    "\n",
    "# 基本設定\n",
    "DATASET_NAME = \"HLE_6Domains_Math\"  # データセット名\n",
    "PROBLEMS_PER_DOMAIN = 20  # 各ドメインの問題数（Colab用に少なめ）\n",
    "OUTPUT_DIR = \"/content/magpie_6domains\"  # 出力ディレクトリ\n",
    "\n",
    "# モデル設定（GPU能力に応じて選択）\n",
    "USE_LIGHTWEIGHT_MODEL = True  # TrueでQwen2.5-3B、FalseでDeepSeek R1\n",
    "\n",
    "if USE_LIGHTWEIGHT_MODEL:\n",
    "    MODEL_PATH = \"Qwen/Qwen2.5-3B-Instruct\"  # 軽量モデル（T4 GPU対応）\n",
    "    TENSOR_PARALLEL = 1\n",
    "    GPU_MEMORY_UTIL = 0.80\n",
    "    BATCH_SIZE = 10\n",
    "else:\n",
    "    MODEL_PATH = \"deepseek-ai/DeepSeek-R1\"  # フルモデル（A100推奨）\n",
    "    TENSOR_PARALLEL = 2  # Colabの制限に合わせて調整\n",
    "    GPU_MEMORY_UTIL = 0.90\n",
    "    BATCH_SIZE = 5\n",
    "\n",
    "# 生成パラメータ\n",
    "INSTRUCTION_TEMP = 1.2\n",
    "INSTRUCTION_TOP_P = 1.0\n",
    "RESPONSE_TEMP = 0.1\n",
    "RESPONSE_TOP_P = 1.0\n",
    "\n",
    "# 対象ドメイン\n",
    "DOMAINS = [\n",
    "    \"algebra\",\n",
    "    \"applied-mathematics\", \n",
    "    \"calculus\",\n",
    "    \"discrete-mathematics\",\n",
    "    \"geometry\",\n",
    "    \"number-theory\"\n",
    "]\n",
    "\n",
    "print(f\"📊 設定完了:\")\n",
    "print(f\"  データセット名: {DATASET_NAME}\")\n",
    "print(f\"  各ドメイン問題数: {PROBLEMS_PER_DOMAIN}\")\n",
    "print(f\"  総問題数: {len(DOMAINS) * PROBLEMS_PER_DOMAIN}\")\n",
    "print(f\"  使用モデル: {MODEL_PATH}\")\n",
    "print(f\"  軽量モード: {'有効' if USE_LIGHTWEIGHT_MODEL else '無効'}\")\n",
    "print(f\"  出力先: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## 🚀 環境セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# GPU確認\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA利用可能: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU名: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPUメモリ: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "repo_setup"
   },
   "outputs": [],
   "source": [
    "# リポジトリクローンと依存関係インストール\n",
    "!git clone https://github.com/your-repo/magpie.git\n",
    "%cd magpie\n",
    "\n",
    "# 必要パッケージのインストール\n",
    "!pip install -q vllm transformers torch accelerate\n",
    "!pip install -q datasets sentencepiece tiktoken\n",
    "!pip install -q numpy pandas tqdm matplotlib\n",
    "\n",
    "# 出力ディレクトリ作成\n",
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"✅ 環境セットアップ完了: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auth_setup"
   },
   "outputs": [],
   "source": [
    "# Hugging Face認証（DeepSeek R1使用時必要）\n",
    "if not USE_LIGHTWEIGHT_MODEL:\n",
    "    from huggingface_hub import login\n",
    "    \n",
    "    print(\"DeepSeek R1を使用するためにHugging Faceトークンが必要です\")\n",
    "    print(\"https://huggingface.co/settings/tokens でトークンを取得してください\")\n",
    "    \n",
    "    # 手動でトークンを入力\n",
    "    # login()  # 対話的ログイン\n",
    "    \n",
    "    print(\"⚠️ 上記のlogin()のコメントアウトを外して実行してください\")\nelse:\n",
    "    print(\"✅ 軽量モデル使用のため認証不要\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generation_header"
   },
   "source": [
    "## 🎯 Step 1: 6ドメイン別データ生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "domain_generation"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def run_domain_generation(domain, model_path, problems, timestamp):\n",
    "    \"\"\"単一ドメインのデータ生成\"\"\"\n",
    "    print(f\"\\n🔄 {domain} ドメイン生成開始...\")\n",
    "    \n",
    "    # 問題生成\n",
    "    ins_cmd = [\n",
    "        \"python\", \"exp/gen_ins.py\",\n",
    "        \"--model_path\", model_path,\n",
    "        \"--total_prompts\", str(problems),\n",
    "        \"--temperature\", str(INSTRUCTION_TEMP),\n",
    "        \"--top_p\", str(INSTRUCTION_TOP_P),\n",
    "        \"--tensor_parallel_size\", str(TENSOR_PARALLEL),\n",
    "        \"--gpu_memory_utilization\", str(GPU_MEMORY_UTIL),\n",
    "        \"--control_tasks\", \"math\",\n",
    "        \"--domain\", domain,\n",
    "        \"--n\", str(BATCH_SIZE),\n",
    "        \"--job_name\", f\"Colab-{domain}\",\n",
    "        \"--timestamp\", str(timestamp),\n",
    "        \"--max_tokens\", \"3072\",\n",
    "        \"--max_model_len\", \"8192\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(ins_cmd, capture_output=True, text=True, check=True)\n",
    "        print(f\"✅ {domain} 問題生成完了\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ {domain} 問題生成エラー: {e.stderr[:500]}\")\n",
    "        return False\n",
    "    \n",
    "    # 生成されたファイルを確認\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    ins_file = f\"data/Colab-{domain}/Magpie_{model_name}_{problems}_{timestamp}_ins.json\"\n",
    "    \n",
    "    if not os.path.exists(ins_file):\n",
    "        print(f\"❌ {domain} 問題ファイルが見つかりません: {ins_file}\")\n",
    "        return False\n",
    "    \n",
    "    # 解答生成\n",
    "    res_cmd = [\n",
    "        \"python\", \"exp/gen_res.py\",\n",
    "        \"--model_path\", model_path,\n",
    "        \"--input_file\", ins_file,\n",
    "        \"--temperature\", str(RESPONSE_TEMP),\n",
    "        \"--top_p\", str(RESPONSE_TOP_P),\n",
    "        \"--tensor_parallel_size\", str(TENSOR_PARALLEL),\n",
    "        \"--gpu_memory_utilization\", str(GPU_MEMORY_UTIL),\n",
    "        \"--batch_size\", str(BATCH_SIZE),\n",
    "        \"--repetition_penalty\", \"1.0\",\n",
    "        \"--use_tokenizer_template\",\n",
    "        \"--offline\",\n",
    "        \"--max_tokens\", \"4096\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(res_cmd, capture_output=True, text=True, check=True)\n",
    "        print(f\"✅ {domain} 解答生成完了\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ {domain} 解答生成エラー: {e.stderr[:500]}\")\n",
    "        return False\n",
    "\n",
    "# 生成実行\n",
    "timestamp = int(time.time())\n",
    "success_domains = []\n",
    "failed_domains = []\n",
    "\n",
    "print(f\"🚀 6ドメインデータ生成開始\")\n",
    "print(f\"📊 設定: {MODEL_PATH}, 各{PROBLEMS_PER_DOMAIN}問題\")\n",
    "\n",
    "for i, domain in enumerate(DOMAINS):\n",
    "    print(f\"\\n📈 進捗: {i+1}/{len(DOMAINS)} - {domain}\")\n",
    "    \n",
    "    if run_domain_generation(domain, MODEL_PATH, PROBLEMS_PER_DOMAIN, timestamp):\n",
    "        success_domains.append(domain)\n",
    "        print(f\"✅ {domain} 完了\")\n",
    "    else:\n",
    "        failed_domains.append(domain)\n",
    "        print(f\"❌ {domain} 失敗\")\n",
    "    \n",
    "    # GPU cooldown\n",
    "    if i < len(DOMAINS) - 1:\n",
    "        print(\"⏸️ GPU cooldown...\")\n",
    "        time.sleep(10)\n",
    "\n",
    "print(f\"\\n📊 生成結果:\")\n",
    "print(f\"  成功: {len(success_domains)}/{len(DOMAINS)} ドメイン\")\n",
    "print(f\"  成功ドメイン: {success_domains}\")\nif failed_domains:\n",
    "    print(f\"  失敗ドメイン: {failed_domains}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "merge_header"
   },
   "source": [
    "## 🔄 Step 2: データ統合・シャッフル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_merge"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def find_generated_files():\n",
    "    \"\"\"生成されたファイルを自動検出\"\"\"\n",
    "    domain_files = {}\n",
    "    model_name = MODEL_PATH.split(\"/\")[-1]\n",
    "    \n",
    "    for domain in success_domains:\n",
    "        pattern = f\"data/Colab-{domain}/Magpie_{model_name}_*_ins_res.json\"\n",
    "        files = glob.glob(pattern)\n",
    "        \n",
    "        if files:\n",
    "            domain_files[domain] = files[0]\n",
    "            print(f\"📁 {domain}: {files[0]}\")\n",
    "        else:\n",
    "            print(f\"⚠️ {domain}: ファイルが見つかりません\")\n",
    "    \n",
    "    return domain_files\n",
    "\n",
    "def merge_and_shuffle_colab(domain_files):\n",
    "    \"\"\"Colab版データ統合・シャッフル\"\"\"\n",
    "    all_data = []\n",
    "    domain_stats = {}\n",
    "    \n",
    "    print(\"📊 ドメイン別データ読み込み中...\")\n",
    "    \n",
    "    for domain, filepath in domain_files.items():\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                domain_data = json.load(f)\n",
    "            \n",
    "            # ドメインメタデータ追加\n",
    "            for item in domain_data:\n",
    "                item['domain'] = domain\n",
    "                item['source'] = 'colab-magpie'\n",
    "                item['dataset_version'] = '1.0'\n",
    "            \n",
    "            all_data.extend(domain_data)\n",
    "            domain_stats[domain] = len(domain_data)\n",
    "            \n",
    "            print(f\"  ✅ {domain}: {len(domain_data)}問題\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ {domain}: エラー - {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 統計表示\n",
    "    total_problems = sum(domain_stats.values())\n",
    "    print(f\"\\n📈 統計:\")\n",
    "    for domain, count in domain_stats.items():\n",
    "        percentage = (count / total_problems) * 100 if total_problems > 0 else 0\n",
    "        print(f\"  {domain}: {count}問題 ({percentage:.1f}%)\")\n",
    "    print(f\"  合計: {total_problems}問題\")\n",
    "    \n",
    "    # シャッフル\n",
    "    print(\"\\n🔀 データシャッフル中...\")\n",
    "    random.seed(42)  # 再現性のため\n",
    "    random.shuffle(all_data)\n",
    "    \n",
    "    return all_data, domain_stats\n",
    "\n",
    "def create_sharegpt_format(data):\n",
    "    \"\"\"ShareGPT形式に変換\"\"\"\n",
    "    sharegpt_data = []\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        sharegpt_entry = {\n",
    "            \"conversation_id\": f\"colab-magpie-{i}\",\n",
    "            \"domain\": item.get('domain', 'unknown'),\n",
    "            \"source\": item.get('source', 'colab-magpie'),\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": item['instruction']},\n",
    "                {\"from\": \"gpt\", \"value\": item['response']}\n",
    "            ],\n",
    "            \"gen_input_configs\": item.get('gen_input_configs', {}),\n",
    "            \"gen_response_configs\": item.get('gen_response_configs', {}),\n",
    "            \"created\": item.get('created', ''),\n",
    "            \"id\": item.get('id', i)\n",
    "        }\n",
    "        sharegpt_data.append(sharegpt_entry)\n",
    "    \n",
    "    return sharegpt_data\n",
    "\n",
    "# 統合実行\n",
    "if success_domains:\n",
    "    print(\"🔄 データ統合・シャッフル開始\")\n",
    "    \n",
    "    domain_files = find_generated_files()\n",
    "    \n",
    "    if domain_files:\n",
    "        merged_data, stats = merge_and_shuffle_colab(domain_files)\n",
    "        \n",
    "        # 統合ファイル保存\n",
    "        timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        merged_file = f\"{OUTPUT_DIR}/{DATASET_NAME}_{len(merged_data)}_{timestamp_str}.json\"\n",
    "        \n",
    "        with open(merged_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"✅ 統合データ保存: {merged_file}\")\n",
    "        \n",
    "        # ShareGPT形式\n",
    "        sharegpt_data = create_sharegpt_format(merged_data)\n",
    "        sharegpt_file = f\"{OUTPUT_DIR}/{DATASET_NAME}_{len(merged_data)}_{timestamp_str}_sharegpt.jsonl\"\n",
    "        \n",
    "        with open(sharegpt_file, 'w', encoding='utf-8') as f:\n",
    "            for item in sharegpt_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"✅ ShareGPT形式保存: {sharegpt_file}\")\n",
    "        \n",
    "        # サンプル表示\n",
    "        if merged_data:\n",
    "            print(\"\\n📝 サンプルデータ:\")\n",
    "            sample = merged_data[0]\n",
    "            print(f\"ドメイン: {sample['domain']}\")\n",
    "            print(f\"問題: {sample['instruction'][:100]}...\")\n",
    "            print(f\"解答: {sample['response'][:100]}...\")\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ 統合可能なファイルが見つかりません\")\nelse:\n",
    "    print(\"⚠️ 成功したドメインがありません。Step 1を確認してください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis_header"
   },
   "source": [
    "## 📊 Step 3: データ品質分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quality_analysis"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def analyze_dataset_quality(data):\n",
    "    \"\"\"データセット品質分析\"\"\"\n",
    "    analysis = {\n",
    "        \"total_samples\": len(data),\n",
    "        \"domains\": Counter(),\n",
    "        \"instruction_lengths\": [],\n",
    "        \"response_lengths\": [],\n",
    "        \"math_keywords\": 0,\n",
    "        \"reasoning_patterns\": 0\n",
    "    }\n",
    "    \n",
    "    math_keywords = ['equation', 'solve', 'calculate', 'theorem', 'proof', \n",
    "                     '方程式', '計算', '定理', '証明', 'derivative', 'integral']\n",
    "    reasoning_patterns = ['step', 'first', 'then', 'therefore', 'because',\n",
    "                         'ステップ', 'まず', 'そして', 'したがって']\n",
    "    \n",
    "    for item in data:\n",
    "        domain = item.get('domain', 'unknown')\n",
    "        instruction = item.get('instruction', '')\n",
    "        response = item.get('response', '')\n",
    "        \n",
    "        analysis['domains'][domain] += 1\n",
    "        analysis['instruction_lengths'].append(len(instruction))\n",
    "        analysis['response_lengths'].append(len(response))\n",
    "        \n",
    "        # 数学キーワード検出\n",
    "        text = (instruction + ' ' + response).lower()\n",
    "        if any(keyword in text for keyword in math_keywords):\n",
    "            analysis['math_keywords'] += 1\n",
    "        \n",
    "        # 推論パターン検出\n",
    "        if any(pattern in response.lower() for pattern in reasoning_patterns):\n",
    "            analysis['reasoning_patterns'] += 1\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def visualize_analysis(analysis):\n",
    "    \"\"\"分析結果の可視化\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # ドメイン分布\n",
    "    domains = list(analysis['domains'].keys())\n",
    "    counts = list(analysis['domains'].values())\n",
    "    \n",
    "    axes[0,0].pie(counts, labels=domains, autopct='%1.1f%%')\n",
    "    axes[0,0].set_title('ドメイン分布')\n",
    "    \n",
    "    # 問題文長分布\n",
    "    axes[0,1].hist(analysis['instruction_lengths'], bins=20, alpha=0.7)\n",
    "    axes[0,1].set_title('問題文長分布')\n",
    "    axes[0,1].set_xlabel('文字数')\n",
    "    axes[0,1].set_ylabel('頻度')\n",
    "    \n",
    "    # 解答長分布\n",
    "    axes[1,0].hist(analysis['response_lengths'], bins=20, alpha=0.7, color='orange')\n",
    "    axes[1,0].set_title('解答長分布')\n",
    "    axes[1,0].set_xlabel('文字数')\n",
    "    axes[1,0].set_ylabel('頻度')\n",
    "    \n",
    "    # 品質指標\n",
    "    total = analysis['total_samples']\n",
    "    metrics = ['数学キーワード', '推論パターン']\n",
    "    values = [analysis['math_keywords']/total*100, analysis['reasoning_patterns']/total*100]\n",
    "    \n",
    "    axes[1,1].bar(metrics, values)\n",
    "    axes[1,1].set_title('品質指標 (%)')\n",
    "    axes[1,1].set_ylabel('含有率 (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/quality_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 品質分析実行\n",
    "if 'merged_data' in locals() and merged_data:\n",
    "    print(\"📊 データセット品質分析中...\")\n",
    "    \n",
    "    analysis = analyze_dataset_quality(merged_data)\n",
    "    \n",
    "    print(\"\\n📈 品質分析結果:\")\n",
    "    print(f\"  総サンプル数: {analysis['total_samples']}\")\n",
    "    print(f\"  平均問題文長: {np.mean(analysis['instruction_lengths']):.1f} 文字\")\n",
    "    print(f\"  平均解答長: {np.mean(analysis['response_lengths']):.1f} 文字\")\n",
    "    print(f\"  数学キーワード含有率: {analysis['math_keywords']/analysis['total_samples']*100:.1f}%\")\n",
    "    print(f\"  推論パターン含有率: {analysis['reasoning_patterns']/analysis['total_samples']*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n📊 ドメイン別統計:\")\n",
    "    for domain, count in analysis['domains'].items():\n",
    "        percentage = count / analysis['total_samples'] * 100\n",
    "        print(f\"  {domain}: {count}問題 ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 可視化\n",
    "    visualize_analysis(analysis)\n",
    "    \n",
    "    # 分析結果保存\n",
    "    analysis_file = f\"{OUTPUT_DIR}/quality_analysis.json\"\n",
    "    analysis_serializable = {\n",
    "        k: (dict(v) if isinstance(v, Counter) else v) \n",
    "        for k, v in analysis.items()\n",
    "    }\n",
    "    \n",
    "    with open(analysis_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis_serializable, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"✅ 分析結果保存: {analysis_file}\")\n",
    "\nelse:\n",
    "    print(\"⚠️ 分析対象データがありません。Step 2を確認してください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_header"
   },
   "source": [
    "## 📥 Step 4: ファイルダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "file_download"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "# 生成ファイル一覧\n",
    "output_files = []\n",
    "for file_path in Path(OUTPUT_DIR).glob('*'):\n",
    "    if file_path.is_file():\n",
    "        output_files.append(str(file_path))\n",
    "\n",
    "print(\"📁 生成されたファイル:\")\n",
    "for file_path in output_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    size = os.path.getsize(file_path) / 1024  # KB\n",
    "    print(f\"  📄 {filename} ({size:.1f} KB)\")\n",
    "\n",
    "if output_files:\n",
    "    # ZIPファイル作成\n",
    "    zip_file = f\"{OUTPUT_DIR}/{DATASET_NAME}_complete.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
    "        for file_path in output_files:\n",
    "            arcname = os.path.basename(file_path)\n",
    "            zipf.write(file_path, arcname)\n",
    "    \n",
    "    print(f\"\\n📦 統合ZIPファイル作成: {zip_file}\")\n",
    "    print(f\"📊 ZIPサイズ: {os.path.getsize(zip_file) / 1024:.1f} KB\")\n",
    "    \n",
    "    # ダウンロード\n",
    "    print(\"\\n📥 ファイルダウンロード中...\")\n",
    "    files.download(zip_file)\n",
    "    \n",
    "    print(\"✅ ダウンロード完了！\")\n",
    "    \n",
    "    # 個別ダウンロードオプション\n",
    "    print(\"\\n💡 個別ファイルダウンロード:\")\n",
    "    print(\"以下のコードで個別にダウンロードできます:\")\n",
    "    print(\"```python\")\n",
    "    for file_path in output_files:\n",
    "        if file_path.endswith('.json') or file_path.endswith('.jsonl'):\n",
    "            print(f\"# files.download('{file_path}')\")\n",
    "    print(\"```\")\n",
    "\nelse:\n",
    "    print(\"⚠️ ダウンロード可能なファイルがありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_header"
   },
   "source": [
    "## 🎉 完了サマリー\n",
    "\n",
    "### 📊 生成結果\n",
    "- **6ドメイン数学データセット**: 代数学、応用数学、微積分学、離散数学、幾何学、数論\n",
    "- **統合・シャッフル済み**: ドメインバランス保持\n",
    "- **ShareGPT互換**: 機械学習フレームワーク対応\n",
    "\n",
    "### 📁 出力ファイル\n",
    "- `{DATASET_NAME}_XXX.json`: 統合データセット\n",
    "- `{DATASET_NAME}_XXX_sharegpt.jsonl`: ShareGPT形式\n",
    "- `quality_analysis.json`: 品質分析結果\n",
    "- `quality_analysis.png`: 可視化グラフ\n",
    "\n",
    "### 🚀 次のステップ\n",
    "1. **ファインチューニング**: 生成データでモデル訓練\n",
    "2. **品質評価**: HLE試験問題での性能測定\n",
    "3. **スケールアップ**: より大規模なデータセット生成\n",
    "4. **ドメイン拡張**: 追加数学分野の対応\n",
    "\n",
    "### 📚 参考資料\n",
    "- [Magpie論文](https://arxiv.org/abs/2406.08464)\n",
    "- [DeepSeek R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)\n",
    "- [プロジェクトGitHub](https://github.com/your-repo/magpie)\n",
    "\n",
    "**🎯 Colab版6ドメイン数学データセット生成完了！**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}