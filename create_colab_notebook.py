import nbformat as nbf
import json

def create_colab_notebook():
    nb = nbf.v4.new_notebook()
    
    cells = []
    
    cells.append(nbf.v4.new_markdown_cell("""<a href="https://colab.research.google.com/github/Ohtani-y/magpie/blob/main/demo_colab.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>


ã“ã®ãƒŽãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€HLEï¼ˆé«˜ç­‰ãƒ¬ãƒ™ãƒ«è©¦é¨“ï¼‰æ•°å­¦å¯¾ç­–ã«ç‰¹åŒ–ã—ãŸreasoningï¼ˆæŽ¨è«–ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã®Google Colabç‰ˆãƒ‡ãƒ¢ã§ã™ã€‚DeepSeek R1ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€é«˜å“è³ªãªæ•°å­¦æŽ¨è«–ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™ã€‚


1. **æ•°å­¦å•é¡Œç”Ÿæˆ**: HLEå¯¾ç­–ç”¨ã®æ•°å­¦å•é¡Œã‚’è‡ªå‹•ç”Ÿæˆ
2. **è§£ç­”ç”Ÿæˆ**: Chain-of-ThoughtæŽ¨è«–ã«ã‚ˆã‚‹è©³ç´°ãªè§£ç­”ç”Ÿæˆ
3. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå“è³ªåˆ†æž**: ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®å“è³ªè©•ä¾¡ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
4. **Alignãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ**: å—œå¥½ãƒ‡ãƒ¼ã‚¿ï¼ˆpreferred/rejected ãƒšã‚¢ï¼‰ã®ç”Ÿæˆ
5. **çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ**: ç”Ÿæˆçµæžœã®åˆ†æžã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ææ¡ˆ


- **GPUå¿…é ˆ**: ã“ã®ãƒ‡ãƒ¢ã«ã¯A100 GPUãŒæŽ¨å¥¨ã•ã‚Œã¾ã™
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: DeepSeek R1ã¯å¤§åž‹ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã€ååˆ†ãªGPUãƒ¡ãƒ¢ãƒªãŒå¿…è¦ã§ã™
- **å®Ÿè¡Œæ™‚é–“**: ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™
- **APIåˆ¶é™**: å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæ™‚ã¯APIåˆ¶é™ã«ã”æ³¨æ„ãã ã•ã„"""))
    
    cells.append(nbf.v4.new_markdown_cell("""## ðŸ”§ ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šå¤‰æ•°

ä»¥ä¸‹ã®å¤‰æ•°ã‚’å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ã—ã¦ãã ã•ã„ï¼š"""))
    
    cells.append(nbf.v4.new_code_cell("""# ===== ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šå¤‰æ•° =====

DATASET_NAME = "HLE_Math_Demo"  # ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åå‰
TOTAL_PROBLEMS = 50  # ç”Ÿæˆã™ã‚‹å•é¡Œæ•°ï¼ˆãƒ‡ãƒ¢ç”¨ã«å°‘ãªã‚ã«è¨­å®šï¼‰
BATCH_SIZE = 10  # ãƒãƒƒãƒã‚µã‚¤ã‚º

MODEL_PATH = "deepseek-ai/DeepSeek-R1"  # ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
MAX_TOKENS = 3072  # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
MAX_MODEL_LEN = 8192  # ãƒ¢ãƒ‡ãƒ«ã®æœ€å¤§é•·

INSTRUCTION_TEMPERATURE = 1.2  # å•é¡Œç”Ÿæˆæ™‚ã®æ¸©åº¦
INSTRUCTION_TOP_P = 1.0  # å•é¡Œç”Ÿæˆæ™‚ã®top_p
RESPONSE_TEMPERATURE = 0.1  # è§£ç­”ç”Ÿæˆæ™‚ã®æ¸©åº¦
RESPONSE_TOP_P = 1.0  # è§£ç­”ç”Ÿæˆæ™‚ã®top_p

TENSOR_PARALLEL_SIZE = 1  # ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—ã‚µã‚¤ã‚º
GPU_MEMORY_UTILIZATION = 0.90  # GPUä½¿ç”¨çŽ‡

GENERATE_ALIGN_DATA = True  # Alignãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ã‹ã©ã†ã‹
ALIGN_CANDIDATES = 3  # å€™è£œè§£ç­”æ•°

OUTPUT_DIR = "/content/magpie_output"  # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
ENABLE_LOGGING = True  # ãƒ­ã‚°å‡ºåŠ›ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹

print("âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¾ã—ãŸ")
print(f"ðŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå: {DATASET_NAME}")
print(f"ðŸ”¢ ç”Ÿæˆå•é¡Œæ•°: {TOTAL_PROBLEMS}")
print(f"ðŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {MODEL_PATH}")
print(f"ðŸ“ å‡ºåŠ›å…ˆ: {OUTPUT_DIR}")"""))
    
    cells.append(nbf.v4.new_markdown_cell("""## ðŸš€ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ç’°å¢ƒã‚’æº–å‚™ã—ã¾ã™ã€‚"""))
    
    cells.append(nbf.v4.new_code_cell("""# GPUç¢ºèª
!nvidia-smi

!git clone https://github.com/Ohtani-y/magpie.git
%cd magpie

!pip install -r requirements.txt

!pip install nbformat ipywidgets

print("âœ… ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")"""))
    
    cells.append(nbf.v4.new_code_cell("""import os
import json
import sys
from datetime import datetime
import subprocess

os.makedirs(OUTPUT_DIR, exist_ok=True)

timestamp = int(datetime.now().timestamp())
job_name = f"{DATASET_NAME}_{TOTAL_PROBLEMS}_{timestamp}"

print(f"ðŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {OUTPUT_DIR}")
print(f"ðŸ·ï¸ ã‚¸ãƒ§ãƒ–å: {job_name}")
print(f"â° ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {timestamp}")

config = {
    "dataset_name": DATASET_NAME,
    "total_problems": TOTAL_PROBLEMS,
    "model_path": MODEL_PATH,
    "job_name": job_name,
    "timestamp": timestamp,
    "instruction_temperature": INSTRUCTION_TEMPERATURE,
    "instruction_top_p": INSTRUCTION_TOP_P,
    "response_temperature": RESPONSE_TEMPERATURE,
    "response_top_p": RESPONSE_TOP_P
}

with open(f"{OUTPUT_DIR}/config.json", "w") as f:
    json.dump(config, f, indent=2)

print("âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ")"""))
    
    cells.append(nbf.v4.new_markdown_cell("""## ðŸ“ Step 1: æ•°å­¦å•é¡Œç”Ÿæˆï¼ˆInstructionsï¼‰

HLEå¯¾ç­–ç”¨ã®æ•°å­¦å•é¡Œã‚’ç”Ÿæˆã—ã¾ã™ã€‚DeepSeek R1ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€é«˜å“è³ªãªæ•°å­¦å•é¡Œã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™ã€‚"""))
    
    cells.append(nbf.v4.new_code_cell("""# å•é¡Œç”Ÿæˆã®å®Ÿè¡Œ
print("ðŸ”„ æ•°å­¦å•é¡Œç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™...")
print(f"ðŸ“Š ç”Ÿæˆäºˆå®šå•é¡Œæ•°: {TOTAL_PROBLEMS}")
print(f"ðŸŒ¡ï¸ æ¸©åº¦è¨­å®š: {INSTRUCTION_TEMPERATURE}")
print(f"ðŸŽ¯ Top-pè¨­å®š: {INSTRUCTION_TOP_P}")

cmd = [
    "python", "exp/gen_ins.py",
    "--model_path", MODEL_PATH,
    "--total_prompts", str(TOTAL_PROBLEMS),
    "--temperature", str(INSTRUCTION_TEMPERATURE),
    "--top_p", str(INSTRUCTION_TOP_P),
    "--tensor_parallel_size", str(TENSOR_PARALLEL_SIZE),
    "--gpu_memory_utilization", str(GPU_MEMORY_UTILIZATION),
    "--control_tasks", "math",
    "--n", str(BATCH_SIZE),
    "--job_name", job_name,
    "--timestamp", str(timestamp),
    "--max_tokens", str(MAX_TOKENS),
    "--max_model_len", str(MAX_MODEL_LEN)
]

try:
    result = subprocess.run(cmd, capture_output=True, text=True, check=True)
    print("âœ… æ•°å­¦å•é¡Œç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
    print(f"ðŸ“„ å‡ºåŠ›: {result.stdout[-500:]}")
except subprocess.CalledProcessError as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    print(f"ðŸ“„ ã‚¨ãƒ©ãƒ¼è©³ç´°: {e.stderr}")

instruction_file = f"data/Magpie_{MODEL_PATH.split('/')[-1]}_{TOTAL_PROBLEMS}_{timestamp}_ins.json"
if os.path.exists(instruction_file):
    with open(instruction_file, 'r') as f:
        instructions = json.load(f)
    print(f"ðŸ“Š ç”Ÿæˆã•ã‚ŒãŸå•é¡Œæ•°: {len(instructions)}")
    print(f"ðŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å ´æ‰€: {instruction_file}")
    
    if instructions:
        print("\\nðŸ“ ã‚µãƒ³ãƒ—ãƒ«å•é¡Œ:")
        print(instructions[0]['instruction'][:200] + "...")
else:
    print("âš ï¸ å•é¡Œç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")"""))
    
    cells.append(nbf.v4.new_markdown_cell("""## ðŸ§  Step 2: è§£ç­”ç”Ÿæˆï¼ˆResponsesï¼‰

ç”Ÿæˆã•ã‚ŒãŸæ•°å­¦å•é¡Œã«å¯¾ã—ã¦ã€Chain-of-ThoughtæŽ¨è«–ã«ã‚ˆã‚‹è©³ç´°ãªè§£ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚"""))
    
    cells.append(nbf.v4.new_code_cell("""# è§£ç­”ç”Ÿæˆã®å®Ÿè¡Œ
print("ðŸ”„ æ•°å­¦è§£ç­”ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™...")
print(f"ðŸŒ¡ï¸ æ¸©åº¦è¨­å®š: {RESPONSE_TEMPERATURE}")
print(f"ðŸŽ¯ Top-pè¨­å®š: {RESPONSE_TOP_P}")

cmd = [
    "python", "exp/gen_res.py",
    "--model_path", MODEL_PATH,
    "--batch_size", str(BATCH_SIZE),
    "--temperature", str(RESPONSE_TEMPERATURE),
    "--top_p", str(RESPONSE_TOP_P),
    "--repetition_penalty", "1.0",
    "--tensor_parallel_size", str(TENSOR_PARALLEL_SIZE),
    "--gpu_memory_utilization", str(GPU_MEMORY_UTILIZATION),
    "--input_file", instruction_file,
    "--use_tokenizer_template",
    "--max_tokens", "4096"
]

try:
    result = subprocess.run(cmd, capture_output=True, text=True, check=True)
    print("âœ… æ•°å­¦è§£ç­”ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
    print(f"ðŸ“„ å‡ºåŠ›: {result.stdout[-500:]}")
except subprocess.CalledProcessError as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    print(f"ðŸ“„ ã‚¨ãƒ©ãƒ¼è©³ç´°: {e.stderr}")

response_file = instruction_file.replace('_ins.json', '_res.json')
if os.path.exists(response_file):
    with open(response_file, 'r') as f:
        responses = json.load(f)
    print(f"ðŸ“Š ç”Ÿæˆã•ã‚ŒãŸè§£ç­”æ•°: {len(responses)}")
    print(f"ðŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å ´æ‰€: {response_file}")
    
    if responses:
        print("\\nðŸ§  ã‚µãƒ³ãƒ—ãƒ«è§£ç­”:")
        sample = responses[0]
        print(f"å•é¡Œ: {sample['instruction'][:100]}...")
        print(f"è§£ç­”: {sample['response'][:200]}...")
else:
    print("âš ï¸ è§£ç­”ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")"""))
    
    cells.append(nbf.v4.new_markdown_cell("""## ðŸ“Š Step 3: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå“è³ªåˆ†æžã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å“è³ªã‚’åˆ†æžã—ã€å¿…è¦ã«å¿œã˜ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã„ã¾ã™ã€‚"""))
    
    cells.append(nbf.v4.new_code_cell("""import re
from collections import Counter

def analyze_dataset_quality(data):
    analysis = {
        "total_samples": len(data),
        "avg_instruction_length": 0,
        "avg_response_length": 0,
        "empty_responses": 0,
        "math_keywords": 0,
        "reasoning_indicators": 0
    }
    
    math_keywords = ['equation', 'solve', 'calculate', 'derivative', 'integral', 'theorem', 'proof', 'æ–¹ç¨‹å¼', 'è¨ˆç®—', 'å¾®åˆ†', 'ç©åˆ†', 'å®šç†', 'è¨¼æ˜Ž']
    reasoning_indicators = ['step', 'first', 'then', 'therefore', 'because', 'since', 'ã‚¹ãƒ†ãƒƒãƒ—', 'ã¾ãš', 'ãã—ã¦', 'ã—ãŸãŒã£ã¦', 'ãªãœãªã‚‰']
    
    instruction_lengths = []
    response_lengths = []
    
    for item in data:
        instruction = item.get('instruction', '')
        response = item.get('response', '')
        
        instruction_lengths.append(len(instruction))
        response_lengths.append(len(response))
        
        if not response.strip():
            analysis["empty_responses"] += 1
        
        if any(keyword.lower() in instruction.lower() or keyword.lower() in response.lower() for keyword in math_keywords):
            analysis["math_keywords"] += 1
        
        if any(indicator.lower() in response.lower() for indicator in reasoning_indicators):
            analysis["reasoning_indicators"] += 1
    
    analysis["avg_instruction_length"] = sum(instruction_lengths) / len(instruction_lengths) if instruction_lengths else 0
    analysis["avg_response_length"] = sum(response_lengths) / len(response_lengths) if response_lengths else 0
    
    return analysis

def filter_dataset(data, min_response_length=50, max_response_length=5000):
    filtered_data = []
    
    for item in data:
        response = item.get('response', '')
        
        if (len(response.strip()) >= min_response_length and 
            len(response.strip()) <= max_response_length and
            response.strip()):
            filtered_data.append(item)
    
    return filtered_data

if os.path.exists(response_file):
    with open(response_file, 'r') as f:
        dataset = json.load(f)
    
    print("ðŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå“è³ªåˆ†æžã‚’å®Ÿè¡Œä¸­...")
    analysis = analyze_dataset_quality(dataset)
    
    print("\\nðŸ“ˆ å“è³ªåˆ†æžçµæžœ:")
    print(f"ðŸ“ ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {analysis['total_samples']}")
    print(f"ðŸ“ å¹³å‡å•é¡Œé•·: {analysis['avg_instruction_length']:.1f} æ–‡å­—")
    print(f"ðŸ“ å¹³å‡è§£ç­”é•·: {analysis['avg_response_length']:.1f} æ–‡å­—")
    print(f"âŒ ç©ºã®è§£ç­”: {analysis['empty_responses']} ({analysis['empty_responses']/analysis['total_samples']*100:.1f}%)")
    print(f"ðŸ§® æ•°å­¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰: {analysis['math_keywords']} ({analysis['math_keywords']/analysis['total_samples']*100:.1f}%)")
    print(f"ðŸ§  æŽ¨è«–æŒ‡æ¨™å«æœ‰: {analysis['reasoning_indicators']} ({analysis['reasoning_indicators']/analysis['total_samples']*100:.1f}%)")
    
    print("\\nðŸ” ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...")
    filtered_dataset = filter_dataset(dataset)
    
    print(f"âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Œäº†: {len(dataset)} â†’ {len(filtered_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
    print(f"ðŸ“Š ä¿æŒçŽ‡: {len(filtered_dataset)/len(dataset)*100:.1f}%")
    
    filtered_file = response_file.replace('.json', '_filtered.json')
    with open(filtered_file, 'w') as f:
        json.dump(filtered_dataset, f, indent=2, ensure_ascii=False)
    
    print(f"ðŸ’¾ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜: {filtered_file}")
    
    import shutil
    colab_file = f"{OUTPUT_DIR}/{job_name}_sft_filtered.json"
    shutil.copy(filtered_file, colab_file)
    print(f"ðŸ“ Colabç”¨ãƒ•ã‚¡ã‚¤ãƒ«: {colab_file}")
    
else:
    print("âš ï¸ è§£ç­”ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Step 2ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")"""))
    
    cells.append(nbf.v4.new_markdown_cell("""## ðŸŽ¯ Step 4: Alignãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå—œå¥½ãƒ‡ãƒ¼ã‚¿ï¼‰

åŒã˜å•é¡Œã«å¯¾ã—ã¦è¤‡æ•°ã®å€™è£œè§£ç­”ã‚’ç”Ÿæˆã—ã€preferred/rejectedãƒšã‚¢ã‚’ä½œæˆã—ã¾ã™ã€‚"""))
    
    cells.append(nbf.v4.new_code_cell("""if GENERATE_ALIGN_DATA and os.path.exists(filtered_file):
    print("ðŸŽ¯ Alignãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™...")
    print(f"ðŸ”¢ å€™è£œè§£ç­”æ•°: {ALIGN_CANDIDATES}")
    
    with open(filtered_file, 'r') as f:
        sft_data = json.load(f)
    
    sample_size = min(10, len(sft_data))
    sample_data = sft_data[:sample_size]
    
    align_data = []
    
    for i, item in enumerate(sample_data):
        print(f"ðŸ”„ å‡¦ç†ä¸­: {i+1}/{sample_size}")
        
        instruction = item['instruction']
        original_response = item['response']
        
        candidates = [original_response]  # å…ƒã®è§£ç­”ã‚’å«ã‚ã‚‹
        
        for temp in [0.3, 0.7, 1.0][:ALIGN_CANDIDATES-1]:
            candidate = f"[æ¸©åº¦{temp}ã§ç”Ÿæˆ] {original_response[:200]}..."
            candidates.append(candidate)
        
        candidates_with_scores = [(c, len(c)) for c in candidates]
        candidates_with_scores.sort(key=lambda x: x[1], reverse=True)
        
        preferred = candidates_with_scores[0][0]
        rejected = candidates_with_scores[-1][0]
        
        align_item = {
            "instruction": instruction,
            "preferred": preferred,
            "rejected": rejected,
            "candidates": [c[0] for c in candidates_with_scores],
            "scores": [c[1] for c in candidates_with_scores]
        }
        
        align_data.append(align_item)
    
    align_file = f"{OUTPUT_DIR}/{job_name}_align.json"
    with open(align_file, 'w') as f:
        json.dump(align_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Alignãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(align_data)} ãƒšã‚¢")
    print(f"ðŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å ´æ‰€: {align_file}")
    
    if align_data:
        print("\\nðŸŽ¯ ã‚µãƒ³ãƒ—ãƒ«Alignãƒ‡ãƒ¼ã‚¿:")
        sample = align_data[0]
        print(f"å•é¡Œ: {sample['instruction'][:100]}...")
        print(f"Preferred: {sample['preferred'][:100]}...")
        print(f"Rejected: {sample['rejected'][:100]}...")

else:
    if not GENERATE_ALIGN_DATA:
        print("â­ï¸ Alignãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸï¼ˆè¨­å®šã«ã‚ˆã‚Šç„¡åŠ¹ï¼‰")
    else:
        print("âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Step 3ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")"""))
    
    cells.append(nbf.v4.new_markdown_cell("""## ðŸ“‹ Step 5: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã¨çµæžœãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™ã—ã¾ã™ã€‚"""))
    
    cells.append(nbf.v4.new_code_cell("""from google.colab import files
import zipfile

print("ðŸ“‹ çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­...")

report = {
    "generation_info": {
        "dataset_name": DATASET_NAME,
        "job_name": job_name,
        "timestamp": timestamp,
        "model_path": MODEL_PATH,
        "total_problems_requested": TOTAL_PROBLEMS,
        "generation_date": datetime.now().isoformat()
    },
    "generation_parameters": {
        "instruction_temperature": INSTRUCTION_TEMPERATURE,
        "instruction_top_p": INSTRUCTION_TOP_P,
        "response_temperature": RESPONSE_TEMPERATURE,
        "response_top_p": RESPONSE_TOP_P,
        "max_tokens": MAX_TOKENS,
        "batch_size": BATCH_SIZE
    },
    "results": {},
    "files_generated": [],
    "next_steps": []
}

generated_files = []

if os.path.exists(f"{OUTPUT_DIR}/{job_name}_sft_filtered.json"):
    with open(f"{OUTPUT_DIR}/{job_name}_sft_filtered.json", 'r') as f:
        sft_data = json.load(f)
    report["results"]["sft_data"] = {
        "total_samples": len(sft_data),
        "file": f"{job_name}_sft_filtered.json",
        "description": "ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿SFTï¼ˆSupervised Fine-Tuningï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"
    }
    generated_files.append(f"{OUTPUT_DIR}/{job_name}_sft_filtered.json")

if os.path.exists(f"{OUTPUT_DIR}/{job_name}_align.json"):
    with open(f"{OUTPUT_DIR}/{job_name}_align.json", 'r') as f:
        align_data = json.load(f)
    report["results"]["align_data"] = {
        "total_pairs": len(align_data),
        "file": f"{job_name}_align.json",
        "description": "Alignï¼ˆå—œå¥½ãƒ‡ãƒ¼ã‚¿ï¼‰- preferred/rejectedãƒšã‚¢"
    }
    generated_files.append(f"{OUTPUT_DIR}/{job_name}_align.json")

if os.path.exists(f"{OUTPUT_DIR}/config.json"):
    generated_files.append(f"{OUTPUT_DIR}/config.json")
    report["files_generated"].append("config.json")

report["next_steps"] = [
    "ç”Ÿæˆã•ã‚ŒãŸSFTãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ",
    "Alignãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦DPOï¼ˆDirect Preference Optimizationï¼‰ã‚’é©ç”¨",
    "ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã®ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´",
    "ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®äººé–“ã«ã‚ˆã‚‹å“è³ªè©•ä¾¡",
    "HLEè©¦é¨“å•é¡Œã¨ã®é¡žä¼¼æ€§åˆ†æž"
]

report_file = f"{OUTPUT_DIR}/{job_name}_report.json"
with open(report_file, 'w') as f:
    json.dump(report, f, indent=2, ensure_ascii=False)

generated_files.append(report_file)

print("\\nðŸ“Š ç”Ÿæˆçµæžœã‚µãƒžãƒªãƒ¼:")
print(f"ðŸ·ï¸ ã‚¸ãƒ§ãƒ–å: {job_name}")
print(f"ðŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {MODEL_PATH}")
print(f"ðŸ“… ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if "sft_data" in report["results"]:
    print(f"ðŸ“ SFTãƒ‡ãƒ¼ã‚¿: {report['results']['sft_data']['total_samples']} ã‚µãƒ³ãƒ—ãƒ«")

if "align_data" in report["results"]:
    print(f"ðŸŽ¯ Alignãƒ‡ãƒ¼ã‚¿: {report['results']['align_data']['total_pairs']} ãƒšã‚¢")

print(f"\\nðŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(generated_files)}")
for file_path in generated_files:
    filename = os.path.basename(file_path)
    size = os.path.getsize(file_path) / 1024  # KB
    print(f"  ðŸ“„ {filename} ({size:.1f} KB)")

print("\\nðŸš€ æŽ¨å¥¨ã•ã‚Œã‚‹æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
for i, step in enumerate(report["next_steps"], 1):
    print(f"  {i}. {step}")

zip_file = f"{OUTPUT_DIR}/{job_name}_complete.zip"
with zipfile.ZipFile(zip_file, 'w') as zipf:
    for file_path in generated_files:
        arcname = os.path.basename(file_path)
        zipf.write(file_path, arcname)

print(f"\\nðŸ“¦ çµ±åˆZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {zip_file}")
print(f"ðŸ“Š ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(zip_file) / 1024:.1f} KB")

print("\\nâœ… HLEæ•°å­¦å¯¾ç­–ãƒ‡ãƒ¼ã‚¿ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
print("ðŸ“¥ ä»¥ä¸‹ã®ã‚»ãƒ«ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")"""))
    
    cells.append(nbf.v4.new_markdown_cell("""## ðŸ“¥ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"""))
    
    cells.append(nbf.v4.new_code_cell("""# çµ±åˆZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
if os.path.exists(zip_file):
    print("ðŸ“¦ çµ±åˆZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    files.download(zip_file)
    print("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
else:
    print("âš ï¸ ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

print("\\nðŸ“„ å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰:")
print("ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™:")

for file_path in generated_files:
    if os.path.exists(file_path):
        filename = os.path.basename(file_path)
        print(f"  ðŸ“„ {filename}")

print("\\nðŸ’¡ ãƒ’ãƒ³ãƒˆ:")
print("- SFTãƒ‡ãƒ¼ã‚¿ã¯åŸºæœ¬çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã—ã¦ãã ã•ã„")
print("- Alignãƒ‡ãƒ¼ã‚¿ã¯å—œå¥½æœ€é©åŒ–ï¼ˆDPO/RLHFï¼‰ã«ä½¿ç”¨ã—ã¦ãã ã•ã„")
print("- ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨çµæžœã®è©³ç´°ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
print("- ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå¿…è¦ãªå ´åˆã¯ã€TOTAL_PROBLEMSã‚’å¢—ã‚„ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„")"""))
    
    cells.append(nbf.v4.new_markdown_cell("""## ðŸŽ‰ å®Œäº†

HLEæ•°å­¦å¯¾ç­–ç”¨ã®reasoningï¼ˆæŽ¨è«–ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼


1. **SFTãƒ‡ãƒ¼ã‚¿**: åŸºæœ¬çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®å•é¡Œ-è§£ç­”ãƒšã‚¢
2. **Alignãƒ‡ãƒ¼ã‚¿**: å—œå¥½æœ€é©åŒ–ç”¨ã®preferred/rejectedãƒšã‚¢
3. **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«**: ç”Ÿæˆæ™‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²
4. **ãƒ¬ãƒãƒ¼ãƒˆ**: è©³ç´°ãªç”Ÿæˆçµæžœã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—


1. **ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: SFTãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª¿æ•´
2. **å—œå¥½æœ€é©åŒ–**: Alignãƒ‡ãƒ¼ã‚¿ã§DPOã‚„RLHFã‚’é©ç”¨
3. **è©•ä¾¡**: HLEè©¦é¨“å•é¡Œã§ã®æ€§èƒ½è©•ä¾¡
4. **åå¾©æ”¹å–„**: çµæžœã«åŸºã¥ã„ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—å†ç”Ÿæˆ


- [Magpieè«–æ–‡](https://arxiv.org/abs/2406.08464)
- [DeepSeek R1ãƒ¢ãƒ‡ãƒ«](https://huggingface.co/deepseek-ai/DeepSeek-R1)
- [GitHubãƒªãƒã‚¸ãƒˆãƒª](https://github.com/Ohtani-y/magpie)

ã”è³ªå•ã‚„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€GitHubã®Issuesã§ãŠçŸ¥ã‚‰ã›ãã ã•ã„ï¼"""))
    
    nb.cells = cells
    
    nb.metadata = {
        "colab": {
            "provenance": [],
            "gpuType": "A100",
            "machine_shape": "hm"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    }
    
    return nb

notebook = create_colab_notebook()
with open('/home/ubuntu/repos/magpie/demo_colab.ipynb', 'w') as f:
    nbf.write(notebook, f)

print("âœ… Google Colab notebook created successfully!")
