{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_header"
   },
   "source": [
    "# 🧮 Magpie: 6ドメイン数学データセット生成 (完全単独版)\n",
    "\n",
    "このノートブックは、**外部ファイル依存一切なし**でDeepSeek R1を使用して6つの数学ドメイン別データセットを生成・統合する完全単独実行版です。\n",
    "\n",
    "## 🎯 特徴\n",
    "- ✅ **完全単独実行**: 外部リポジトリ・ファイル不要\n",
    "- ✅ **ドメイン特化**: 各数学領域に特化した問題生成\n",
    "- ✅ **自動統合**: 6ドメインデータの自動マージ・シャッフル\n",
    "- ✅ **GPU自動最適化**: T4/A100両対応\n",
    "- ✅ **エラー回復**: 堅牢なエラーハンドリング\n",
    "\n",
    "## 📊 対応ドメイン（各領域に特化）\n",
    "1. **Algebra** (代数学): 方程式、多項式、関数\n",
    "2. **Applied Mathematics** (応用数学): 微分方程式、最適化\n",
    "3. **Calculus** (微積分学): 微積分、極限、級数\n",
    "4. **Discrete Mathematics** (離散数学): 組合せ、グラフ理論\n",
    "5. **Geometry** (幾何学): 解析幾何、空間図形\n",
    "6. **Number Theory** (数論): 素数、合同式、暗号応用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "settings_header"
   },
   "source": [
    "## ⚙️ ユーザー設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "user_settings"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ユーザー設定\n",
    "# =============================================================================\n",
    "\n",
    "# 基本設定\n",
    "USE_LIGHTWEIGHT_MODEL = True  # True: 軽量モード (T4推奨), False: フルモード (A100推奨)\n",
    "PROBLEMS_PER_DOMAIN = 20      # 各ドメインの問題数 (10-100推奨)\n",
    "USE_HF_LOGIN = True           # Hugging Face認証を使用\n",
    "\n",
    "# モデル設定 (USE_LIGHTWEIGHT_MODELに応じて自動調整)\n",
    "if USE_LIGHTWEIGHT_MODEL:\n",
    "    MODEL_PATH = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "    TENSOR_PARALLEL = 1\n",
    "    GPU_MEMORY_UTIL = 0.80\n",
    "    MAX_MODEL_LEN = 4096\n",
    "else:\n",
    "    MODEL_PATH = \"deepseek-ai/DeepSeek-R1\"\n",
    "    TENSOR_PARALLEL = 2\n",
    "    GPU_MEMORY_UTIL = 0.90\n",
    "    MAX_MODEL_LEN = 8192\n",
    "\n",
    "# 生成パラメータ\n",
    "INSTRUCTION_TEMP = 1.2\n",
    "INSTRUCTION_TOP_P = 1.0\n",
    "RESPONSE_TEMP = 0.1\n",
    "RESPONSE_TOP_P = 1.0\n",
    "\n",
    "# 出力設定\n",
    "OUTPUT_DIR = \"/content/magpie_6domains\"\n",
    "DATASET_NAME = f\"HLE_6Domains_Math_{PROBLEMS_PER_DOMAIN * 6}\"\n",
    "\n",
    "print(f\"🚀 設定完了\")\n",
    "print(f\"📊 モデル: {MODEL_PATH}\")\n",
    "print(f\"📈 問題数: {PROBLEMS_PER_DOMAIN} × 6ドメイン = {PROBLEMS_PER_DOMAIN * 6}問題\")\n",
    "print(f\"🔧 軽量モード: {'有効' if USE_LIGHTWEIGHT_MODEL else '無効'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## 🔧 環境セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 依存関係インストール\n",
    "# =============================================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "# 必要なパッケージ\n",
    "packages = [\n",
    "    \"vllm>=0.6.0\",\n",
    "    \"transformers>=4.45.0\", \n",
    "    \"torch>=2.0.0\",\n",
    "    \"accelerate\",\n",
    "    \"datasets\", \n",
    "    \"sentencepiece\",\n",
    "    \"tiktoken\",\n",
    "    \"numpy\",\n",
    "    \"pandas\", \n",
    "    \"tqdm\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\"\n",
    "]\n",
    "\n",
    "print(\"📦 パッケージインストール中...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        install_package(package)\n",
    "        print(f\"✅ {package}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {package}: {e}\")\n",
    "\n",
    "print(\"\\n🔧 インストール完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GPU環境確認・最適化\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# GPU確認\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"🎮 GPU情報:\")\n",
    "    print(f\"   名前: {gpu_name}\")\n",
    "    print(f\"   数量: {gpu_count}\")\n",
    "    print(f\"   メモリ: {gpu_memory:.1f}GB\")\n",
    "    \n",
    "    # メモリに応じた自動調整\n",
    "    if gpu_memory < 20:  # T4相当\n",
    "        if not USE_LIGHTWEIGHT_MODEL:\n",
    "            print(\"⚠️  GPU メモリが少ないため軽量モードに変更\")\n",
    "            MODEL_PATH = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "            TENSOR_PARALLEL = 1\n",
    "            GPU_MEMORY_UTIL = 0.75\n",
    "            if PROBLEMS_PER_DOMAIN > 30:\n",
    "                PROBLEMS_PER_DOMAIN = 20\n",
    "                print(f\"📉 問題数を{PROBLEMS_PER_DOMAIN}に調整\")\n",
    "else:\n",
    "    print(\"❌ GPU が利用できません\")\n",
    "    raise RuntimeError(\"GPU必須です\")\n",
    "\n",
    "# 作業ディレクトリ作成\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"📁 作業ディレクトリ: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf_login"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Hugging Face認証 (DeepSeek R1使用時)\n",
    "# =============================================================================\n",
    "\n",
    "if USE_HF_LOGIN and \"deepseek\" in MODEL_PATH.lower():\n",
    "    try:\n",
    "        from huggingface_hub import login\n",
    "        print(\"🔐 Hugging Face認証が必要です\")\n",
    "        print(\"   https://huggingface.co/settings/tokens でトークンを取得\")\n",
    "        login()\n",
    "        print(\"✅ 認証完了\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 認証エラー: {e}\")\n",
    "        print(\"   軽量モデルに変更します\")\n",
    "        MODEL_PATH = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "else:\n",
    "    print(\"ℹ️ 認証不要 (軽量モデル使用)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "core_functions_header"
   },
   "source": [
    "## 🔧 コア機能実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "domain_templates"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ドメイン特化システムプロンプト\n",
    "# =============================================================================\n",
    "\n",
    "DOMAIN_TEMPLATES = {\n",
    "    \"algebra\": {\n",
    "        \"name\": \"代数学 (Algebra)\",\n",
    "        \"system_prompt\": \"\"\"あなたは代数学専門の数学教師です。以下の領域に特化した高品質な数学問題を生成してください：\n",
    "\n",
    "**対象領域：**\n",
    "- 線形代数：行列、ベクトル空間、固有値・固有ベクトル\n",
    "- 抽象代数：群、環、体の理論\n",
    "- 多項式代数：因数分解、根と係数の関係\n",
    "- 方程式論：連立方程式、高次方程式\n",
    "- 線形変換：基底変換、対角化\n",
    "\n",
    "**問題の特徴：**\n",
    "- 概念の深い理解を問う問題\n",
    "- 証明や論理的推論を含む\n",
    "- 実際の応用例との関連\n",
    "- 段階的解法が明確な問題\"\"\"\n",
    "    },\n",
    "    \"applied-mathematics\": {\n",
    "        \"name\": \"応用数学 (Applied Mathematics)\", \n",
    "        \"system_prompt\": \"\"\"あなたは応用数学専門の研究者です。以下の領域に特化した実践的な数学問題を生成してください：\n",
    "\n",
    "**対象領域：**\n",
    "- 微分方程式：常微分方程式、偏微分方程式\n",
    "- 最適化理論：線形計画法、非線形最適化\n",
    "- 数値解析：数値積分、数値微分、近似解法\n",
    "- 確率・統計：統計的推測、回帰分析\n",
    "- 動的システム：カオス理論、フラクタル\n",
    "\n",
    "**問題の特徴：**\n",
    "- 現実世界の問題をモデル化\n",
    "- 工学・物理・経済学との関連\n",
    "- 数値計算やシミュレーション要素\n",
    "- アルゴリズム的思考を要求\"\"\"\n",
    "    },\n",
    "    \"calculus\": {\n",
    "        \"name\": \"微積分学 (Calculus)\",\n",
    "        \"system_prompt\": \"\"\"あなたは微積分学専門の数学教授です。以下の領域に特化した理論的・実践的な問題を生成してください：\n",
    "\n",
    "**対象領域：**\n",
    "- 極限理論：関数の極限、連続性\n",
    "- 微分法：導関数、偏微分、全微分\n",
    "- 積分法：不定積分、定積分、重積分\n",
    "- 級数理論：テイラー級数、フーリエ級数\n",
    "- ベクトル解析：勾配、発散、回転\n",
    "\n",
    "**問題の特徴：**\n",
    "- 厳密な数学的証明を含む\n",
    "- 幾何学的直観との結合\n",
    "- 物理現象との関連\n",
    "- 計算技法の習得を促進\"\"\"\n",
    "    },\n",
    "    \"discrete-mathematics\": {\n",
    "        \"name\": \"離散数学 (Discrete Mathematics)\",\n",
    "        \"system_prompt\": \"\"\"あなたは離散数学とコンピュータサイエンス専門の教員です。以下の領域に特化した論理的思考を鍛える問題を生成してください：\n",
    "\n",
    "**対象領域：**\n",
    "- 組合せ論：順列、組合せ、鳩の巣原理\n",
    "- グラフ理論：グラフの性質、最短経路、ネットワーク\n",
    "- 論理学：命題論理、述語論理、推論規則\n",
    "- 集合論：集合演算、関係、写像\n",
    "- 計算複雑性：アルゴリズム、計算量理論\n",
    "\n",
    "**問題の特徴：**\n",
    "- アルゴリズム的思考を重視\n",
    "- コンピュータサイエンスへの応用\n",
    "- 構成的証明手法\n",
    "- パズル的要素を含む創意工夫\"\"\"\n",
    "    },\n",
    "    \"geometry\": {\n",
    "        \"name\": \"幾何学 (Geometry)\",\n",
    "        \"system_prompt\": \"\"\"あなたは幾何学専門の数学教師です。以下の領域に特化した視覚的理解を促進する問題を生成してください：\n",
    "\n",
    "**対象領域：**\n",
    "- 解析幾何：座標系、直線・円・楕円の方程式\n",
    "- 立体幾何：空間図形、体積・表面積の計算\n",
    "- 射影幾何：射影変換、無限遠点\n",
    "- 微分幾何：曲線・曲面の理論\n",
    "- トポロジー：位相的性質、連続変形\n",
    "\n",
    "**問題の特徴：**\n",
    "- 図形の性質や関係性に焦点\n",
    "- 視覚的イメージと代数的計算の結合\n",
    "- 空間認識能力の向上\n",
    "- 美的側面も重視した問題設計\"\"\"\n",
    "    },\n",
    "    \"number-theory\": {\n",
    "        \"name\": \"数論 (Number Theory)\",\n",
    "        \"system_prompt\": \"\"\"あなたは数論専門の数学研究者です。以下の領域に特化した純粋数学の美しさを伝える問題を生成してください：\n",
    "\n",
    "**対象領域：**\n",
    "- 初等数論：素数、最大公約数、ユークリッドの互除法\n",
    "- 合同式理論：モジュラー算術、中国剰余定理\n",
    "- 二次形式：ペル方程式、連分数\n",
    "- 解析的数論：素数定理、ゼータ関数\n",
    "- 暗号理論：RSA暗号、楕円曲線暗号\n",
    "\n",
    "**問題の特徴：**\n",
    "- 数の本質的性質を探究\n",
    "- 歴史的背景や数学者のエピソード\n",
    "- 現代暗号学への応用\n",
    "- 厳密な証明技法の習得\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ドメインリスト\n",
    "DOMAINS = list(DOMAIN_TEMPLATES.keys())\n",
    "\n",
    "print(\"🎯 ドメイン特化システム準備完了\")\n",
    "for domain, info in DOMAIN_TEMPLATES.items():\n",
    "    print(f\"   📚 {info['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generation_engine"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# データ生成エンジン\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import vllm\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "class MagpieDomainGenerator:\n",
    "    def __init__(self, model_path, tensor_parallel=1, gpu_memory_util=0.8, max_model_len=4096):\n",
    "        self.model_path = model_path\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        print(f\"🚀 vLLM初期化中: {model_path}\")\n",
    "        try:\n",
    "            self.llm = LLM(\n",
    "                model=model_path,\n",
    "                tensor_parallel_size=tensor_parallel,\n",
    "                gpu_memory_utilization=gpu_memory_util,\n",
    "                max_model_len=max_model_len,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"✅ vLLM初期化成功\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ vLLM初期化エラー: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_instructions(self, domain, num_problems, output_file):\n",
    "        \"\"\"ドメイン特化問題生成\"\"\"\n",
    "        domain_info = DOMAIN_TEMPLATES[domain]\n",
    "        print(f\"\\n📝 {domain_info['name']} 問題生成開始 ({num_problems}問)\")\n",
    "        \n",
    "        # サンプリングパラメータ\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=INSTRUCTION_TEMP,\n",
    "            top_p=INSTRUCTION_TOP_P,\n",
    "            max_tokens=512,\n",
    "            stop=[\"\\n\\n\", \"問題:\", \"Problem:\"]\n",
    "        )\n",
    "        \n",
    "        # プロンプト生成\n",
    "        base_prompt = f\"\"\"{domain_info['system_prompt']}\n",
    "\n",
    "高等レベル試験（HLE）に適した{domain_info['name']}の問題を1つ生成してください。\n",
    "問題は思考力と応用力を試すものとし、明確で完結な文章で記述してください。\n",
    "\n",
    "問題:\"\"\"\n",
    "        \n",
    "        prompts = [base_prompt] * num_problems\n",
    "        \n",
    "        # 生成実行\n",
    "        print(\"🔄 生成実行中...\")\n",
    "        outputs = self.llm.generate(prompts, sampling_params)\n",
    "        \n",
    "        # 結果保存\n",
    "        results = []\n",
    "        for i, output in enumerate(outputs):\n",
    "            instruction = output.outputs[0].text.strip()\n",
    "            if instruction:  # 空でない場合のみ保存\n",
    "                results.append({\n",
    "                    \"id\": i,\n",
    "                    \"instruction\": instruction,\n",
    "                    \"domain\": domain,\n",
    "                    \"created\": self.timestamp,\n",
    "                    \"gen_input_configs\": {\n",
    "                        \"model\": self.model_path,\n",
    "                        \"temperature\": INSTRUCTION_TEMP,\n",
    "                        \"top_p\": INSTRUCTION_TOP_P,\n",
    "                        \"domain\": domain\n",
    "                    }\n",
    "                })\n",
    "        \n",
    "        # ファイル保存\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"✅ {domain_info['name']} 問題生成完了: {len(results)}問\")\n",
    "        return results\n",
    "    \n",
    "    def generate_responses(self, instructions_file, output_file):\n",
    "        \"\"\"Chain-of-Thought解答生成\"\"\"\n",
    "        print(f\"\\n🧠 解答生成開始: {instructions_file}\")\n",
    "        \n",
    "        # 問題読み込み\n",
    "        with open(instructions_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # サンプリングパラメータ\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=RESPONSE_TEMP,\n",
    "            top_p=RESPONSE_TOP_P,\n",
    "            max_tokens=2048\n",
    "        )\n",
    "        \n",
    "        # プロンプト生成\n",
    "        prompts = []\n",
    "        for item in data:\n",
    "            domain_info = DOMAIN_TEMPLATES[item['domain']]\n",
    "            prompt = f\"\"\"以下の{domain_info['name']}の問題に、段階的思考プロセス（Chain-of-Thought）を用いて詳細に解答してください。\n",
    "\n",
    "問題: {item['instruction']}\n",
    "\n",
    "解答:\"\"\"\n",
    "            prompts.append(prompt)\n",
    "        \n",
    "        # 生成実行\n",
    "        print(\"🔄 解答生成実行中...\")\n",
    "        outputs = self.llm.generate(prompts, sampling_params)\n",
    "        \n",
    "        # 結果統合\n",
    "        for i, output in enumerate(outputs):\n",
    "            data[i]['response'] = output.outputs[0].text.strip()\n",
    "            data[i]['gen_response_configs'] = {\n",
    "                \"model\": self.model_path,\n",
    "                \"temperature\": RESPONSE_TEMP,\n",
    "                \"top_p\": RESPONSE_TOP_P\n",
    "            }\n",
    "        \n",
    "        # ファイル保存\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"✅ 解答生成完了: {len(data)}問\")\n",
    "        return data\n",
    "\n",
    "print(\"🔧 生成エンジン準備完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_integration"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# データ統合・シャッフル機能\n",
    "# =============================================================================\n",
    "\n",
    "import glob\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "def merge_domain_datasets(data_dir, output_file, sharegpt_file=None):\n",
    "    \"\"\"6ドメインデータセットの統合・シャッフル\"\"\"\n",
    "    print(\"\\n🔄 ドメインデータセット統合開始\")\n",
    "    \n",
    "    all_data = []\n",
    "    domain_counts = {}\n",
    "    \n",
    "    # 各ドメインのデータ読み込み\n",
    "    for domain in DOMAINS:\n",
    "        pattern = f\"{data_dir}/*{domain}*ins_res.json\"\n",
    "        files = glob.glob(pattern)\n",
    "        \n",
    "        if files:\n",
    "            latest_file = max(files, key=lambda x: os.path.getctime(x))\n",
    "            print(f\"📂 {domain}: {latest_file}\")\n",
    "            \n",
    "            with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "                domain_data = json.load(f)\n",
    "            \n",
    "            # メタデータ追加\n",
    "            for item in domain_data:\n",
    "                item['source'] = 'magpie-standalone'\n",
    "                item['dataset_version'] = '1.0'\n",
    "                if 'domain' not in item:\n",
    "                    item['domain'] = domain\n",
    "            \n",
    "            all_data.extend(domain_data)\n",
    "            domain_counts[domain] = len(domain_data)\n",
    "            print(f\"✅ {domain}: {len(domain_data)}問題\")\n",
    "        else:\n",
    "            print(f\"⚠️ {domain}: データファイル未発見\")\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"❌ 統合可能なデータがありません\")\n",
    "        return None\n",
    "    \n",
    "    # シャッフル\n",
    "    random.shuffle(all_data)\n",
    "    \n",
    "    # ID再割り当て\n",
    "    for i, item in enumerate(all_data):\n",
    "        item['id'] = i\n",
    "    \n",
    "    # 統合ファイル保存\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # ShareGPT形式変換\n",
    "    if sharegpt_file:\n",
    "        convert_to_sharegpt(all_data, sharegpt_file)\n",
    "    \n",
    "    # 統計表示\n",
    "    print(f\"\\n📊 統合結果:\")\n",
    "    print(f\"   総問題数: {len(all_data)}\")\n",
    "    print(f\"   ドメイン分布:\")\n",
    "    for domain, count in domain_counts.items():\n",
    "        print(f\"     {DOMAIN_TEMPLATES[domain]['name']}: {count}問題\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def convert_to_sharegpt(data, output_file):\n",
    "    \"\"\"ShareGPT形式変換\"\"\"\n",
    "    print(f\"🔄 ShareGPT形式変換: {output_file}\")\n",
    "    \n",
    "    sharegpt_data = []\n",
    "    for item in data:\n",
    "        sharegpt_item = {\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": item['instruction']\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": item['response']\n",
    "                }\n",
    "            ],\n",
    "            \"domain\": item['domain'],\n",
    "            \"source\": item.get('source', 'magpie-standalone'),\n",
    "            \"id\": item['id']\n",
    "        }\n",
    "        sharegpt_data.append(sharegpt_item)\n",
    "    \n",
    "    # JSONL形式で保存\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in sharegpt_data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"✅ ShareGPT変換完了: {len(sharegpt_data)}項目\")\n",
    "\n",
    "print(\"🔧 データ統合機能準備完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generation_header"
   },
   "source": [
    "## 🚀 6ドメインデータ生成・統合実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "main_generation"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# メイン実行: 6ドメインデータ生成・統合\n",
    "# =============================================================================\n",
    "\n",
    "import traceback\n",
    "\n",
    "def main_generation_pipeline():\n",
    "    \"\"\"メイン生成パイプライン\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n🚀 6ドメインデータ生成開始\")\n",
    "    print(f\"📊 設定: {MODEL_PATH}, 各{PROBLEMS_PER_DOMAIN}問題\")\n",
    "    print(f\"⏰ 開始時刻: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    try:\n",
    "        # 生成エンジン初期化\n",
    "        generator = MagpieDomainGenerator(\n",
    "            model_path=MODEL_PATH,\n",
    "            tensor_parallel=TENSOR_PARALLEL,\n",
    "            gpu_memory_util=GPU_MEMORY_UTIL,\n",
    "            max_model_len=MAX_MODEL_LEN\n",
    "        )\n",
    "        \n",
    "        generated_files = []\n",
    "        \n",
    "        # 各ドメインの生成\n",
    "        for i, domain in enumerate(DOMAINS, 1):\n",
    "            domain_info = DOMAIN_TEMPLATES[domain]\n",
    "            print(f\"\\n📚 [{i}/{len(DOMAINS)}] {domain_info['name']} 処理中...\")\n",
    "            \n",
    "            try:\n",
    "                # ファイル名生成\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                ins_file = f\"{OUTPUT_DIR}/{domain}_{PROBLEMS_PER_DOMAIN}_{timestamp}_ins.json\"\n",
    "                res_file = f\"{OUTPUT_DIR}/{domain}_{PROBLEMS_PER_DOMAIN}_{timestamp}_ins_res.json\"\n",
    "                \n",
    "                # 問題生成\n",
    "                instructions = generator.generate_instructions(domain, PROBLEMS_PER_DOMAIN, ins_file)\n",
    "                \n",
    "                # 解答生成\n",
    "                responses = generator.generate_responses(ins_file, res_file)\n",
    "                \n",
    "                generated_files.append(res_file)\n",
    "                print(f\"✅ {domain_info['name']} 完了\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ {domain_info['name']} エラー: {e}\")\n",
    "                print(traceback.format_exc())\n",
    "                continue\n",
    "        \n",
    "        # データ統合\n",
    "        if generated_files:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            merged_file = f\"{OUTPUT_DIR}/{DATASET_NAME}_{timestamp}.json\"\n",
    "            sharegpt_file = f\"{OUTPUT_DIR}/{DATASET_NAME}_{timestamp}_sharegpt.jsonl\"\n",
    "            \n",
    "            print(f\"\\n🔄 データ統合開始...\")\n",
    "            merged_data = merge_domain_datasets(OUTPUT_DIR, merged_file, sharegpt_file)\n",
    "            \n",
    "            if merged_data:\n",
    "                print(f\"\\n🎉 全処理完了!\")\n",
    "                print(f\"📁 統合ファイル: {merged_file}\")\n",
    "                print(f\"📁 ShareGPT: {sharegpt_file}\")\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"⏱️ 実行時間: {elapsed/60:.1f}分\")\n",
    "                \n",
    "                return merged_file, sharegpt_file\n",
    "            else:\n",
    "                print(\"❌ データ統合に失敗\")\n",
    "        else:\n",
    "            print(\"❌ 生成されたファイルがありません\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 重大エラー: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        return None, None\n",
    "\n",
    "# 実行\n",
    "merged_file, sharegpt_file = main_generation_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis_header"
   },
   "source": [
    "## 📊 データ分析・品質確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_analysis"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# データ分析・品質確認\n",
    "# =============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_generated_data(data_file):\n",
    "    \"\"\"生成データ分析\"\"\"\n",
    "    if not data_file or not os.path.exists(data_file):\n",
    "        print(\"❌ データファイルが見つかりません\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n📊 データ分析: {data_file}\")\n",
    "    \n",
    "    # データ読み込み\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 基本統計\n",
    "    print(f\"\\n📈 基本統計:\")\n",
    "    print(f\"   総問題数: {len(data)}\")\n",
    "    \n",
    "    # ドメイン分布\n",
    "    domains = [item['domain'] for item in data]\n",
    "    domain_counts = Counter(domains)\n",
    "    \n",
    "    print(f\"\\n🎯 ドメイン分布:\")\n",
    "    for domain, count in domain_counts.items():\n",
    "        domain_name = DOMAIN_TEMPLATES[domain]['name']\n",
    "        percentage = (count / len(data)) * 100\n",
    "        print(f\"   {domain_name}: {count}問題 ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 文字数統計\n",
    "    instruction_lengths = [len(item['instruction']) for item in data]\n",
    "    response_lengths = [len(item['response']) for item in data]\n",
    "    \n",
    "    print(f\"\\n📝 文字数統計:\")\n",
    "    print(f\"   問題文:\")\n",
    "    print(f\"     平均: {sum(instruction_lengths)/len(instruction_lengths):.0f}文字\")\n",
    "    print(f\"     最小-最大: {min(instruction_lengths)}-{max(instruction_lengths)}文字\")\n",
    "    print(f\"   解答文:\")\n",
    "    print(f\"     平均: {sum(response_lengths)/len(response_lengths):.0f}文字\")\n",
    "    print(f\"     最小-最大: {min(response_lengths)}-{max(response_lengths)}文字\")\n",
    "    \n",
    "    # 可視化\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # ドメイン分布\n",
    "    domain_names = [DOMAIN_TEMPLATES[d]['name'] for d in domain_counts.keys()]\n",
    "    axes[0,0].pie(domain_counts.values(), labels=domain_names, autopct='%1.1f%%')\n",
    "    axes[0,0].set_title('ドメイン分布')\n",
    "    \n",
    "    # 問題文長分布\n",
    "    axes[0,1].hist(instruction_lengths, bins=20, alpha=0.7)\n",
    "    axes[0,1].set_title('問題文長分布')\n",
    "    axes[0,1].set_xlabel('文字数')\n",
    "    \n",
    "    # 解答文長分布\n",
    "    axes[1,0].hist(response_lengths, bins=20, alpha=0.7, color='orange')\n",
    "    axes[1,0].set_title('解答文長分布')\n",
    "    axes[1,0].set_xlabel('文字数')\n",
    "    \n",
    "    # ドメイン別文字数\n",
    "    domain_data = []\n",
    "    for item in data:\n",
    "        domain_data.append({\n",
    "            'domain': DOMAIN_TEMPLATES[item['domain']]['name'],\n",
    "            'instruction_length': len(item['instruction']),\n",
    "            'response_length': len(item['response'])\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(domain_data)\n",
    "    sns.boxplot(data=df, x='domain', y='response_length', ax=axes[1,1])\n",
    "    axes[1,1].set_title('ドメイン別解答文長')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/data_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # サンプル表示\n",
    "    print(f\"\\n🔍 サンプル問題 (各ドメイン1問):\")\n",
    "    shown_domains = set()\n",
    "    for item in data:\n",
    "        domain = item['domain']\n",
    "        if domain not in shown_domains:\n",
    "            domain_name = DOMAIN_TEMPLATES[domain]['name']\n",
    "            print(f\"\\n📚 {domain_name}:\")\n",
    "            print(f\"   問題: {item['instruction'][:100]}...\")\n",
    "            print(f\"   解答: {item['response'][:150]}...\")\n",
    "            shown_domains.add(domain)\n",
    "            if len(shown_domains) >= 3:  # 3ドメインまで表示\n",
    "                break\n",
    "\n",
    "# 分析実行\n",
    "if merged_file:\n",
    "    analyze_generated_data(merged_file)\n",
    "else:\n",
    "    print(\"⚠️ 分析対象データがありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_header"
   },
   "source": [
    "## 📥 データダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_download"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# データダウンロード\n",
    "# =============================================================================\n",
    "\n",
    "import zipfile\n",
    "from google.colab import files\n",
    "\n",
    "def create_download_package():\n",
    "    \"\"\"ダウンロード用パッケージ作成\"\"\"\n",
    "    print(\"\\n📦 ダウンロードパッケージ作成中...\")\n",
    "    \n",
    "    # 利用可能ファイル検索\n",
    "    available_files = []\n",
    "    \n",
    "    # 統合データファイル\n",
    "    json_files = glob.glob(f\"{OUTPUT_DIR}/*{DATASET_NAME}*.json\")\n",
    "    jsonl_files = glob.glob(f\"{OUTPUT_DIR}/*{DATASET_NAME}*.jsonl\")\n",
    "    \n",
    "    available_files.extend(json_files)\n",
    "    available_files.extend(jsonl_files)\n",
    "    \n",
    "    # 個別ドメインファイル\n",
    "    for domain in DOMAINS:\n",
    "        domain_files = glob.glob(f\"{OUTPUT_DIR}/{domain}_*_ins_res.json\")\n",
    "        available_files.extend(domain_files)\n",
    "    \n",
    "    # 分析図\n",
    "    plot_files = glob.glob(f\"{OUTPUT_DIR}/*.png\")\n",
    "    available_files.extend(plot_files)\n",
    "    \n",
    "    if not available_files:\n",
    "        print(\"❌ ダウンロード可能なファイルがありません\")\n",
    "        return None\n",
    "    \n",
    "    # ZIP作成\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    zip_filename = f\"{OUTPUT_DIR}/{DATASET_NAME}_complete_{timestamp}.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "        for file_path in available_files:\n",
    "            arcname = os.path.basename(file_path)\n",
    "            zipf.write(file_path, arcname)\n",
    "            print(f\"📁 追加: {arcname}\")\n",
    "    \n",
    "    # ファイルサイズ確認\n",
    "    zip_size = os.path.getsize(zip_filename) / (1024 * 1024)  # MB\n",
    "    print(f\"\\n✅ ZIPパッケージ作成完了\")\n",
    "    print(f\"📦 ファイル: {zip_filename}\")\n",
    "    print(f\"📊 サイズ: {zip_size:.2f}MB\")\n",
    "    print(f\"📂 含有ファイル数: {len(available_files)}\")\n",
    "    \n",
    "    return zip_filename\n",
    "\n",
    "def download_files():\n",
    "    \"\"\"ファイルダウンロード実行\"\"\"\n",
    "    # ZIPパッケージ作成\n",
    "    zip_file = create_download_package()\n",
    "    \n",
    "    if zip_file:\n",
    "        print(f\"\\n⬇️ ダウンロード開始...\")\n",
    "        try:\n",
    "            files.download(zip_file)\n",
    "            print(f\"✅ ダウンロード完了\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ダウンロードエラー: {e}\")\n",
    "            \n",
    "            # 個別ファイルダウンロード\n",
    "            print(\"🔄 個別ファイルダウンロード試行...\")\n",
    "            \n",
    "            # 統合データ優先\n",
    "            if merged_file and os.path.exists(merged_file):\n",
    "                try:\n",
    "                    files.download(merged_file)\n",
    "                    print(f\"✅ 統合データダウンロード完了\")\n",
    "                except:\n",
    "                    print(f\"❌ 統合データダウンロード失敗\")\n",
    "            \n",
    "            # ShareGPTデータ\n",
    "            if sharegpt_file and os.path.exists(sharegpt_file):\n",
    "                try:\n",
    "                    files.download(sharegpt_file)\n",
    "                    print(f\"✅ ShareGPTデータダウンロード完了\")\n",
    "                except:\n",
    "                    print(f\"❌ ShareGPTデータダウンロード失敗\")\n",
    "    \n",
    "    # ファイルリスト表示\n",
    "    print(f\"\\n📋 生成ファイル一覧:\")\n",
    "    all_files = glob.glob(f\"{OUTPUT_DIR}/*\")\n",
    "    for file_path in sorted(all_files):\n",
    "        if os.path.isfile(file_path):\n",
    "            filename = os.path.basename(file_path)\n",
    "            size = os.path.getsize(file_path) / 1024  # KB\n",
    "            print(f\"   📄 {filename} ({size:.1f}KB)\")\n",
    "\n",
    "# ダウンロード実行\n",
    "download_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_header"
   },
   "source": [
    "## 📋 実行結果サマリー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "execution_summary"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 実行結果サマリー\n",
    "# =============================================================================\n",
    "\n",
    "def print_execution_summary():\n",
    "    \"\"\"実行結果サマリー表示\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎉 Magpie 6ドメイン数学データセット生成完了\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n📊 生成設定:\")\n",
    "    print(f\"   🤖 モデル: {MODEL_PATH}\")\n",
    "    print(f\"   📝 各ドメイン問題数: {PROBLEMS_PER_DOMAIN}\")\n",
    "    print(f\"   🎯 総問題数: {PROBLEMS_PER_DOMAIN * 6}\")\n",
    "    print(f\"   🔧 軽量モード: {'有効' if USE_LIGHTWEIGHT_MODEL else '無効'}\")\n",
    "    \n",
    "    print(f\"\\n🎯 対象ドメイン:\")\n",
    "    for i, domain in enumerate(DOMAINS, 1):\n",
    "        domain_name = DOMAIN_TEMPLATES[domain]['name']\n",
    "        print(f\"   {i}. {domain_name}\")\n",
    "    \n",
    "    if merged_file and os.path.exists(merged_file):\n",
    "        # 最終データ確認\n",
    "        with open(merged_file, 'r', encoding='utf-8') as f:\n",
    "            final_data = json.load(f)\n",
    "        \n",
    "        domain_counts = Counter(item['domain'] for item in final_data)\n",
    "        \n",
    "        print(f\"\\n✅ 生成結果:\")\n",
    "        print(f\"   📂 統合データ: {os.path.basename(merged_file)}\")\n",
    "        if sharegpt_file and os.path.exists(sharegpt_file):\n",
    "            print(f\"   📂 ShareGPT形式: {os.path.basename(sharegpt_file)}\")\n",
    "        print(f\"   📊 総問題数: {len(final_data)}\")\n",
    "        \n",
    "        print(f\"\\n📈 ドメイン別結果:\")\n",
    "        success_domains = 0\n",
    "        for domain in DOMAINS:\n",
    "            count = domain_counts.get(domain, 0)\n",
    "            domain_name = DOMAIN_TEMPLATES[domain]['name']\n",
    "            status = \"✅\" if count > 0 else \"❌\"\n",
    "            print(f\"   {status} {domain_name}: {count}問題\")\n",
    "            if count > 0:\n",
    "                success_domains += 1\n",
    "        \n",
    "        success_rate = (success_domains / len(DOMAINS)) * 100\n",
    "        print(f\"\\n🎯 成功率: {success_domains}/{len(DOMAINS)} ドメイン ({success_rate:.0f}%)\")\n",
    "        \n",
    "        if success_rate == 100:\n",
    "            print(\"\\n🏆 全ドメイン生成成功！\")\n",
    "        elif success_rate >= 80:\n",
    "            print(\"\\n🎉 高い成功率で完了！\")\n",
    "        else:\n",
    "            print(\"\\n⚠️ 一部のドメインで生成に失敗\")\n",
    "    else:\n",
    "        print(\"\\n❌ 統合データファイルが見つかりません\")\n",
    "    \n",
    "    print(f\"\\n💡 使用方法:\")\n",
    "    print(f\"   1. 上記でダウンロードしたZIPファイルを展開\")\n",
    "    print(f\"   2. .jsonファイルは直接利用可能\")\n",
    "    print(f\"   3. _sharegpt.jsonlファイルは機械学習フレームワーク用\")\n",
    "    print(f\"   4. 各ドメイン別ファイルも個別利用可能\")\n",
    "    \n",
    "    print(\"\\n🔗 関連情報:\")\n",
    "    print(\"   📚 Magpie論文: https://arxiv.org/abs/2406.08464\")\n",
    "    print(\"   🤖 DeepSeek R1: https://huggingface.co/deepseek-ai/DeepSeek-R1\")\n",
    "    print(\"   💬 ShareGPT形式: 機械学習の標準的な対話データ形式\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✨ HLE数学対策データセット生成完了 ✨\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# サマリー表示\n",
    "print_execution_summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",\n  "colab": {\n   \"gpuType\": \"T4\",\n   \"provenance\": []\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"name\": \"python\"\n  }\n }\n}