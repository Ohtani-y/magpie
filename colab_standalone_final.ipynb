{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_header"
   },
   "source": [
    "# ğŸ§® Magpie: 6ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ (å®Œå…¨å˜ç‹¬ç‰ˆ)\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€**å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ä¾å­˜ä¸€åˆ‡ãªã—**ã§DeepSeek R1ã‚’ä½¿ç”¨ã—ã¦6ã¤ã®æ•°å­¦ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆãƒ»çµ±åˆã™ã‚‹å®Œå…¨å˜ç‹¬å®Ÿè¡Œç‰ˆã§ã™ã€‚\n",
    "\n",
    "## ğŸ¯ ç‰¹å¾´\n",
    "- âœ… **å®Œå…¨å˜ç‹¬å®Ÿè¡Œ**: å¤–éƒ¨ãƒªãƒã‚¸ãƒˆãƒªãƒ»ãƒ•ã‚¡ã‚¤ãƒ«ä¸è¦\n",
    "- âœ… **ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–**: å„æ•°å­¦é ˜åŸŸã«ç‰¹åŒ–ã—ãŸå•é¡Œç”Ÿæˆ\n",
    "- âœ… **è‡ªå‹•çµ±åˆ**: 6ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•ãƒãƒ¼ã‚¸ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "- âœ… **GPUè‡ªå‹•æœ€é©åŒ–**: T4/A100ä¸¡å¯¾å¿œ\n",
    "- âœ… **ã‚¨ãƒ©ãƒ¼å›å¾©**: å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°\n",
    "\n",
    "## ğŸ“Š å¯¾å¿œãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆå„é ˜åŸŸã«ç‰¹åŒ–ï¼‰\n",
    "1. **Algebra** (ä»£æ•°å­¦): æ–¹ç¨‹å¼ã€å¤šé …å¼ã€é–¢æ•°\n",
    "2. **Applied Mathematics** (å¿œç”¨æ•°å­¦): å¾®åˆ†æ–¹ç¨‹å¼ã€æœ€é©åŒ–\n",
    "3. **Calculus** (å¾®ç©åˆ†å­¦): å¾®ç©åˆ†ã€æ¥µé™ã€ç´šæ•°\n",
    "4. **Discrete Mathematics** (é›¢æ•£æ•°å­¦): çµ„åˆã›ã€ã‚°ãƒ©ãƒ•ç†è«–\n",
    "5. **Geometry** (å¹¾ä½•å­¦): è§£æå¹¾ä½•ã€ç©ºé–“å›³å½¢\n",
    "6. **Number Theory** (æ•°è«–): ç´ æ•°ã€åˆåŒå¼ã€æš—å·å¿œç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "settings_header"
   },
   "source": [
    "## âš™ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "user_settings"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š\n",
    "# =============================================================================\n",
    "\n",
    "# åŸºæœ¬è¨­å®š\n",
    "USE_LIGHTWEIGHT_MODEL = True  # True: è»½é‡ãƒ¢ãƒ¼ãƒ‰ (T4æ¨å¥¨), False: ãƒ•ãƒ«ãƒ¢ãƒ¼ãƒ‰ (A100æ¨å¥¨)\n",
    "PROBLEMS_PER_DOMAIN = 20      # å„ãƒ‰ãƒ¡ã‚¤ãƒ³ã®å•é¡Œæ•° (10-100æ¨å¥¨)\n",
    "USE_HF_LOGIN = True           # Hugging Faceèªè¨¼ã‚’ä½¿ç”¨\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«è¨­å®š (USE_LIGHTWEIGHT_MODELã«å¿œã˜ã¦è‡ªå‹•èª¿æ•´)\n",
    "if USE_LIGHTWEIGHT_MODEL:\n",
    "    MODEL_PATH = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "    TENSOR_PARALLEL = 1\n",
    "    GPU_MEMORY_UTIL = 0.80\n",
    "    MAX_MODEL_LEN = 4096\n",
    "else:\n",
    "    MODEL_PATH = \"deepseek-ai/DeepSeek-R1\"\n",
    "    TENSOR_PARALLEL = 2\n",
    "    GPU_MEMORY_UTIL = 0.90\n",
    "    MAX_MODEL_LEN = 8192\n",
    "\n",
    "# ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "INSTRUCTION_TEMP = 1.2\n",
    "INSTRUCTION_TOP_P = 1.0\n",
    "RESPONSE_TEMP = 0.1\n",
    "RESPONSE_TOP_P = 1.0\n",
    "\n",
    "# å‡ºåŠ›è¨­å®š\n",
    "OUTPUT_DIR = \"/content/magpie_6domains\"\n",
    "DATASET_NAME = f\"HLE_6Domains_Math_{PROBLEMS_PER_DOMAIN * 6}\"\n",
    "\n",
    "print(f\"ğŸš€ è¨­å®šå®Œäº†\")\n",
    "print(f\"ğŸ“Š ãƒ¢ãƒ‡ãƒ«: {MODEL_PATH}\")\n",
    "print(f\"ğŸ“ˆ å•é¡Œæ•°: {PROBLEMS_PER_DOMAIN} Ã— 6ãƒ‰ãƒ¡ã‚¤ãƒ³ = {PROBLEMS_PER_DOMAIN * 6}å•é¡Œ\")\n",
    "print(f\"ğŸ”§ è»½é‡ãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if USE_LIGHTWEIGHT_MODEL else 'ç„¡åŠ¹'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## ğŸ”§ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# =============================================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸\n",
    "packages = [\n",
    "    \"vllm>=0.6.0\",\n",
    "    \"transformers>=4.45.0\", \n",
    "    \"torch>=2.0.0\",\n",
    "    \"accelerate\",\n",
    "    \"datasets\", \n",
    "    \"sentencepiece\",\n",
    "    \"tiktoken\",\n",
    "    \"numpy\",\n",
    "    \"pandas\", \n",
    "    \"tqdm\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“¦ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        install_package(package)\n",
    "        print(f\"âœ… {package}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {package}: {e}\")\n",
    "\n",
    "print(\"\\nğŸ”§ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GPUç’°å¢ƒç¢ºèªãƒ»æœ€é©åŒ–\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# GPUç¢ºèª\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"ğŸ® GPUæƒ…å ±:\")\n",
    "    print(f\"   åå‰: {gpu_name}\")\n",
    "    print(f\"   æ•°é‡: {gpu_count}\")\n",
    "    print(f\"   ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB\")\n",
    "    \n",
    "    # ãƒ¡ãƒ¢ãƒªã«å¿œã˜ãŸè‡ªå‹•èª¿æ•´\n",
    "    if gpu_memory < 20:  # T4ç›¸å½“\n",
    "        if not USE_LIGHTWEIGHT_MODEL:\n",
    "            print(\"âš ï¸  GPU ãƒ¡ãƒ¢ãƒªãŒå°‘ãªã„ãŸã‚è»½é‡ãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´\")\n",
    "            MODEL_PATH = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "            TENSOR_PARALLEL = 1\n",
    "            GPU_MEMORY_UTIL = 0.75\n",
    "            if PROBLEMS_PER_DOMAIN > 30:\n",
    "                PROBLEMS_PER_DOMAIN = 20\n",
    "                print(f\"ğŸ“‰ å•é¡Œæ•°ã‚’{PROBLEMS_PER_DOMAIN}ã«èª¿æ•´\")\n",
    "else:\n",
    "    print(\"âŒ GPU ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“\")\n",
    "    raise RuntimeError(\"GPUå¿…é ˆã§ã™\")\n",
    "\n",
    "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"ğŸ“ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf_login"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Hugging Faceèªè¨¼ (DeepSeek R1ä½¿ç”¨æ™‚)\n",
    "# =============================================================================\n",
    "\n",
    "if USE_HF_LOGIN and \"deepseek\" in MODEL_PATH.lower():\n",
    "    try:\n",
    "        from huggingface_hub import login\n",
    "        print(\"ğŸ” Hugging Faceèªè¨¼ãŒå¿…è¦ã§ã™\")\n",
    "        print(\"   https://huggingface.co/settings/tokens ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—\")\n",
    "        login()\n",
    "        print(\"âœ… èªè¨¼å®Œäº†\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ èªè¨¼ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(\"   è»½é‡ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´ã—ã¾ã™\")\n",
    "        MODEL_PATH = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "else:\n",
    "    print(\"â„¹ï¸ èªè¨¼ä¸è¦ (è»½é‡ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "core_functions_header"
   },
   "source": [
    "## ğŸ”§ ã‚³ã‚¢æ©Ÿèƒ½å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "domain_templates"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "# =============================================================================\n",
    "\n",
    "DOMAIN_TEMPLATES = {\n",
    "    \"algebra\": {\n",
    "        \"name\": \"ä»£æ•°å­¦ (Algebra)\",\n",
    "        \"system_prompt\": \"\"\"ã‚ãªãŸã¯ä»£æ•°å­¦å°‚é–€ã®æ•°å­¦æ•™å¸«ã§ã™ã€‚ä»¥ä¸‹ã®é ˜åŸŸã«ç‰¹åŒ–ã—ãŸé«˜å“è³ªãªæ•°å­¦å•é¡Œã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "**å¯¾è±¡é ˜åŸŸï¼š**\n",
    "- ç·šå½¢ä»£æ•°ï¼šè¡Œåˆ—ã€ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã€å›ºæœ‰å€¤ãƒ»å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«\n",
    "- æŠ½è±¡ä»£æ•°ï¼šç¾¤ã€ç’°ã€ä½“ã®ç†è«–\n",
    "- å¤šé …å¼ä»£æ•°ï¼šå› æ•°åˆ†è§£ã€æ ¹ã¨ä¿‚æ•°ã®é–¢ä¿‚\n",
    "- æ–¹ç¨‹å¼è«–ï¼šé€£ç«‹æ–¹ç¨‹å¼ã€é«˜æ¬¡æ–¹ç¨‹å¼\n",
    "- ç·šå½¢å¤‰æ›ï¼šåŸºåº•å¤‰æ›ã€å¯¾è§’åŒ–\n",
    "\n",
    "**å•é¡Œã®ç‰¹å¾´ï¼š**\n",
    "- æ¦‚å¿µã®æ·±ã„ç†è§£ã‚’å•ã†å•é¡Œ\n",
    "- è¨¼æ˜ã‚„è«–ç†çš„æ¨è«–ã‚’å«ã‚€\n",
    "- å®Ÿéš›ã®å¿œç”¨ä¾‹ã¨ã®é–¢é€£\n",
    "- æ®µéšçš„è§£æ³•ãŒæ˜ç¢ºãªå•é¡Œ\"\"\"\n",
    "    },\n",
    "    \"applied-mathematics\": {\n",
    "        \"name\": \"å¿œç”¨æ•°å­¦ (Applied Mathematics)\", \n",
    "        \"system_prompt\": \"\"\"ã‚ãªãŸã¯å¿œç”¨æ•°å­¦å°‚é–€ã®ç ”ç©¶è€…ã§ã™ã€‚ä»¥ä¸‹ã®é ˜åŸŸã«ç‰¹åŒ–ã—ãŸå®Ÿè·µçš„ãªæ•°å­¦å•é¡Œã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "**å¯¾è±¡é ˜åŸŸï¼š**\n",
    "- å¾®åˆ†æ–¹ç¨‹å¼ï¼šå¸¸å¾®åˆ†æ–¹ç¨‹å¼ã€åå¾®åˆ†æ–¹ç¨‹å¼\n",
    "- æœ€é©åŒ–ç†è«–ï¼šç·šå½¢è¨ˆç”»æ³•ã€éç·šå½¢æœ€é©åŒ–\n",
    "- æ•°å€¤è§£æï¼šæ•°å€¤ç©åˆ†ã€æ•°å€¤å¾®åˆ†ã€è¿‘ä¼¼è§£æ³•\n",
    "- ç¢ºç‡ãƒ»çµ±è¨ˆï¼šçµ±è¨ˆçš„æ¨æ¸¬ã€å›å¸°åˆ†æ\n",
    "- å‹•çš„ã‚·ã‚¹ãƒ†ãƒ ï¼šã‚«ã‚ªã‚¹ç†è«–ã€ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«\n",
    "\n",
    "**å•é¡Œã®ç‰¹å¾´ï¼š**\n",
    "- ç¾å®Ÿä¸–ç•Œã®å•é¡Œã‚’ãƒ¢ãƒ‡ãƒ«åŒ–\n",
    "- å·¥å­¦ãƒ»ç‰©ç†ãƒ»çµŒæ¸ˆå­¦ã¨ã®é–¢é€£\n",
    "- æ•°å€¤è¨ˆç®—ã‚„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ \n",
    "- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çš„æ€è€ƒã‚’è¦æ±‚\"\"\"\n",
    "    },\n",
    "    \"calculus\": {\n",
    "        \"name\": \"å¾®ç©åˆ†å­¦ (Calculus)\",\n",
    "        \"system_prompt\": \"\"\"ã‚ãªãŸã¯å¾®ç©åˆ†å­¦å°‚é–€ã®æ•°å­¦æ•™æˆã§ã™ã€‚ä»¥ä¸‹ã®é ˜åŸŸã«ç‰¹åŒ–ã—ãŸç†è«–çš„ãƒ»å®Ÿè·µçš„ãªå•é¡Œã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "**å¯¾è±¡é ˜åŸŸï¼š**\n",
    "- æ¥µé™ç†è«–ï¼šé–¢æ•°ã®æ¥µé™ã€é€£ç¶šæ€§\n",
    "- å¾®åˆ†æ³•ï¼šå°é–¢æ•°ã€åå¾®åˆ†ã€å…¨å¾®åˆ†\n",
    "- ç©åˆ†æ³•ï¼šä¸å®šç©åˆ†ã€å®šç©åˆ†ã€é‡ç©åˆ†\n",
    "- ç´šæ•°ç†è«–ï¼šãƒ†ã‚¤ãƒ©ãƒ¼ç´šæ•°ã€ãƒ•ãƒ¼ãƒªã‚¨ç´šæ•°\n",
    "- ãƒ™ã‚¯ãƒˆãƒ«è§£æï¼šå‹¾é…ã€ç™ºæ•£ã€å›è»¢\n",
    "\n",
    "**å•é¡Œã®ç‰¹å¾´ï¼š**\n",
    "- å³å¯†ãªæ•°å­¦çš„è¨¼æ˜ã‚’å«ã‚€\n",
    "- å¹¾ä½•å­¦çš„ç›´è¦³ã¨ã®çµåˆ\n",
    "- ç‰©ç†ç¾è±¡ã¨ã®é–¢é€£\n",
    "- è¨ˆç®—æŠ€æ³•ã®ç¿’å¾—ã‚’ä¿ƒé€²\"\"\"\n",
    "    },\n",
    "    \"discrete-mathematics\": {\n",
    "        \"name\": \"é›¢æ•£æ•°å­¦ (Discrete Mathematics)\",\n",
    "        \"system_prompt\": \"\"\"ã‚ãªãŸã¯é›¢æ•£æ•°å­¦ã¨ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹å°‚é–€ã®æ•™å“¡ã§ã™ã€‚ä»¥ä¸‹ã®é ˜åŸŸã«ç‰¹åŒ–ã—ãŸè«–ç†çš„æ€è€ƒã‚’é›ãˆã‚‹å•é¡Œã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "**å¯¾è±¡é ˜åŸŸï¼š**\n",
    "- çµ„åˆã›è«–ï¼šé †åˆ—ã€çµ„åˆã›ã€é³©ã®å·£åŸç†\n",
    "- ã‚°ãƒ©ãƒ•ç†è«–ï¼šã‚°ãƒ©ãƒ•ã®æ€§è³ªã€æœ€çŸ­çµŒè·¯ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\n",
    "- è«–ç†å­¦ï¼šå‘½é¡Œè«–ç†ã€è¿°èªè«–ç†ã€æ¨è«–è¦å‰‡\n",
    "- é›†åˆè«–ï¼šé›†åˆæ¼”ç®—ã€é–¢ä¿‚ã€å†™åƒ\n",
    "- è¨ˆç®—è¤‡é›‘æ€§ï¼šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€è¨ˆç®—é‡ç†è«–\n",
    "\n",
    "**å•é¡Œã®ç‰¹å¾´ï¼š**\n",
    "- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çš„æ€è€ƒã‚’é‡è¦–\n",
    "- ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã¸ã®å¿œç”¨\n",
    "- æ§‹æˆçš„è¨¼æ˜æ‰‹æ³•\n",
    "- ãƒ‘ã‚ºãƒ«çš„è¦ç´ ã‚’å«ã‚€å‰µæ„å·¥å¤«\"\"\"\n",
    "    },\n",
    "    \"geometry\": {\n",
    "        \"name\": \"å¹¾ä½•å­¦ (Geometry)\",\n",
    "        \"system_prompt\": \"\"\"ã‚ãªãŸã¯å¹¾ä½•å­¦å°‚é–€ã®æ•°å­¦æ•™å¸«ã§ã™ã€‚ä»¥ä¸‹ã®é ˜åŸŸã«ç‰¹åŒ–ã—ãŸè¦–è¦šçš„ç†è§£ã‚’ä¿ƒé€²ã™ã‚‹å•é¡Œã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "**å¯¾è±¡é ˜åŸŸï¼š**\n",
    "- è§£æå¹¾ä½•ï¼šåº§æ¨™ç³»ã€ç›´ç·šãƒ»å††ãƒ»æ¥•å††ã®æ–¹ç¨‹å¼\n",
    "- ç«‹ä½“å¹¾ä½•ï¼šç©ºé–“å›³å½¢ã€ä½“ç©ãƒ»è¡¨é¢ç©ã®è¨ˆç®—\n",
    "- å°„å½±å¹¾ä½•ï¼šå°„å½±å¤‰æ›ã€ç„¡é™é ç‚¹\n",
    "- å¾®åˆ†å¹¾ä½•ï¼šæ›²ç·šãƒ»æ›²é¢ã®ç†è«–\n",
    "- ãƒˆãƒãƒ­ã‚¸ãƒ¼ï¼šä½ç›¸çš„æ€§è³ªã€é€£ç¶šå¤‰å½¢\n",
    "\n",
    "**å•é¡Œã®ç‰¹å¾´ï¼š**\n",
    "- å›³å½¢ã®æ€§è³ªã‚„é–¢ä¿‚æ€§ã«ç„¦ç‚¹\n",
    "- è¦–è¦šçš„ã‚¤ãƒ¡ãƒ¼ã‚¸ã¨ä»£æ•°çš„è¨ˆç®—ã®çµåˆ\n",
    "- ç©ºé–“èªè­˜èƒ½åŠ›ã®å‘ä¸Š\n",
    "- ç¾çš„å´é¢ã‚‚é‡è¦–ã—ãŸå•é¡Œè¨­è¨ˆ\"\"\"\n",
    "    },\n",
    "    \"number-theory\": {\n",
    "        \"name\": \"æ•°è«– (Number Theory)\",\n",
    "        \"system_prompt\": \"\"\"ã‚ãªãŸã¯æ•°è«–å°‚é–€ã®æ•°å­¦ç ”ç©¶è€…ã§ã™ã€‚ä»¥ä¸‹ã®é ˜åŸŸã«ç‰¹åŒ–ã—ãŸç´”ç²‹æ•°å­¦ã®ç¾ã—ã•ã‚’ä¼ãˆã‚‹å•é¡Œã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "**å¯¾è±¡é ˜åŸŸï¼š**\n",
    "- åˆç­‰æ•°è«–ï¼šç´ æ•°ã€æœ€å¤§å…¬ç´„æ•°ã€ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ã®äº’é™¤æ³•\n",
    "- åˆåŒå¼ç†è«–ï¼šãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ç®—è¡“ã€ä¸­å›½å‰°ä½™å®šç†\n",
    "- äºŒæ¬¡å½¢å¼ï¼šãƒšãƒ«æ–¹ç¨‹å¼ã€é€£åˆ†æ•°\n",
    "- è§£æçš„æ•°è«–ï¼šç´ æ•°å®šç†ã€ã‚¼ãƒ¼ã‚¿é–¢æ•°\n",
    "- æš—å·ç†è«–ï¼šRSAæš—å·ã€æ¥•å††æ›²ç·šæš—å·\n",
    "\n",
    "**å•é¡Œã®ç‰¹å¾´ï¼š**\n",
    "- æ•°ã®æœ¬è³ªçš„æ€§è³ªã‚’æ¢ç©¶\n",
    "- æ­´å²çš„èƒŒæ™¯ã‚„æ•°å­¦è€…ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "- ç¾ä»£æš—å·å­¦ã¸ã®å¿œç”¨\n",
    "- å³å¯†ãªè¨¼æ˜æŠ€æ³•ã®ç¿’å¾—\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆ\n",
    "DOMAINS = list(DOMAIN_TEMPLATES.keys())\n",
    "\n",
    "print(\"ğŸ¯ ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†\")\n",
    "for domain, info in DOMAIN_TEMPLATES.items():\n",
    "    print(f\"   ğŸ“š {info['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generation_engine"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import vllm\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "class MagpieDomainGenerator:\n",
    "    def __init__(self, model_path, tensor_parallel=1, gpu_memory_util=0.8, max_model_len=4096):\n",
    "        self.model_path = model_path\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        print(f\"ğŸš€ vLLMåˆæœŸåŒ–ä¸­: {model_path}\")\n",
    "        try:\n",
    "            self.llm = LLM(\n",
    "                model=model_path,\n",
    "                tensor_parallel_size=tensor_parallel,\n",
    "                gpu_memory_utilization=gpu_memory_util,\n",
    "                max_model_len=max_model_len,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"âœ… vLLMåˆæœŸåŒ–æˆåŠŸ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ vLLMåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_instructions(self, domain, num_problems, output_file):\n",
    "        \"\"\"ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–å•é¡Œç”Ÿæˆ\"\"\"\n",
    "        domain_info = DOMAIN_TEMPLATES[domain]\n",
    "        print(f\"\\nğŸ“ {domain_info['name']} å•é¡Œç”Ÿæˆé–‹å§‹ ({num_problems}å•)\")\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=INSTRUCTION_TEMP,\n",
    "            top_p=INSTRUCTION_TOP_P,\n",
    "            max_tokens=512,\n",
    "            stop=[\"\\n\\n\", \"å•é¡Œ:\", \"Problem:\"]\n",
    "        )\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ\n",
    "        base_prompt = f\"\"\"{domain_info['system_prompt']}\n",
    "\n",
    "é«˜ç­‰ãƒ¬ãƒ™ãƒ«è©¦é¨“ï¼ˆHLEï¼‰ã«é©ã—ãŸ{domain_info['name']}ã®å•é¡Œã‚’1ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n",
    "å•é¡Œã¯æ€è€ƒåŠ›ã¨å¿œç”¨åŠ›ã‚’è©¦ã™ã‚‚ã®ã¨ã—ã€æ˜ç¢ºã§å®Œçµãªæ–‡ç« ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "å•é¡Œ:\"\"\"\n",
    "        \n",
    "        prompts = [base_prompt] * num_problems\n",
    "        \n",
    "        # ç”Ÿæˆå®Ÿè¡Œ\n",
    "        print(\"ğŸ”„ ç”Ÿæˆå®Ÿè¡Œä¸­...\")\n",
    "        outputs = self.llm.generate(prompts, sampling_params)\n",
    "        \n",
    "        # çµæœä¿å­˜\n",
    "        results = []\n",
    "        for i, output in enumerate(outputs):\n",
    "            instruction = output.outputs[0].text.strip()\n",
    "            if instruction:  # ç©ºã§ãªã„å ´åˆã®ã¿ä¿å­˜\n",
    "                results.append({\n",
    "                    \"id\": i,\n",
    "                    \"instruction\": instruction,\n",
    "                    \"domain\": domain,\n",
    "                    \"created\": self.timestamp,\n",
    "                    \"gen_input_configs\": {\n",
    "                        \"model\": self.model_path,\n",
    "                        \"temperature\": INSTRUCTION_TEMP,\n",
    "                        \"top_p\": INSTRUCTION_TOP_P,\n",
    "                        \"domain\": domain\n",
    "                    }\n",
    "                })\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"âœ… {domain_info['name']} å•é¡Œç”Ÿæˆå®Œäº†: {len(results)}å•\")\n",
    "        return results\n",
    "    \n",
    "    def generate_responses(self, instructions_file, output_file):\n",
    "        \"\"\"Chain-of-Thoughtè§£ç­”ç”Ÿæˆ\"\"\"\n",
    "        print(f\"\\nğŸ§  è§£ç­”ç”Ÿæˆé–‹å§‹: {instructions_file}\")\n",
    "        \n",
    "        # å•é¡Œèª­ã¿è¾¼ã¿\n",
    "        with open(instructions_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=RESPONSE_TEMP,\n",
    "            top_p=RESPONSE_TOP_P,\n",
    "            max_tokens=2048\n",
    "        )\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ\n",
    "        prompts = []\n",
    "        for item in data:\n",
    "            domain_info = DOMAIN_TEMPLATES[item['domain']]\n",
    "            prompt = f\"\"\"ä»¥ä¸‹ã®{domain_info['name']}ã®å•é¡Œã«ã€æ®µéšçš„æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ï¼ˆChain-of-Thoughtï¼‰ã‚’ç”¨ã„ã¦è©³ç´°ã«è§£ç­”ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "å•é¡Œ: {item['instruction']}\n",
    "\n",
    "è§£ç­”:\"\"\"\n",
    "            prompts.append(prompt)\n",
    "        \n",
    "        # ç”Ÿæˆå®Ÿè¡Œ\n",
    "        print(\"ğŸ”„ è§£ç­”ç”Ÿæˆå®Ÿè¡Œä¸­...\")\n",
    "        outputs = self.llm.generate(prompts, sampling_params)\n",
    "        \n",
    "        # çµæœçµ±åˆ\n",
    "        for i, output in enumerate(outputs):\n",
    "            data[i]['response'] = output.outputs[0].text.strip()\n",
    "            data[i]['gen_response_configs'] = {\n",
    "                \"model\": self.model_path,\n",
    "                \"temperature\": RESPONSE_TEMP,\n",
    "                \"top_p\": RESPONSE_TOP_P\n",
    "            }\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"âœ… è§£ç­”ç”Ÿæˆå®Œäº†: {len(data)}å•\")\n",
    "        return data\n",
    "\n",
    "print(\"ğŸ”§ ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³æº–å‚™å®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_integration"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ©Ÿèƒ½\n",
    "# =============================================================================\n",
    "\n",
    "import glob\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "def merge_domain_datasets(data_dir, output_file, sharegpt_file=None):\n",
    "    \"\"\"6ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±åˆãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«\"\"\"\n",
    "    print(\"\\nğŸ”„ ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±åˆé–‹å§‹\")\n",
    "    \n",
    "    all_data = []\n",
    "    domain_counts = {}\n",
    "    \n",
    "    # å„ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "    for domain in DOMAINS:\n",
    "        pattern = f\"{data_dir}/*{domain}*ins_res.json\"\n",
    "        files = glob.glob(pattern)\n",
    "        \n",
    "        if files:\n",
    "            latest_file = max(files, key=lambda x: os.path.getctime(x))\n",
    "            print(f\"ğŸ“‚ {domain}: {latest_file}\")\n",
    "            \n",
    "            with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "                domain_data = json.load(f)\n",
    "            \n",
    "            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ \n",
    "            for item in domain_data:\n",
    "                item['source'] = 'magpie-standalone'\n",
    "                item['dataset_version'] = '1.0'\n",
    "                if 'domain' not in item:\n",
    "                    item['domain'] = domain\n",
    "            \n",
    "            all_data.extend(domain_data)\n",
    "            domain_counts[domain] = len(domain_data)\n",
    "            print(f\"âœ… {domain}: {len(domain_data)}å•é¡Œ\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ {domain}: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«æœªç™ºè¦‹\")\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"âŒ çµ±åˆå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "        return None\n",
    "    \n",
    "    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "    random.shuffle(all_data)\n",
    "    \n",
    "    # IDå†å‰²ã‚Šå½“ã¦\n",
    "    for i, item in enumerate(all_data):\n",
    "        item['id'] = i\n",
    "    \n",
    "    # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # ShareGPTå½¢å¼å¤‰æ›\n",
    "    if sharegpt_file:\n",
    "        convert_to_sharegpt(all_data, sharegpt_file)\n",
    "    \n",
    "    # çµ±è¨ˆè¡¨ç¤º\n",
    "    print(f\"\\nğŸ“Š çµ±åˆçµæœ:\")\n",
    "    print(f\"   ç·å•é¡Œæ•°: {len(all_data)}\")\n",
    "    print(f\"   ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†å¸ƒ:\")\n",
    "    for domain, count in domain_counts.items():\n",
    "        print(f\"     {DOMAIN_TEMPLATES[domain]['name']}: {count}å•é¡Œ\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def convert_to_sharegpt(data, output_file):\n",
    "    \"\"\"ShareGPTå½¢å¼å¤‰æ›\"\"\"\n",
    "    print(f\"ğŸ”„ ShareGPTå½¢å¼å¤‰æ›: {output_file}\")\n",
    "    \n",
    "    sharegpt_data = []\n",
    "    for item in data:\n",
    "        sharegpt_item = {\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": item['instruction']\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": item['response']\n",
    "                }\n",
    "            ],\n",
    "            \"domain\": item['domain'],\n",
    "            \"source\": item.get('source', 'magpie-standalone'),\n",
    "            \"id\": item['id']\n",
    "        }\n",
    "        sharegpt_data.append(sharegpt_item)\n",
    "    \n",
    "    # JSONLå½¢å¼ã§ä¿å­˜\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in sharegpt_data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"âœ… ShareGPTå¤‰æ›å®Œäº†: {len(sharegpt_data)}é …ç›®\")\n",
    "\n",
    "print(\"ğŸ”§ ãƒ‡ãƒ¼ã‚¿çµ±åˆæ©Ÿèƒ½æº–å‚™å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generation_header"
   },
   "source": [
    "## ğŸš€ 6ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ»çµ±åˆå®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "main_generation"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ: 6ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ»çµ±åˆ\n",
    "# =============================================================================\n",
    "\n",
    "import traceback\n",
    "\n",
    "def main_generation_pipeline():\n",
    "    \"\"\"ãƒ¡ã‚¤ãƒ³ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nğŸš€ 6ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–‹å§‹\")\n",
    "    print(f\"ğŸ“Š è¨­å®š: {MODEL_PATH}, å„{PROBLEMS_PER_DOMAIN}å•é¡Œ\")\n",
    "    print(f\"â° é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    try:\n",
    "        # ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–\n",
    "        generator = MagpieDomainGenerator(\n",
    "            model_path=MODEL_PATH,\n",
    "            tensor_parallel=TENSOR_PARALLEL,\n",
    "            gpu_memory_util=GPU_MEMORY_UTIL,\n",
    "            max_model_len=MAX_MODEL_LEN\n",
    "        )\n",
    "        \n",
    "        generated_files = []\n",
    "        \n",
    "        # å„ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ç”Ÿæˆ\n",
    "        for i, domain in enumerate(DOMAINS, 1):\n",
    "            domain_info = DOMAIN_TEMPLATES[domain]\n",
    "            print(f\"\\nğŸ“š [{i}/{len(DOMAINS)}] {domain_info['name']} å‡¦ç†ä¸­...\")\n",
    "            \n",
    "            try:\n",
    "                # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                ins_file = f\"{OUTPUT_DIR}/{domain}_{PROBLEMS_PER_DOMAIN}_{timestamp}_ins.json\"\n",
    "                res_file = f\"{OUTPUT_DIR}/{domain}_{PROBLEMS_PER_DOMAIN}_{timestamp}_ins_res.json\"\n",
    "                \n",
    "                # å•é¡Œç”Ÿæˆ\n",
    "                instructions = generator.generate_instructions(domain, PROBLEMS_PER_DOMAIN, ins_file)\n",
    "                \n",
    "                # è§£ç­”ç”Ÿæˆ\n",
    "                responses = generator.generate_responses(ins_file, res_file)\n",
    "                \n",
    "                generated_files.append(res_file)\n",
    "                print(f\"âœ… {domain_info['name']} å®Œäº†\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ {domain_info['name']} ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "                print(traceback.format_exc())\n",
    "                continue\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿çµ±åˆ\n",
    "        if generated_files:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            merged_file = f\"{OUTPUT_DIR}/{DATASET_NAME}_{timestamp}.json\"\n",
    "            sharegpt_file = f\"{OUTPUT_DIR}/{DATASET_NAME}_{timestamp}_sharegpt.jsonl\"\n",
    "            \n",
    "            print(f\"\\nğŸ”„ ãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹...\")\n",
    "            merged_data = merge_domain_datasets(OUTPUT_DIR, merged_file, sharegpt_file)\n",
    "            \n",
    "            if merged_data:\n",
    "                print(f\"\\nğŸ‰ å…¨å‡¦ç†å®Œäº†!\")\n",
    "                print(f\"ğŸ“ çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«: {merged_file}\")\n",
    "                print(f\"ğŸ“ ShareGPT: {sharegpt_file}\")\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"â±ï¸ å®Ÿè¡Œæ™‚é–“: {elapsed/60:.1f}åˆ†\")\n",
    "                \n",
    "                return merged_file, sharegpt_file\n",
    "            else:\n",
    "                print(\"âŒ ãƒ‡ãƒ¼ã‚¿çµ±åˆã«å¤±æ•—\")\n",
    "        else:\n",
    "            print(\"âŒ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ é‡å¤§ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        return None, None\n",
    "\n",
    "# å®Ÿè¡Œ\n",
    "merged_file, sharegpt_file = main_generation_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis_header"
   },
   "source": [
    "## ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»å“è³ªç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_analysis"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»å“è³ªç¢ºèª\n",
    "# =============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_generated_data(data_file):\n",
    "    \"\"\"ç”Ÿæˆãƒ‡ãƒ¼ã‚¿åˆ†æ\"\"\"\n",
    "    if not data_file or not os.path.exists(data_file):\n",
    "        print(\"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†æ: {data_file}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # åŸºæœ¬çµ±è¨ˆ\n",
    "    print(f\"\\nğŸ“ˆ åŸºæœ¬çµ±è¨ˆ:\")\n",
    "    print(f\"   ç·å•é¡Œæ•°: {len(data)}\")\n",
    "    \n",
    "    # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†å¸ƒ\n",
    "    domains = [item['domain'] for item in data]\n",
    "    domain_counts = Counter(domains)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†å¸ƒ:\")\n",
    "    for domain, count in domain_counts.items():\n",
    "        domain_name = DOMAIN_TEMPLATES[domain]['name']\n",
    "        percentage = (count / len(data)) * 100\n",
    "        print(f\"   {domain_name}: {count}å•é¡Œ ({percentage:.1f}%)\")\n",
    "    \n",
    "    # æ–‡å­—æ•°çµ±è¨ˆ\n",
    "    instruction_lengths = [len(item['instruction']) for item in data]\n",
    "    response_lengths = [len(item['response']) for item in data]\n",
    "    \n",
    "    print(f\"\\nğŸ“ æ–‡å­—æ•°çµ±è¨ˆ:\")\n",
    "    print(f\"   å•é¡Œæ–‡:\")\n",
    "    print(f\"     å¹³å‡: {sum(instruction_lengths)/len(instruction_lengths):.0f}æ–‡å­—\")\n",
    "    print(f\"     æœ€å°-æœ€å¤§: {min(instruction_lengths)}-{max(instruction_lengths)}æ–‡å­—\")\n",
    "    print(f\"   è§£ç­”æ–‡:\")\n",
    "    print(f\"     å¹³å‡: {sum(response_lengths)/len(response_lengths):.0f}æ–‡å­—\")\n",
    "    print(f\"     æœ€å°-æœ€å¤§: {min(response_lengths)}-{max(response_lengths)}æ–‡å­—\")\n",
    "    \n",
    "    # å¯è¦–åŒ–\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†å¸ƒ\n",
    "    domain_names = [DOMAIN_TEMPLATES[d]['name'] for d in domain_counts.keys()]\n",
    "    axes[0,0].pie(domain_counts.values(), labels=domain_names, autopct='%1.1f%%')\n",
    "    axes[0,0].set_title('ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†å¸ƒ')\n",
    "    \n",
    "    # å•é¡Œæ–‡é•·åˆ†å¸ƒ\n",
    "    axes[0,1].hist(instruction_lengths, bins=20, alpha=0.7)\n",
    "    axes[0,1].set_title('å•é¡Œæ–‡é•·åˆ†å¸ƒ')\n",
    "    axes[0,1].set_xlabel('æ–‡å­—æ•°')\n",
    "    \n",
    "    # è§£ç­”æ–‡é•·åˆ†å¸ƒ\n",
    "    axes[1,0].hist(response_lengths, bins=20, alpha=0.7, color='orange')\n",
    "    axes[1,0].set_title('è§£ç­”æ–‡é•·åˆ†å¸ƒ')\n",
    "    axes[1,0].set_xlabel('æ–‡å­—æ•°')\n",
    "    \n",
    "    # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥æ–‡å­—æ•°\n",
    "    domain_data = []\n",
    "    for item in data:\n",
    "        domain_data.append({\n",
    "            'domain': DOMAIN_TEMPLATES[item['domain']]['name'],\n",
    "            'instruction_length': len(item['instruction']),\n",
    "            'response_length': len(item['response'])\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(domain_data)\n",
    "    sns.boxplot(data=df, x='domain', y='response_length', ax=axes[1,1])\n",
    "    axes[1,1].set_title('ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥è§£ç­”æ–‡é•·')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/data_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n",
    "    print(f\"\\nğŸ” ã‚µãƒ³ãƒ—ãƒ«å•é¡Œ (å„ãƒ‰ãƒ¡ã‚¤ãƒ³1å•):\")\n",
    "    shown_domains = set()\n",
    "    for item in data:\n",
    "        domain = item['domain']\n",
    "        if domain not in shown_domains:\n",
    "            domain_name = DOMAIN_TEMPLATES[domain]['name']\n",
    "            print(f\"\\nğŸ“š {domain_name}:\")\n",
    "            print(f\"   å•é¡Œ: {item['instruction'][:100]}...\")\n",
    "            print(f\"   è§£ç­”: {item['response'][:150]}...\")\n",
    "            shown_domains.add(domain)\n",
    "            if len(shown_domains) >= 3:  # 3ãƒ‰ãƒ¡ã‚¤ãƒ³ã¾ã§è¡¨ç¤º\n",
    "                break\n",
    "\n",
    "# åˆ†æå®Ÿè¡Œ\n",
    "if merged_file:\n",
    "    analyze_generated_data(merged_file)\n",
    "else:\n",
    "    print(\"âš ï¸ åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_header"
   },
   "source": [
    "## ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_download"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "# =============================================================================\n",
    "\n",
    "import zipfile\n",
    "from google.colab import files\n",
    "\n",
    "def create_download_package():\n",
    "    \"\"\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä½œæˆ\"\"\"\n",
    "    print(\"\\nğŸ“¦ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä½œæˆä¸­...\")\n",
    "    \n",
    "    # åˆ©ç”¨å¯èƒ½ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢\n",
    "    available_files = []\n",
    "    \n",
    "    # çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "    json_files = glob.glob(f\"{OUTPUT_DIR}/*{DATASET_NAME}*.json\")\n",
    "    jsonl_files = glob.glob(f\"{OUTPUT_DIR}/*{DATASET_NAME}*.jsonl\")\n",
    "    \n",
    "    available_files.extend(json_files)\n",
    "    available_files.extend(jsonl_files)\n",
    "    \n",
    "    # å€‹åˆ¥ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "    for domain in DOMAINS:\n",
    "        domain_files = glob.glob(f\"{OUTPUT_DIR}/{domain}_*_ins_res.json\")\n",
    "        available_files.extend(domain_files)\n",
    "    \n",
    "    # åˆ†æå›³\n",
    "    plot_files = glob.glob(f\"{OUTPUT_DIR}/*.png\")\n",
    "    available_files.extend(plot_files)\n",
    "    \n",
    "    if not available_files:\n",
    "        print(\"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "        return None\n",
    "    \n",
    "    # ZIPä½œæˆ\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    zip_filename = f\"{OUTPUT_DIR}/{DATASET_NAME}_complete_{timestamp}.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "        for file_path in available_files:\n",
    "            arcname = os.path.basename(file_path)\n",
    "            zipf.write(file_path, arcname)\n",
    "            print(f\"ğŸ“ è¿½åŠ : {arcname}\")\n",
    "    \n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª\n",
    "    zip_size = os.path.getsize(zip_filename) / (1024 * 1024)  # MB\n",
    "    print(f\"\\nâœ… ZIPãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä½œæˆå®Œäº†\")\n",
    "    print(f\"ğŸ“¦ ãƒ•ã‚¡ã‚¤ãƒ«: {zip_filename}\")\n",
    "    print(f\"ğŸ“Š ã‚µã‚¤ã‚º: {zip_size:.2f}MB\")\n",
    "    print(f\"ğŸ“‚ å«æœ‰ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(available_files)}\")\n",
    "    \n",
    "    return zip_filename\n",
    "\n",
    "def download_files():\n",
    "    \"\"\"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ\"\"\"\n",
    "    # ZIPãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä½œæˆ\n",
    "    zip_file = create_download_package()\n",
    "    \n",
    "    if zip_file:\n",
    "        print(f\"\\nâ¬‡ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...\")\n",
    "        try:\n",
    "            files.download(zip_file)\n",
    "            print(f\"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            \n",
    "            # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "            print(\"ğŸ”„ å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è©¦è¡Œ...\")\n",
    "            \n",
    "            # çµ±åˆãƒ‡ãƒ¼ã‚¿å„ªå…ˆ\n",
    "            if merged_file and os.path.exists(merged_file):\n",
    "                try:\n",
    "                    files.download(merged_file)\n",
    "                    print(f\"âœ… çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†\")\n",
    "                except:\n",
    "                    print(f\"âŒ çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—\")\n",
    "            \n",
    "            # ShareGPTãƒ‡ãƒ¼ã‚¿\n",
    "            if sharegpt_file and os.path.exists(sharegpt_file):\n",
    "                try:\n",
    "                    files.download(sharegpt_file)\n",
    "                    print(f\"âœ… ShareGPTãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†\")\n",
    "                except:\n",
    "                    print(f\"âŒ ShareGPTãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—\")\n",
    "    \n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆè¡¨ç¤º\n",
    "    print(f\"\\nğŸ“‹ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:\")\n",
    "    all_files = glob.glob(f\"{OUTPUT_DIR}/*\")\n",
    "    for file_path in sorted(all_files):\n",
    "        if os.path.isfile(file_path):\n",
    "            filename = os.path.basename(file_path)\n",
    "            size = os.path.getsize(file_path) / 1024  # KB\n",
    "            print(f\"   ğŸ“„ {filename} ({size:.1f}KB)\")\n",
    "\n",
    "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ\n",
    "download_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_header"
   },
   "source": [
    "## ğŸ“‹ å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "execution_summary"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼\n",
    "# =============================================================================\n",
    "\n",
    "def print_execution_summary():\n",
    "    \"\"\"å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ‰ Magpie 6ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆå®Œäº†\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ç”Ÿæˆè¨­å®š:\")\n",
    "    print(f\"   ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {MODEL_PATH}\")\n",
    "    print(f\"   ğŸ“ å„ãƒ‰ãƒ¡ã‚¤ãƒ³å•é¡Œæ•°: {PROBLEMS_PER_DOMAIN}\")\n",
    "    print(f\"   ğŸ¯ ç·å•é¡Œæ•°: {PROBLEMS_PER_DOMAIN * 6}\")\n",
    "    print(f\"   ğŸ”§ è»½é‡ãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if USE_LIGHTWEIGHT_MODEL else 'ç„¡åŠ¹'}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³:\")\n",
    "    for i, domain in enumerate(DOMAINS, 1):\n",
    "        domain_name = DOMAIN_TEMPLATES[domain]['name']\n",
    "        print(f\"   {i}. {domain_name}\")\n",
    "    \n",
    "    if merged_file and os.path.exists(merged_file):\n",
    "        # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ç¢ºèª\n",
    "        with open(merged_file, 'r', encoding='utf-8') as f:\n",
    "            final_data = json.load(f)\n",
    "        \n",
    "        domain_counts = Counter(item['domain'] for item in final_data)\n",
    "        \n",
    "        print(f\"\\nâœ… ç”Ÿæˆçµæœ:\")\n",
    "        print(f\"   ğŸ“‚ çµ±åˆãƒ‡ãƒ¼ã‚¿: {os.path.basename(merged_file)}\")\n",
    "        if sharegpt_file and os.path.exists(sharegpt_file):\n",
    "            print(f\"   ğŸ“‚ ShareGPTå½¢å¼: {os.path.basename(sharegpt_file)}\")\n",
    "        print(f\"   ğŸ“Š ç·å•é¡Œæ•°: {len(final_data)}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥çµæœ:\")\n",
    "        success_domains = 0\n",
    "        for domain in DOMAINS:\n",
    "            count = domain_counts.get(domain, 0)\n",
    "            domain_name = DOMAIN_TEMPLATES[domain]['name']\n",
    "            status = \"âœ…\" if count > 0 else \"âŒ\"\n",
    "            print(f\"   {status} {domain_name}: {count}å•é¡Œ\")\n",
    "            if count > 0:\n",
    "                success_domains += 1\n",
    "        \n",
    "        success_rate = (success_domains / len(DOMAINS)) * 100\n",
    "        print(f\"\\nğŸ¯ æˆåŠŸç‡: {success_domains}/{len(DOMAINS)} ãƒ‰ãƒ¡ã‚¤ãƒ³ ({success_rate:.0f}%)\")\n",
    "        \n",
    "        if success_rate == 100:\n",
    "            print(\"\\nğŸ† å…¨ãƒ‰ãƒ¡ã‚¤ãƒ³ç”ŸæˆæˆåŠŸï¼\")\n",
    "        elif success_rate >= 80:\n",
    "            print(\"\\nğŸ‰ é«˜ã„æˆåŠŸç‡ã§å®Œäº†ï¼\")\n",
    "        else:\n",
    "            print(\"\\nâš ï¸ ä¸€éƒ¨ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ç”Ÿæˆã«å¤±æ•—\")\n",
    "    else:\n",
    "        print(\"\\nâŒ çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:\")\n",
    "    print(f\"   1. ä¸Šè¨˜ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹\")\n",
    "    print(f\"   2. .jsonãƒ•ã‚¡ã‚¤ãƒ«ã¯ç›´æ¥åˆ©ç”¨å¯èƒ½\")\n",
    "    print(f\"   3. _sharegpt.jsonlãƒ•ã‚¡ã‚¤ãƒ«ã¯æ©Ÿæ¢°å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ç”¨\")\n",
    "    print(f\"   4. å„ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å€‹åˆ¥åˆ©ç”¨å¯èƒ½\")\n",
    "    \n",
    "    print(\"\\nğŸ”— é–¢é€£æƒ…å ±:\")\n",
    "    print(\"   ğŸ“š Magpieè«–æ–‡: https://arxiv.org/abs/2406.08464\")\n",
    "    print(\"   ğŸ¤– DeepSeek R1: https://huggingface.co/deepseek-ai/DeepSeek-R1\")\n",
    "    print(\"   ğŸ’¬ ShareGPTå½¢å¼: æ©Ÿæ¢°å­¦ç¿’ã®æ¨™æº–çš„ãªå¯¾è©±ãƒ‡ãƒ¼ã‚¿å½¢å¼\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ¨ HLEæ•°å­¦å¯¾ç­–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆå®Œäº† âœ¨\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# ã‚µãƒãƒªãƒ¼è¡¨ç¤º\n",
    "print_execution_summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",\n  "colab": {\n   \"gpuType\": \"T4\",\n   \"provenance\": []\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"name\": \"python\"\n  }\n }\n}