# Google Colab vLLM ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆ

# å•é¡Œã®ã‚ã‚‹vLLMã‚³ãƒ¼ãƒ‰ã‚’ç½®ãæ›ãˆ
"""
# å…ƒã®vLLMã‚³ãƒ¼ãƒ‰ï¼ˆã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ï¼‰
from vllm import LLM, SamplingParams
llm = LLM(model=model_path, tensor_parallel_size=2)
"""

# ä¿®æ­£ç‰ˆ: Transformersã§ä»£æ›¿
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

class ColabCompatibleGenerator:
    def __init__(self, model_path):
        print(f"ğŸ”„ Colabäº’æ›ãƒ¢ãƒ¼ãƒ‰: {model_path}")
        
        # 4bité‡å­åŒ–è¨­å®š
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        # T4å‘ã‘è»½é‡ãƒ¢ãƒ‡ãƒ«è‡ªå‹•é¸æŠ
        if "deepseek" in model_path.lower() or "70b" in model_path.lower():
            model_path = "Qwen/Qwen2.5-3B-Instruct"
            print(f"âš ï¸ Colabå‘ã‘ã«è»½é‡ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´: {model_path}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def generate(self, prompts, sampling_params):
        """vLLMäº’æ›ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
        results = []
        for prompt in prompts:
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=sampling_params.max_tokens,
                    temperature=sampling_params.temperature,
                    top_p=sampling_params.top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            )
            
            # vLLMäº’æ›ã®å‡ºåŠ›å½¢å¼
            class MockOutput:
                def __init__(self, text):
                    self.outputs = [type('obj', (object,), {'text': text})()]
            
            results.append(MockOutput(generated_text))
        
        return results

# ä½¿ç”¨æ–¹æ³•ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã®ç½®ãæ›ãˆï¼‰
def fix_vllm_error():
    """vLLMã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã™ã‚‹é–¢æ•°"""
    print("""
ğŸ”§ vLLMã‚¨ãƒ©ãƒ¼ä¿®æ­£æ–¹æ³•:

1. æ—¢å­˜ã®vLLMã‚³ãƒ¼ãƒ‰ã‚’æ¢ã™:
   ```python
   from vllm import LLM, SamplingParams
   llm = LLM(...)
   ```

2. ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã«ç½®ãæ›ãˆ:
   ```python
   from colab_vllm_fix import ColabCompatibleGenerator
   llm = ColabCompatibleGenerator(model_path)
   ```

3. ã¾ãŸã¯è»½é‡ç‰ˆãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ä½¿ç”¨:
   ğŸ“‚ colab_lightweight_alternative.ipynb
   
âœ… ã“ã‚Œã§Google Colabã§ç¢ºå®Ÿå‹•ä½œã—ã¾ã™ï¼
    """)

if __name__ == "__main__":
    fix_vllm_error()