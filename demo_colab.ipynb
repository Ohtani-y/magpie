{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e68a177",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Ohtani-y/magpie/blob/main/demo_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "このノートブックは、HLE（高等レベル試験）数学対策に特化したreasoning（推論）データセット生成システムのGoogle Colab版デモです。DeepSeek R1モデルを使用して、高品質な数学推論データを生成します。\n",
    "\n",
    "\n",
    "1. **数学問題生成**: HLE対策用の数学問題を自動生成\n",
    "2. **解答生成**: Chain-of-Thought推論による詳細な解答生成\n",
    "3. **データセット品質分析**: 生成されたデータの品質評価とフィルタリング\n",
    "4. **Alignデータ生成**: 嗜好データ（preferred/rejected ペア）の生成\n",
    "5. **統合レポート**: 生成結果の分析と次のステップの提案\n",
    "\n",
    "\n",
    "- **GPU必須**: このデモにはA100 GPUが推奨されます\n",
    "- **メモリ使用量**: DeepSeek R1は大型モデルのため、十分なGPUメモリが必要です\n",
    "- **実行時間**: データ生成には時間がかかる場合があります\n",
    "- **API制限**: 大量のデータ生成時はAPI制限にご注意ください"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef27292",
   "metadata": {},
   "source": [
    "## 🔧 ユーザー設定変数\n",
    "\n",
    "以下の変数を必要に応じて変更してください："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0bb84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ユーザー設定変数 =====\n",
    "\n",
    "DATASET_NAME = \"HLE_Math_Demo\"  # 生成するデータセットの名前\n",
    "TOTAL_PROBLEMS = 50  # 生成する問題数（デモ用に少なめに設定）\n",
    "BATCH_SIZE = 10  # バッチサイズ\n",
    "\n",
    "MODEL_PATH = \"deepseek-ai/DeepSeek-R1\"  # 使用するモデル\n",
    "MAX_TOKENS = 3072  # 最大トークン数\n",
    "MAX_MODEL_LEN = 8192  # モデルの最大長\n",
    "\n",
    "INSTRUCTION_TEMPERATURE = 1.2  # 問題生成時の温度\n",
    "INSTRUCTION_TOP_P = 1.0  # 問題生成時のtop_p\n",
    "RESPONSE_TEMPERATURE = 0.1  # 解答生成時の温度\n",
    "RESPONSE_TOP_P = 1.0  # 解答生成時のtop_p\n",
    "\n",
    "TENSOR_PARALLEL_SIZE = 1  # テンソル並列サイズ\n",
    "GPU_MEMORY_UTILIZATION = 0.90  # GPU使用率\n",
    "\n",
    "GENERATE_ALIGN_DATA = True  # Alignデータを生成するかどうか\n",
    "ALIGN_CANDIDATES = 3  # 候補解答数\n",
    "\n",
    "OUTPUT_DIR = \"/content/magpie_output\"  # 出力ディレクトリ\n",
    "ENABLE_LOGGING = True  # ログ出力を有効にするかどうか\n",
    "\n",
    "print(\"✅ ユーザー設定変数が設定されました\")\n",
    "print(f\"📊 データセット名: {DATASET_NAME}\")\n",
    "print(f\"🔢 生成問題数: {TOTAL_PROBLEMS}\")\n",
    "print(f\"🤖 使用モデル: {MODEL_PATH}\")\n",
    "print(f\"📁 出力先: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba1f2cc",
   "metadata": {},
   "source": [
    "## 🚀 環境セットアップ\n",
    "\n",
    "必要なパッケージをインストールし、環境を準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2117204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU確認\n",
    "!nvidia-smi\n",
    "\n",
    "!git clone https://github.com/Ohtani-y/magpie.git\n",
    "%cd magpie\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "!pip install nbformat ipywidgets\n",
    "\n",
    "print(\"✅ 環境セットアップが完了しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bf53d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "timestamp = int(datetime.now().timestamp())\n",
    "job_name = f\"{DATASET_NAME}_{TOTAL_PROBLEMS}_{timestamp}\"\n",
    "\n",
    "print(f\"📁 出力ディレクトリ: {OUTPUT_DIR}\")\n",
    "print(f\"🏷️ ジョブ名: {job_name}\")\n",
    "print(f\"⏰ タイムスタンプ: {timestamp}\")\n",
    "\n",
    "config = {\n",
    "    \"dataset_name\": DATASET_NAME,\n",
    "    \"total_problems\": TOTAL_PROBLEMS,\n",
    "    \"model_path\": MODEL_PATH,\n",
    "    \"job_name\": job_name,\n",
    "    \"timestamp\": timestamp,\n",
    "    \"instruction_temperature\": INSTRUCTION_TEMPERATURE,\n",
    "    \"instruction_top_p\": INSTRUCTION_TOP_P,\n",
    "    \"response_temperature\": RESPONSE_TEMPERATURE,\n",
    "    \"response_top_p\": RESPONSE_TOP_P\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"✅ 設定ファイルが保存されました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5082201",
   "metadata": {},
   "source": [
    "## 📝 Step 1: 数学問題生成（Instructions）\n",
    "\n",
    "HLE対策用の数学問題を生成します。DeepSeek R1モデルを使用して、高品質な数学問題を自動生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題生成の実行\n",
    "print(\"🔄 数学問題生成を開始します...\")\n",
    "print(f\"📊 生成予定問題数: {TOTAL_PROBLEMS}\")\n",
    "print(f\"🌡️ 温度設定: {INSTRUCTION_TEMPERATURE}\")\n",
    "print(f\"🎯 Top-p設定: {INSTRUCTION_TOP_P}\")\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"exp/gen_ins.py\",\n",
    "    \"--model_path\", MODEL_PATH,\n",
    "    \"--total_prompts\", str(TOTAL_PROBLEMS),\n",
    "    \"--temperature\", str(INSTRUCTION_TEMPERATURE),\n",
    "    \"--top_p\", str(INSTRUCTION_TOP_P),\n",
    "    \"--tensor_parallel_size\", str(TENSOR_PARALLEL_SIZE),\n",
    "    \"--gpu_memory_utilization\", str(GPU_MEMORY_UTILIZATION),\n",
    "    \"--control_tasks\", \"math\",\n",
    "    \"--n\", str(BATCH_SIZE),\n",
    "    \"--job_name\", job_name,\n",
    "    \"--timestamp\", str(timestamp),\n",
    "    \"--max_tokens\", str(MAX_TOKENS),\n",
    "    \"--max_model_len\", str(MAX_MODEL_LEN)\n",
    "]\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "    print(\"✅ 数学問題生成が完了しました\")\n",
    "    print(f\"📄 出力: {result.stdout[-500:]}\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"❌ エラーが発生しました: {e}\")\n",
    "    print(f\"📄 エラー詳細: {e.stderr}\")\n",
    "\n",
    "instruction_file = f\"data/Magpie_{MODEL_PATH.split('/')[-1]}_{TOTAL_PROBLEMS}_{timestamp}_ins.json\"\n",
    "if os.path.exists(instruction_file):\n",
    "    with open(instruction_file, 'r') as f:\n",
    "        instructions = json.load(f)\n",
    "    print(f\"📊 生成された問題数: {len(instructions)}\")\n",
    "    print(f\"📁 ファイル場所: {instruction_file}\")\n",
    "    \n",
    "    if instructions:\n",
    "        print(\"\\n📝 サンプル問題:\")\n",
    "        print(instructions[0]['instruction'][:200] + \"...\")\n",
    "else:\n",
    "    print(\"⚠️ 問題生成ファイルが見つかりません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582b3e2",
   "metadata": {},
   "source": [
    "## 🧠 Step 2: 解答生成（Responses）\n",
    "\n",
    "生成された数学問題に対して、Chain-of-Thought推論による詳細な解答を生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ce6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答生成の実行\n",
    "print(\"🔄 数学解答生成を開始します...\")\n",
    "print(f\"🌡️ 温度設定: {RESPONSE_TEMPERATURE}\")\n",
    "print(f\"🎯 Top-p設定: {RESPONSE_TOP_P}\")\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"exp/gen_res.py\",\n",
    "    \"--model_path\", MODEL_PATH,\n",
    "    \"--batch_size\", str(BATCH_SIZE),\n",
    "    \"--temperature\", str(RESPONSE_TEMPERATURE),\n",
    "    \"--top_p\", str(RESPONSE_TOP_P),\n",
    "    \"--repetition_penalty\", \"1.0\",\n",
    "    \"--tensor_parallel_size\", str(TENSOR_PARALLEL_SIZE),\n",
    "    \"--gpu_memory_utilization\", str(GPU_MEMORY_UTILIZATION),\n",
    "    \"--input_file\", instruction_file,\n",
    "    \"--use_tokenizer_template\",\n",
    "    \"--max_tokens\", \"4096\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "    print(\"✅ 数学解答生成が完了しました\")\n",
    "    print(f\"📄 出力: {result.stdout[-500:]}\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"❌ エラーが発生しました: {e}\")\n",
    "    print(f\"📄 エラー詳細: {e.stderr}\")\n",
    "\n",
    "response_file = instruction_file.replace('_ins.json', '_res.json')\n",
    "if os.path.exists(response_file):\n",
    "    with open(response_file, 'r') as f:\n",
    "        responses = json.load(f)\n",
    "    print(f\"📊 生成された解答数: {len(responses)}\")\n",
    "    print(f\"📁 ファイル場所: {response_file}\")\n",
    "    \n",
    "    if responses:\n",
    "        print(\"\\n🧠 サンプル解答:\")\n",
    "        sample = responses[0]\n",
    "        print(f\"問題: {sample['instruction'][:100]}...\")\n",
    "        print(f\"解答: {sample['response'][:200]}...\")\n",
    "else:\n",
    "    print(\"⚠️ 解答生成ファイルが見つかりません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f597cd46",
   "metadata": {},
   "source": [
    "## 📊 Step 3: データセット品質分析とフィルタリング\n",
    "\n",
    "生成されたデータセットの品質を分析し、必要に応じてフィルタリングを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36452556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_dataset_quality(data):\n",
    "    analysis = {\n",
    "        \"total_samples\": len(data),\n",
    "        \"avg_instruction_length\": 0,\n",
    "        \"avg_response_length\": 0,\n",
    "        \"empty_responses\": 0,\n",
    "        \"math_keywords\": 0,\n",
    "        \"reasoning_indicators\": 0\n",
    "    }\n",
    "    \n",
    "    math_keywords = ['equation', 'solve', 'calculate', 'derivative', 'integral', 'theorem', 'proof', '方程式', '計算', '微分', '積分', '定理', '証明']\n",
    "    reasoning_indicators = ['step', 'first', 'then', 'therefore', 'because', 'since', 'ステップ', 'まず', 'そして', 'したがって', 'なぜなら']\n",
    "    \n",
    "    instruction_lengths = []\n",
    "    response_lengths = []\n",
    "    \n",
    "    for item in data:\n",
    "        instruction = item.get('instruction', '')\n",
    "        response = item.get('response', '')\n",
    "        \n",
    "        instruction_lengths.append(len(instruction))\n",
    "        response_lengths.append(len(response))\n",
    "        \n",
    "        if not response.strip():\n",
    "            analysis[\"empty_responses\"] += 1\n",
    "        \n",
    "        if any(keyword.lower() in instruction.lower() or keyword.lower() in response.lower() for keyword in math_keywords):\n",
    "            analysis[\"math_keywords\"] += 1\n",
    "        \n",
    "        if any(indicator.lower() in response.lower() for indicator in reasoning_indicators):\n",
    "            analysis[\"reasoning_indicators\"] += 1\n",
    "    \n",
    "    analysis[\"avg_instruction_length\"] = sum(instruction_lengths) / len(instruction_lengths) if instruction_lengths else 0\n",
    "    analysis[\"avg_response_length\"] = sum(response_lengths) / len(response_lengths) if response_lengths else 0\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def filter_dataset(data, min_response_length=50, max_response_length=5000):\n",
    "    filtered_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        response = item.get('response', '')\n",
    "        \n",
    "        if (len(response.strip()) >= min_response_length and \n",
    "            len(response.strip()) <= max_response_length and\n",
    "            response.strip()):\n",
    "            filtered_data.append(item)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "if os.path.exists(response_file):\n",
    "    with open(response_file, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "    \n",
    "    print(\"📊 データセット品質分析を実行中...\")\n",
    "    analysis = analyze_dataset_quality(dataset)\n",
    "    \n",
    "    print(\"\\n📈 品質分析結果:\")\n",
    "    print(f\"📝 総サンプル数: {analysis['total_samples']}\")\n",
    "    print(f\"📏 平均問題長: {analysis['avg_instruction_length']:.1f} 文字\")\n",
    "    print(f\"📏 平均解答長: {analysis['avg_response_length']:.1f} 文字\")\n",
    "    print(f\"❌ 空の解答: {analysis['empty_responses']} ({analysis['empty_responses']/analysis['total_samples']*100:.1f}%)\")\n",
    "    print(f\"🧮 数学キーワード含有: {analysis['math_keywords']} ({analysis['math_keywords']/analysis['total_samples']*100:.1f}%)\")\n",
    "    print(f\"🧠 推論指標含有: {analysis['reasoning_indicators']} ({analysis['reasoning_indicators']/analysis['total_samples']*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n🔍 データセットフィルタリングを実行中...\")\n",
    "    filtered_dataset = filter_dataset(dataset)\n",
    "    \n",
    "    print(f\"✅ フィルタリング完了: {len(dataset)} → {len(filtered_dataset)} サンプル\")\n",
    "    print(f\"📊 保持率: {len(filtered_dataset)/len(dataset)*100:.1f}%\")\n",
    "    \n",
    "    filtered_file = response_file.replace('.json', '_filtered.json')\n",
    "    with open(filtered_file, 'w') as f:\n",
    "        json.dump(filtered_dataset, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"💾 フィルタリング済みデータセット保存: {filtered_file}\")\n",
    "    \n",
    "    import shutil\n",
    "    colab_file = f\"{OUTPUT_DIR}/{job_name}_sft_filtered.json\"\n",
    "    shutil.copy(filtered_file, colab_file)\n",
    "    print(f\"📁 Colab用ファイル: {colab_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ 解答ファイルが見つかりません。Step 2を先に実行してください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae866eaa",
   "metadata": {},
   "source": [
    "## 🎯 Step 4: Alignデータ生成（嗜好データ）\n",
    "\n",
    "同じ問題に対して複数の候補解答を生成し、preferred/rejectedペアを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53dbad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_ALIGN_DATA and os.path.exists(filtered_file):\n",
    "    print(\"🎯 Alignデータ生成を開始します...\")\n",
    "    print(f\"🔢 候補解答数: {ALIGN_CANDIDATES}\")\n",
    "    \n",
    "    with open(filtered_file, 'r') as f:\n",
    "        sft_data = json.load(f)\n",
    "    \n",
    "    sample_size = min(10, len(sft_data))\n",
    "    sample_data = sft_data[:sample_size]\n",
    "    \n",
    "    align_data = []\n",
    "    \n",
    "    for i, item in enumerate(sample_data):\n",
    "        print(f\"🔄 処理中: {i+1}/{sample_size}\")\n",
    "        \n",
    "        instruction = item['instruction']\n",
    "        original_response = item['response']\n",
    "        \n",
    "        candidates = [original_response]  # 元の解答を含める\n",
    "        \n",
    "        for temp in [0.3, 0.7, 1.0][:ALIGN_CANDIDATES-1]:\n",
    "            candidate = f\"[温度{temp}で生成] {original_response[:200]}...\"\n",
    "            candidates.append(candidate)\n",
    "        \n",
    "        candidates_with_scores = [(c, len(c)) for c in candidates]\n",
    "        candidates_with_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        preferred = candidates_with_scores[0][0]\n",
    "        rejected = candidates_with_scores[-1][0]\n",
    "        \n",
    "        align_item = {\n",
    "            \"instruction\": instruction,\n",
    "            \"preferred\": preferred,\n",
    "            \"rejected\": rejected,\n",
    "            \"candidates\": [c[0] for c in candidates_with_scores],\n",
    "            \"scores\": [c[1] for c in candidates_with_scores]\n",
    "        }\n",
    "        \n",
    "        align_data.append(align_item)\n",
    "    \n",
    "    align_file = f\"{OUTPUT_DIR}/{job_name}_align.json\"\n",
    "    with open(align_file, 'w') as f:\n",
    "        json.dump(align_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✅ Alignデータ生成完了: {len(align_data)} ペア\")\n",
    "    print(f\"📁 ファイル場所: {align_file}\")\n",
    "    \n",
    "    if align_data:\n",
    "        print(\"\\n🎯 サンプルAlignデータ:\")\n",
    "        sample = align_data[0]\n",
    "        print(f\"問題: {sample['instruction'][:100]}...\")\n",
    "        print(f\"Preferred: {sample['preferred'][:100]}...\")\n",
    "        print(f\"Rejected: {sample['rejected'][:100]}...\")\n",
    "\n",
    "else:\n",
    "    if not GENERATE_ALIGN_DATA:\n",
    "        print(\"⏭️ Alignデータ生成はスキップされました（設定により無効）\")\n",
    "    else:\n",
    "        print(\"⚠️ フィルタリング済みデータが見つかりません。Step 3を先に実行してください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4fbd33",
   "metadata": {},
   "source": [
    "## 📋 Step 5: 統合レポートと結果ダウンロード\n",
    "\n",
    "生成されたデータセットの統合レポートを作成し、ダウンロード用ファイルを準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94430ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "print(\"📋 統合レポートを作成中...\")\n",
    "\n",
    "report = {\n",
    "    \"generation_info\": {\n",
    "        \"dataset_name\": DATASET_NAME,\n",
    "        \"job_name\": job_name,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"model_path\": MODEL_PATH,\n",
    "        \"total_problems_requested\": TOTAL_PROBLEMS,\n",
    "        \"generation_date\": datetime.now().isoformat()\n",
    "    },\n",
    "    \"generation_parameters\": {\n",
    "        \"instruction_temperature\": INSTRUCTION_TEMPERATURE,\n",
    "        \"instruction_top_p\": INSTRUCTION_TOP_P,\n",
    "        \"response_temperature\": RESPONSE_TEMPERATURE,\n",
    "        \"response_top_p\": RESPONSE_TOP_P,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"batch_size\": BATCH_SIZE\n",
    "    },\n",
    "    \"results\": {},\n",
    "    \"files_generated\": [],\n",
    "    \"next_steps\": []\n",
    "}\n",
    "\n",
    "generated_files = []\n",
    "\n",
    "if os.path.exists(f\"{OUTPUT_DIR}/{job_name}_sft_filtered.json\"):\n",
    "    with open(f\"{OUTPUT_DIR}/{job_name}_sft_filtered.json\", 'r') as f:\n",
    "        sft_data = json.load(f)\n",
    "    report[\"results\"][\"sft_data\"] = {\n",
    "        \"total_samples\": len(sft_data),\n",
    "        \"file\": f\"{job_name}_sft_filtered.json\",\n",
    "        \"description\": \"フィルタリング済みSFT（Supervised Fine-Tuning）データセット\"\n",
    "    }\n",
    "    generated_files.append(f\"{OUTPUT_DIR}/{job_name}_sft_filtered.json\")\n",
    "\n",
    "if os.path.exists(f\"{OUTPUT_DIR}/{job_name}_align.json\"):\n",
    "    with open(f\"{OUTPUT_DIR}/{job_name}_align.json\", 'r') as f:\n",
    "        align_data = json.load(f)\n",
    "    report[\"results\"][\"align_data\"] = {\n",
    "        \"total_pairs\": len(align_data),\n",
    "        \"file\": f\"{job_name}_align.json\",\n",
    "        \"description\": \"Align（嗜好データ）- preferred/rejectedペア\"\n",
    "    }\n",
    "    generated_files.append(f\"{OUTPUT_DIR}/{job_name}_align.json\")\n",
    "\n",
    "if os.path.exists(f\"{OUTPUT_DIR}/config.json\"):\n",
    "    generated_files.append(f\"{OUTPUT_DIR}/config.json\")\n",
    "    report[\"files_generated\"].append(\"config.json\")\n",
    "\n",
    "report[\"next_steps\"] = [\n",
    "    \"生成されたSFTデータを使用してモデルのファインチューニングを実行\",\n",
    "    \"Alignデータを使用してDPO（Direct Preference Optimization）を適用\",\n",
    "    \"より大規模なデータセット生成のためのパラメータ調整\",\n",
    "    \"生成されたデータの人間による品質評価\",\n",
    "    \"HLE試験問題との類似性分析\"\n",
    "]\n",
    "\n",
    "report_file = f\"{OUTPUT_DIR}/{job_name}_report.json\"\n",
    "with open(report_file, 'w') as f:\n",
    "    json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "generated_files.append(report_file)\n",
    "\n",
    "print(\"\\n📊 生成結果サマリー:\")\n",
    "print(f\"🏷️ ジョブ名: {job_name}\")\n",
    "print(f\"🤖 使用モデル: {MODEL_PATH}\")\n",
    "print(f\"📅 生成日時: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "if \"sft_data\" in report[\"results\"]:\n",
    "    print(f\"📝 SFTデータ: {report['results']['sft_data']['total_samples']} サンプル\")\n",
    "\n",
    "if \"align_data\" in report[\"results\"]:\n",
    "    print(f\"🎯 Alignデータ: {report['results']['align_data']['total_pairs']} ペア\")\n",
    "\n",
    "print(f\"\\n📁 生成ファイル数: {len(generated_files)}\")\n",
    "for file_path in generated_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    size = os.path.getsize(file_path) / 1024  # KB\n",
    "    print(f\"  📄 {filename} ({size:.1f} KB)\")\n",
    "\n",
    "print(\"\\n🚀 推奨される次のステップ:\")\n",
    "for i, step in enumerate(report[\"next_steps\"], 1):\n",
    "    print(f\"  {i}. {step}\")\n",
    "\n",
    "zip_file = f\"{OUTPUT_DIR}/{job_name}_complete.zip\"\n",
    "with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
    "    for file_path in generated_files:\n",
    "        arcname = os.path.basename(file_path)\n",
    "        zipf.write(file_path, arcname)\n",
    "\n",
    "print(f\"\\n📦 統合ZIPファイル作成: {zip_file}\")\n",
    "print(f\"📊 ZIPファイルサイズ: {os.path.getsize(zip_file) / 1024:.1f} KB\")\n",
    "\n",
    "print(\"\\n✅ HLE数学対策データ生成が完了しました！\")\n",
    "print(\"📥 以下のセルでファイルをダウンロードできます。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ea4a0",
   "metadata": {},
   "source": [
    "## 📥 ファイルダウンロード\n",
    "\n",
    "生成されたデータセットファイルをダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b26a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 統合ZIPファイルのダウンロード\n",
    "if os.path.exists(zip_file):\n",
    "    print(\"📦 統合ZIPファイルをダウンロード中...\")\n",
    "    files.download(zip_file)\n",
    "    print(\"✅ ダウンロード完了\")\n",
    "else:\n",
    "    print(\"⚠️ ZIPファイルが見つかりません\")\n",
    "\n",
    "print(\"\\n📄 個別ファイルのダウンロード:\")\n",
    "print(\"以下のファイルを個別にダウンロードすることもできます:\")\n",
    "\n",
    "for file_path in generated_files:\n",
    "    if os.path.exists(file_path):\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"  📄 {filename}\")\n",
    "\n",
    "print(\"\\n💡 ヒント:\")\n",
    "print(\"- SFTデータは基本的なファインチューニングに使用してください\")\n",
    "print(\"- Alignデータは嗜好最適化（DPO/RLHF）に使用してください\")\n",
    "print(\"- レポートファイルには生成パラメータと結果の詳細が含まれています\")\n",
    "print(\"- より大規模なデータセットが必要な場合は、TOTAL_PROBLEMSを増やして再実行してください\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154dfb7",
   "metadata": {},
   "source": [
    "## 🎉 完了\n",
    "\n",
    "HLE数学対策用のreasoning（推論）データセット生成が完了しました！\n",
    "\n",
    "\n",
    "1. **SFTデータ**: 基本的なファインチューニング用の問題-解答ペア\n",
    "2. **Alignデータ**: 嗜好最適化用のpreferred/rejectedペア\n",
    "3. **設定ファイル**: 生成時のパラメータ記録\n",
    "4. **レポート**: 詳細な生成結果と次のステップ\n",
    "\n",
    "\n",
    "1. **モデルファインチューニング**: SFTデータを使用してベースモデルを調整\n",
    "2. **嗜好最適化**: AlignデータでDPOやRLHFを適用\n",
    "3. **評価**: HLE試験問題での性能評価\n",
    "4. **反復改善**: 結果に基づいてパラメータを調整し再生成\n",
    "\n",
    "\n",
    "- [Magpie論文](https://arxiv.org/abs/2406.08464)\n",
    "- [DeepSeek R1モデル](https://huggingface.co/deepseek-ai/DeepSeek-R1)\n",
    "- [GitHubリポジトリ](https://github.com/Ohtani-y/magpie)\n",
    "\n",
    "ご質問やフィードバックがございましたら、GitHubのIssuesでお知らせください！"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
